[
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nNLP • Retrieval Augmented Generation\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nMotivation\n\n  \nLexical Retrieval\n\n  \nSemantic Retrieval\n\n  \nHybrid Retrieval (Lexical + Semantic)\n\n  \nThe Retrieval Augmented Generation (RAG) Pipeline\n\n  \nBenefits of RAG\n    \n\n      \nRAG vs. Fine-tuning\n\n    \n\n  \n\n  \nEnsemble of RAG\n\n  \nChoosing a Vector DB using a Feature Matrix\n\n  \nBuilding a RAG pipeline\n    \n\n      \nIngestion\n        \n\n          \nChunking\n            \n\n              \nFiguring out the ideal chunk size\n                \n\n                  \nRetriever Ensembling and Reranking\n\n                \n\n              \n\n            \n\n          \n\n          \nEmbeddings\n\n          \nNaive Chunking vs. Late Chunking vs. Late Interaction (ColBERT and ColPali)\n            \n\n              \nOverview\n\n              \nNaive/Vanilla Chunking\n                \n\n                  \nWhat is Naive/Vanilla Chunking?\n\n                  \nExample\n\n                  \nAdvantages and Limitations\n\n                \n\n              \n\n              \nLate Chunking\n                \n\n                  \nWhat is Late Chunking?\n\n                  \nHow Late Chunking Works\n\n                  \nExample\n\n                  \nAdvantages and Trade-offs\n\n                \n\n              \n\n              \nLate Interaction\n                \n\n                  \nWhat is Late Interaction?\n\n                  \nColBERT: Late Interaction in Practice\n\n                  \nMaxSim: A Key Component of ColBERT\n\n                  \nExample\n\n                  \nAdvantages and Trade-offs\n\n                \n\n              \n\n              \nColPali: Expanding to Multimodal Retrieval\n                \n\n                  \nExample\n\n                \n\n              \n\n              \nComparative Analysis\n\n            \n\n          \n\n          \nSentence Embeddings: The What and Why\n            \n\n              \nBackground: Differences compared to Token-Level Models like BERT\n\n              \nRelated: Training Process for Sentence Transformers vs. Token-Level Embedding Models\n\n              \nApplying Sentence Transformers for RAG\n\n            \n\n          \n\n        \n\n      \n\n      \nRetrieval\n        \n\n          \nStandard/Naive approach\n            \n\n              \nAdvantages\n\n              \nDisadvantages\n\n            \n\n          \n\n          \nSentence-Window Retrieval / Small-to-Large Retrieval\n            \n\n              \nAdvantages\n\n              \nDisadvantages\n\n            \n\n          \n\n          \nAuto-merging Retriever / Hierarchical Retriever\n            \n\n              \nAdvantages\n\n              \nDisadvantages\n\n            \n\n          \n\n          \nContextual Retrieval\n\n          \nUsing Approximate Nearest Neighbors (ANN) for Retrieval\n\n        \n\n      \n\n      \nRe-ranking\n        \n\n          \nNeural Re-rankers: Types and Architectures\n            \n\n              \nDomain-Specific Adaptations\n\n            \n\n          \n\n          \nInstruction-Following Re-ranking: Precision and Control in RAG\n\n        \n\n      \n\n      \nResponse Generation / Synthesis\n        \n\n          \nLost in the Middle: How Language Models Use Long Contexts\n\n          \nThe “Needle in a Haystack” Test\n\n        \n\n      \n\n    \n\n  \n\n  \nRAG in Multi-Turn Chatbots: Embedding Queries for Retrieval\n    \n\n      \nEmbedding the Latest User Turn Only\n\n      \nEmbedding Concatenated Recent Turns (Truncated Dialogue History)\n\n      \nEmbedding a Condensed or Summarized History\n\n      \nEmbedding Structured Dialogue State\n\n      \nTask-Optimized Embedding via Query Reformulation\n\n      \nBest Practices and Considerations\n\n    \n\n  \n\n  \nComponent-Wise Evaluation\n    \n\n      \nRetrieval Metrics\n        \n\n          \nContext Precision\n\n          \nContext Recall\n\n          \nContext Relevance\n\n        \n\n      \n\n      \nGeneration Metrics\n        \n\n          \nGroundedness (a.k.a. Faithfulness)\n\n          \nAnswer Relevance\n\n          \nAnswer Semantic Similarity\n\n          \nBLEU Score\n\n          \nROUGE Score\n\n          \nString Presence\n\n          \nExact Match\n\n          \nContext Entities Recall\n\n        \n\n      \n\n    \n\n  \n\n  \nMultimodal Input Handling\n    \n\n      \nFlow of Multimodal Input\n\n      \nBenefits of Multimodal Embeddings in RAG\n\n    \n\n  \n\n  \nMultimodal RAG\n\n  \nAgentic Retrieval-Augmented Generation\n    \n\n      \nHow Agentic RAG Works\n\n      \nAgentic Decision-Making in Retrieval\n\n      \nAgentic RAG Architectures: Single-Agent vs. Multi-Agent Systems\n        \n\n          \nSingle-Agent RAG (Router)\n\n          \nMulti-Agent RAG Systems\n\n        \n\n      \n\n      \nBeyond Retrieval: Expanding Agentic RAG’s Capabilities\n\n      \nAgentic RAG vs. Vanilla RAG: Key Differences\n\n      \nImplementing Agentic RAG: Key Approaches\n        \n\n          \nLanguage Models with Function Calling\n\n          \nAgent Frameworks\n\n        \n\n      \n\n      \nEnterprise-driven Adoption\n\n      \nBenefits\n\n      \nLimitations\n\n      \nCode\n        \n\n          \nImplementing Agentic RAG with Function Calling\n            \n\n              \nDefine the Function for Retrieval\n\n              \nDefine the Tools Schema\n\n              \nSetting Up the Interaction Loop\n\n              \nExecuting the Agentic RAG Query\n\n            \n\n          \n\n          \nImplementing Agentic RAG with Agent Frameworks\n            \n\n              \nStep 1: Define Agents and Tools\n\n              \nStep 2: Configure Agent Routing\n\n              \nStep 3: Chain Agents for Multi-Agent RAG\n\n              \nRunning the Multi-Agent Query\n\n            \n\n          \n\n        \n\n      \n\n      \nDisadvantages of Agentic RAG\n\n      \nSummary\n\n    \n\n  \n\n  \nRAG vs. Long Context Windows\n    \n\n      \nComputational Cost\n\n      \nInference Latency and Throughput\n\n      \nContextual Comprehension and Model Training Limitations\n\n      \nRAG as a Targeted, Cost-Efficient Solution\n\n    \n\n  \n\n  \nImproving RAG Systems\n\n  \nRAG 2.0\n\n  \nSelected Papers\n    \n\n      \nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n      \nActive Retrieval Augmented Generation\n\n      \nMuRAG: Multimodal Retrieval-Augmented Generator\n\n      \nHypothetical Document Embeddings (HyDE)\n\n      \nRAGAS: Automated Evaluation of Retrieval Augmented Generation\n\n      \nFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\n\n      \nDense X Retrieval: What Retrieval Granularity Should We Use?\n\n      \nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n\n      \nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n      \nRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n\n      \nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n      \nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n\n      \nRAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture\n\n      \nRAFT: Adapting Language Model to Domain Specific RAG\n\n      \nCorrective Retrieval Augmented Generation\n\n      \nFine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\n\n      \nHGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation\n\n      \nHow faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior\n\n      \nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\n\n      \nRichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation\n\n      \nHiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA\n\n    \n\n  \n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \nRetrieval-Augmented Generation (RAG) is an advanced technique designed to enhance the output of Language Models (LMs) by incorporating external knowledge sources.\n\n  \nRAG is achieved by retrieving relevant information from a large corpus of documents and utilizing that information to guide and inform the generative process of the model. The subsequent sections provide a detailed examination of this methodology.\n\n\n\n\n\nMotivation\n\n\n\n\n  \nIn many real-world scenarios, organizations maintain extensive collections of proprietary documents, such as technical manuals, from which precise information must be extracted. This challenge is often analogous to locating a needle in a haystack, given the sheer volume and complexity of the content.\n\n  \nWhile recent advancements, such as OpenAI’s introduction of GPT-4 Turbo, offer improved capabilities for processing lengthy documents, they are not without limitations. Notably, these models exhibit a tendency known as the “Lost in the Middle” phenomenon, wherein information positioned near the center of the context window is more likely to be overlooked or forgotten. This issue is akin to reading a comprehensive text such as the Bible, yet struggling to recall specific content from its middle chapters.\n\n  \nTo address this shortcoming, the RAG approach has been introduced. This method involves segmenting documents into discrete units—typically paragraphs—and creating an index for each. Upon receiving a query, the system efficiently identifies and retrieves the most relevant segments, which are then supplied to the language model. By narrowing the input to only the most pertinent information, this strategy mitigates cognitive overload within the model and substantially improves the relevance and accuracy of its responses.\n\n\n\n\n\nLexical Retrieval\n\n\n\n\n  \n\n    \nLexical retrieval is the traditional approach to information retrieval based on exact word matches and term frequency. Two commonly used methods in this category are TF-IDF and BM25.\n\n\n    \n\n      \nTF-IDF (Term Frequency-Inverse Document Frequency):\n\n        \n\n          \nTF-IDF evaluates the importance of a word in a document relative to a corpus. It increases proportionally with the number of times a word appears in the document but is offset by how frequently the word appears across all documents.\n\n          \nWhile TF-IDF is simple and effective for many scenarios, it does not take into account the saturation of term frequency and lacks the ability to differentiate between rare and common words beyond the basic IDF scaling.\n\n        \n\n      \n\n      \nBM25 (Best Matching 25):\n\n        \n\n          \nBM25 is a more refined version of TF-IDF. It introduces term frequency saturation and document length normalization, improving relevance scoring.\n\n          \nOne of the key advantages of BM25 over TF-IDF is its ability to handle multiple occurrences of a term in a more nuanced way. It prevents overly frequent terms from dominating the score, making retrieval results more balanced.\n\n          \nBM25 also scales better with document length, giving fair chances to both short and long documents.\n\n        \n\n      \n\n    \n\n  \n\n  \nAdvantages of Lexical Retrieval:\n\n    \n\n      \nFast and computationally efficient.\n\n      \nEasy to interpret and implement.\n\n      \nWorks well when exact keyword matching is important.\n\n    \n\n  \n\n  \nLimitations:\n\n    \n\n      \nCannot handle synonyms or paraphrased queries effectively.\n\n      \nLimited ability to capture semantic meaning.\n\n    \n\n  \n\n\n\n\n\nSemantic Retrieval\n\n\n\n\n  \n\n    \nSemantic retrieval, previously referred to as neural retrieval, is a more recent approach that relies on machine learning models to understand the meaning behind queries and documents.\n\n  \n\n  \n\n    \nThese systems use neural networks to embed both queries and documents into a shared vector space, where semantic similarity can be calculated using metrics like cosine similarity.\n\n  \n\n  \nHow it works:\n\n    \n\n      \nVector Encoding:\n        \n\n          \nBoth queries and documents are transformed into dense vectors using pre-trained or fine-tuned encoders.\n\n          \nThese encoders are typically trained on large datasets, enabling them to capture semantic nuances beyond surface-level keyword overlap.\n\n        \n\n      \n\n      \nSemantic Matching:\n        \n\n          \nVectors are compared to identify the most semantically relevant documents, even if they don’t share explicit terms with the query.\n\n        \n\n      \n\n    \n\n  \n\n  \nAdvantages of Semantic Retrieval:\n\n    \n\n      \nHandles paraphrasing, synonyms, and conceptual similarity effectively.\n\n      \nSupports more natural and conversational queries.\n\n      \nMultilingual capabilities are often built-in.\n\n    \n\n  \n\n  \nChallenges and Considerations:\n\n    \n\n      \nRequires significant computational resources.\n\n      \nRetrieval quality is sensitive to training data and may reflect biases.\n\n      \nUpdating document embeddings for dynamic content can be complex.\n\n    \n\n  \n\n\n\n\n\nHybrid Retrieval (Lexical + Semantic)\n\n\n\n\n  \n\n    \nA hybrid retrieval system combines the strengths of lexical and semantic methods to deliver more accurate and robust results.\n\n  \n\n  \n\n    \nOne popular technique for hybrid retrieval is Reciprocal Rank Fusion (RRF). RRF merges the rankings from different retrieval models (e.g., BM25 and a neural retriever) by assigning higher scores to documents that consistently rank well across systems.\n\n  \n\n  \nHow RRF works:\n\n    \n\n      \nEach document receives a score based on its position in the ranked lists from multiple retrieval methods.\n\n      \nThe scores are combined using the following reciprocal formula:\n\n    \n\n\n\\[\\text{RRF Score}(d) = \\sum_{i=1}^{n} \\frac{1}{k + \\text{rank}_i(d)}\\]\n\n    \nwhere:\n\n    \n\n      \n\\(d\\) is the document,\n\n      \n\\(\\text{rank}_i(d)\\) is the rank position of document \\(d\\) in the \\(i^{\\text{th}}\\) ranked list,\n\n      \n\\(k\\) is a constant (typically set to 60),\n\n      \n\\(n\\) is the number of retrieval systems.\n\n    \n\n  \n\n  \nExample:\n\n    \n\n      \n\n        \nSuppose two retrieval systems return the following top-5 rankings for a given query:\n\n      \n\n      \nBM25:\n [DocA, DocB, DocC, DocD, DocE]\n\n      \n\n        \nNeural Retriever:\n [DocF, DocC, DocA, DocG, DocB]\n\n      \n\n      \n\n        \nRRF scores are calculated as follows:\n\n\n        \n\n          \n\n            \nFor DocA (rank 1 in BM25, rank 3 in Neural Retriever):\n\n\n\\[\\text{RRF Score}(\\text{DocA}) = \\frac{1}{60 + 1} + \\frac{1}{60 + 3}\\]\n          \n\n          \n\n            \nFor DocC (rank 3 in BM25, rank 2 in Neural Retriever):\n\n\n\\[\\text{RRF Score}(\\text{DocC}) = \\frac{1}{60 + 3} + \\frac{1}{60 + 2}\\]\n          \n\n          \n\n            \nFor DocB (rank 2 in BM25, rank 5 in Neural Retriever):\n\n\n\\[\\text{RRF Score}(\\text{DocB}) = \\frac{1}{60 + 2} + \\frac{1}{60 + 5}\\]\n          \n\n          \n\n            \nAfter computing scores for all documents, the final RRF ranking is determined by sorting them in descending order of their cumulative scores.\n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \nBenefits of Hybrid Retrieval:\n\n    \n\n      \nIncreases recall by retrieving relevant documents that either lexical or semantic methods might miss individually.\n\n      \nBalances precision and coverage.\n\n      \nMakes the retrieval system more resilient to query variations and noise.\n\n    \n\n  \n\n\n\n\n\nThe Retrieval Augmented Generation (RAG) Pipeline\n\n\n\n\n  \nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases.\n\n  \nIt leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.\n\n  \nThe retriever here could be any of the following depending on the need for semantic retrieval or not:\n    \n\n      \nVector database:\n Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.\n\n      \nGraph database:\n Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.\n\n      \nRegular SQL database:\n Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.\n\n    \n\n  \n\n  \nThe image below from \nDamien Benveniste, PhD\n talks a bit about the difference between using Graph vs Vector database for RAG.\n\n\n\n\n\n\n\n\n\n  \nIn his post linked above, Damien states that Graph Databases are favored for Retrieval Augmented Generation (RAG) when compared to Vector Databases. While Vector Databases partition and index data using LLM-encoded vectors, allowing for semantically similar vector retrieval, they may fetch irrelevant data.\n\n  \nGraph Databases, on the other hand, build a knowledge base from extracted entity relationships in the text, making retrievals concise. However, it requires exact query matching which can be limiting.\n\n  \n\n    \nA potential solution could be to combine the strengths of both databases: indexing parsed entity relationships with vector representations in a graph database for more flexible information retrieval. It remains to be seen if such a hybrid model exists.\n\n  \n\n  \nAfter retrieving, you may want to look into filtering the candidates further by adding ranking and/or fine ranking layers that allow you to filter down candidates that do not match your business rules, are not personalized for the user, current context, or response limit.\n\n  \nLet’s succinctly summarize the process of RAG and then delve into its pros and cons:\n    \n\n      \nVector Database Creation\n: RAG starts by converting an internal dataset into vectors and storing them in a vector database (or a database of your choosing).\n\n      \nUser Input\n: A user provides a query in natural language, seeking an answer or completion.\n\n      \nInformation Retrieval\n: The retrieval mechanism scans the vector database to identify segments that are semantically similar to the user’s query (which is also embedded). These segments are then given to the LLM to enrich its context for generating responses.\n\n      \nCombining Data\n: The chosen data segments from the database are combined with the user’s initial query, creating an expanded prompt.\n\n      \nGenerating Text\n: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-aware response.\n\n    \n\n  \n\n  \nThe image below \n(source)\n displays the high-level working of RAG.\n\n\n\n\n\n\nBenefits of RAG\n\n\n\n\n  \nSo why should you use RAG for your application?\n    \n\n      \nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge bases.\n\n      \nRAG doesn’t require model retraining, saving time and computational resources.\n\n      \nIt’s effective even with a limited amount of labeled data.\n\n      \nHowever, it does have its drawbacks, namely RAG’s performance depends on the comprehensiveness and correctness of the retriever’s knowledge base.\n\n      \nRAG is best suited for scenarios with abundant unlabeled data but scarce labeled data and is ideal for applications like virtual assistants needing real-time access to specific information like product manuals.\n        \n\n          \nScenarios with abundant unlabeled data but scarce labeled data: RAG is useful in situations where there is a lot of data available, but most of it is not categorized or labeled in a way that’s useful for training models. As an example, the internet has vast amounts of text, but most of it isn’t organized in a way that directly answers specific questions.\n\n          \nFurthermore, RAG is ideal for applications like virtual assistants: Virtual assistants, like Siri or Alexa, need to pull information from a wide range of sources to answer questions in real-time. They need to understand the question, retrieve relevant information, and then generate a coherent and accurate response.\n\n          \nNeeding real-time access to specific information like product manuals: This is an example of a situation where RAG models are particularly useful. Imagine you ask a virtual assistant a specific question about a product, like “How do I reset my XYZ brand thermostat?” The RAG model would first retrieve relevant information from product manuals or other resources, and then use that information to generate a clear, concise answer.\n\n          \nIn summary, RAG models are well-suited for applications where there’s a lot of information available, but it’s not neatly organized or labeled.\n\n        \n\n      \n\n    \n\n  \n\n  \nBelow, let’s take a look at the publication that introduced RAG and how the original paper implemented the framework.\n\n\n\n\n\nRAG vs. Fine-tuning\n\n\n\n\n  \nThe table below \n(source)\n compares RAG vs. fine-tuning.\n\n\n\n\n\n\n\n\n\n  \nTo summarize the above table:\n    \n\n      \nRAG offers Large Language Models (LLMs) access to factual, access-controlled, timely information. This integration enables LLMs to fetch precise and verified facts directly from relevant databases and knowledge repositories in real-time. While fine-tuning can address some of these aspects by adapting the model to specific data, RAG excels at providing up-to-date and specific information without the substantial costs associated with fine-tuning. Moreover, RAG enhances the model’s ability to remain current and relevant by dynamically accessing and retrieving the latest data, thus ensuring the responses are accurate and contextually appropriate. Additionally, RAG’s approach to leveraging external sources can be more flexible and scalable, allowing for easy updates and adjustments without the need for extensive retraining.\n\n      \nFine-tuning adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style. RAG does not provide this level of customization in terms of linguistic style and vocabulary.\n\n      \nFocus on RAG first. A successful LLM application typically involves connecting specialized data to the LLM workflow. Once you have a functional application, you can add fine-tuning to enhance the style and vocabulary of the system.\n\n    \n\n  \n\n\n\n\n\nEnsemble of RAG\n\n\n\n\n  \nLeveraging an ensemble of RAG systems offers a substantial upgrade to the model’s ability to produce rich and contextually accurate text. Here’s an enhanced breakdown of how this procedure could work:\n    \n\n      \nKnowledge sources:\n RAG models retrieve information from external knowledge stores to augment their knowledge in a particular domain. These can include passages, tables, images, etc. from domains like Wikipedia, books, news, databases.\n\n      \nCombining sources:\n At inference time, multiple retrievers can pull relevant content from different corpora. For example, one retriever searches Wikipedia, another searches news sources. Their results are concatenated into a pooled set of candidates.\n\n      \nRanking:\n The model ranks the pooled candidates by their relevance to the context.\n\n      \nSelection:\n Highly ranked candidates are selected to condition the language model for generation.\n\n      \nEnsembling:\n Separate RAG models specialized on different corpora can be ensembled. Their outputs are merged, ranked, and voted on.\n\n    \n\n  \n\n  \nMultiple knowledge sources can augment RAG models through pooling and ensembles. Careful ranking and selection helps integrate these diverse sources for improved generation.\n\n  \nOne thing to keep in mind when using multiple retrievers is to rank the different outputs from each retriever before merging them to form a response. This can be done in a variety of ways, using LTR algorithms, multi-armed bandit framework, multi-objective optimization, or according to specific business use cases.\n\n\n\n\n\nChoosing a Vector DB using a Feature Matrix\n\n\n\n\n  \nTo compare the plethora of Vector DB offerings, a feature matrix that highlights the differences between Vector DBs and which to use in which scenario is essential.\n\n  \nVector DB Comparison by VectorHub\n offers a great comparison spanning 37 vendors and 29 features (as of this writing).\n\n\n\n\n\n\n\n\n\n  \nAs a secondary resource, the following table (\nsource\n) shows a comparison of some of the prevalent Vector DB offers along various feature dimensions:\n\n\n\n\n\n\n\n\n\n  \nAccess the full spreadsheet \nhere\n.\n\n\n\n\n\nBuilding a RAG pipeline\n\n\n\n\n  \nThe image below \n(source)\n, gives a visual overview of the three different steps of RAG: Ingestion, Retrieval, and Synthesis/Response Generation.\n\n\n\n\n\n\n\n\n\n  \nIn the sections below, we will go over these key areas.\n\n\n\n\n\nIngestion\n\n\n\nChunking\n\n\n\n\n  \nChunking is the process of dividing the prompts and/or the documents to be retrieved, into smaller, manageable segments or chunks. These chunks can be defined either by a fixed size, such as a specific number of characters, sentences, or paragraphs. The choice of chunking strategy plays a critical role in determining both the performance and efficiency of your RAG system.\n\n  \nEach chunk is encoded into an embedding vector for retrieval. Smaller, more precise chunks lead to a finer match between the user’s query and the content, enhancing the accuracy and relevance of the information retrieved.\n\n  \nLarger chunks might include irrelevant information, introducing noise and potentially reducing the retrieval accuracy. By controlling the chunk size, RAG can maintain a balance between comprehensiveness and precision.\n\n  \nSo the next natural question that comes up is, how do you choose the right chunk size for your use case? The choice of chunk size in RAG is crucial. It needs to be small enough to ensure relevance and reduce noise but large enough to maintain the context’s integrity. Let’s look at a few methods below referred from \nPinecone\n:\n    \n\n      \nFixed-size Chunking:\n Simply decide the number of tokens in our chunk along with whether there should be overlap between them or not. Overlap between chunks guarantees there to be minimal semantic context loss between chunks. This option is computationally cheap and simple to implement.\n        \ntext\n \n=\n \n\"...\"\n \n# your text\n\nfrom\n \nlangchain.text_splitter\n \nimport\n \nCharacterTextSplitter\n\n\ntext_splitter\n \n=\n \nCharacterTextSplitter\n(\n\n    \nseparator\n \n=\n \n\"\n\\n\\n\n\"\n,\n\n    \nchunk_size\n \n=\n \n256\n,\n\n    \nchunk_overlap\n  \n=\n \n20\n\n\n)\n\n\ndocs\n \n=\n \ntext_splitter\n.\ncreate_documents\n([\ntext\n])\n\n\n        \n\n      \n\n      \nContext-aware Chunking:\n Content-aware chunking leverages the intrinsic structure of the text to create chunks that are more meaningful and contextually relevant. Here are several approaches to achieving this:\n        \n\n          \nSentence Splitting\n: This method aligns with models optimized for embedding sentence-level content. Different tools and techniques can be used for sentence splitting:\n            \n\n              \nNaive Splitting:\n A basic method where sentences are split using periods and new lines. Example:\n                \n   \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\n   \ndocs\n \n=\n \ntext\n.\nsplit\n(\n\".\"\n)\n\n\n                \n\n                \n\n                  \nThis method is quick but may overlook complex sentence structures.\n\n                \n\n              \n\n              \nNLTK (Natural Language Toolkit):\n A comprehensive Python library for language processing. NLTK includes a sentence tokenizer that effectively splits text into sentences. Example:\n                \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\nfrom\n \nlangchain.text_splitter\n \nimport\n \nNLTKTextSplitter\n\n\ntext_splitter\n \n=\n \nNLTKTextSplitter\n()\n\n\ndocs\n \n=\n \ntext_splitter\n.\nsplit_text\n(\ntext\n)\n\n\n                \n\n              \n\n              \nspaCy:\n An advanced Python library for NLP tasks, spaCy offers efficient sentence segmentation. Example:\n                \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\nfrom\n \nlangchain.text_splitter\n \nimport\n \nSpacyTextSplitter\n\n\ntext_splitter\n \n=\n \nSpacyTextSplitter\n()\n\n\ndocs\n \n=\n \ntext_splitter\n.\nsplit_text\n(\ntext\n)\n\n\n                \n\n              \n\n            \n\n          \n\n          \nRecursive Chunking:\n Recursive chunking is an iterative method that splits text hierarchically using various separators. It adapts to create chunks of similar size or structure by recursively applying different criteria. Example using LangChain:\n            \n   \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\n   \nfrom\n \nlangchain.text_splitter\n \nimport\n \nRecursiveCharacterTextSplitter\n\n   \ntext_splitter\n \n=\n \nRecursiveCharacterTextSplitter\n(\n\n       \nchunk_size\n \n=\n \n256\n,\n\n       \nchunk_overlap\n \n=\n \n20\n\n   \n)\n\n   \ndocs\n \n=\n \ntext_splitter\n.\ncreate_documents\n([\ntext\n])\n\n\n            \n\n          \n\n          \nStructure-based Chunking:\n For formatted content like Markdown or LaTeX, specialized chunking can be applied to maintain the original structure:\n            \n\n              \nMarkdown Chunking:\n Recognizes Markdown syntax and divides content based on structure. Example:\n                \nfrom\n \nlangchain.text_splitter\n \nimport\n \nMarkdownTextSplitter\n\n\nmarkdown_text\n \n=\n \n\"...\"\n\n\nmarkdown_splitter\n \n=\n \nMarkdownTextSplitter\n(\nchunk_size\n=\n100\n,\n \nchunk_overlap\n=\n0\n)\n\n\ndocs\n \n=\n \nmarkdown_splitter\n.\ncreate_documents\n([\nmarkdown_text\n])\n\n\n                \n\n              \n\n              \nLaTeX Chunking:\n Parses LaTeX commands and environments to chunk content while preserving its logical organization.\n\n            \n\n          \n\n          \nSemantic Chunking:\n Segment text based on semantic similarity. This means that sentences with the strongest semantic connections are grouped together, while sentences that move to another topic or theme are separated into distinct chunks. \nNotebook\n.\n            \n\n              \nSemantic chunking can be summarized in four steps:\n                \n\n                  \nSplit the text into sentences, paragraphs, or other rule-based units.\n\n                  \nVectorize a window of sentences or other units.\n\n                  \nCalculate the cosine distance between the embedded windows.\n\n                  \nMerge sentences or units until the cosine similarity value reaches a specific threshold.\n\n                \n\n              \n\n              \nThe following figure (\nsource\n) visually summarizes the overall process:\n \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \n“As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.” \n(source)\n\n\n\n\n\nFiguring out the ideal chunk size\n\n\n\n\n  \nChoosing the right chunk size is foundational to building an effective RAG system. It directly influences retrieval quality, model efficiency, and how well the system captures relevant context for downstream tasks. Poor chunking can lead to fragmented information or excessive context loss, undermining overall performance.\n\n  \n\n    \nBuilding a RAG system involves determining the ideal chunk sizes for the documents that the retriever component will process. The ideal chunk size depends on several factors:\n\n\n    \n\n      \n\n        \nData Characteristics\n: The nature of your data is crucial. For text documents, consider the average length of paragraphs or sections. If the documents are well-structured with distinct sections, these natural divisions might serve as a good basis for chunking.\n\n      \n\n      \n\n        \nRetriever Constraints\n: The retriever model you choose (like BM25, TF-IDF, or a neural retriever like DPR) might have limitations on the input length. It’s essential to ensure that the chunks are compatible with these constraints.\n\n      \n\n      \n\n        \nMemory and Computational Resources\n: Larger chunk sizes can lead to higher memory usage and computational overhead. Balance the chunk size with the available resources to ensure efficient processing.\n\n      \n\n      \n\n        \nTask Requirements\n: The nature of the task (e.g., question answering, document summarization) can influence the ideal chunk size. For detailed tasks, smaller chunks might be more effective to capture specific details, while broader tasks might benefit from larger chunks to capture more context.\n\n      \n\n      \n\n        \nExperimentation\n: Often, the best way to determine the ideal chunk size is through empirical testing. Run experiments with different chunk sizes and evaluate the performance on a validation set to find the optimal balance between granularity and context.\n\n      \n\n      \n\n        \nOverlap Consideration\n: Sometimes, it’s beneficial to have overlap between chunks to ensure that no important information is missed at the boundaries. Decide on an appropriate overlap size based on the task and data characteristics.\n\n      \n\n    \n\n  \n\n  \nTo summarize, determining the ideal chunk size for a RAG system is a balancing act that involves considering the characteristics of your data, the limitations of your retriever model, the resources at your disposal, the specific requirements of your task, and empirical experimentation. It’s a process that may require iteration and fine-tuning to achieve the best results.\n\n\n\n\n\nRetriever Ensembling and Reranking\n\n\n\n\n  \nIn some scenarios, it may be beneficial to simultaneously utilize multiple chunk sizes and apply a re-ranking mechanism to refine the retrieved results. A detailed discourse on re-ranking is available in the \nRe-ranking\n section.\n\n  \nThis approach serves two primary purposes:\n    \n\n      \nIt potentially improves the quality of retrieved content—albeit at increased computational cost—by aggregating outputs from multiple chunking strategies, provided the re-ranker performs with a reasonable degree of accuracy.\n\n      \nIt enables systematic comparison of different retrieval methods relative to the re-ranker’s effectiveness.\n\n    \n\n  \n\n  \n\n    \nThe methodology proceeds as follows:\n\n\n    \n\n      \nSegment the same source document using various chunk sizes, for example: 128, 256, 512, and 1024 tokens.\n\n      \nDuring the retrieval phase, extract relevant segments from each retrieval method, thereby forming an ensemble of retrievers.\n\n      \nApply a re-ranking model to prioritize and filter the aggregated results.\n\n    \n\n  \n\n  \nThe following diagram \n(source)\n illustrates the process.\n\n\n\n\n\n\n\n\n\n  \nAccording to \nevaluation data provided by LlamaIndex\n, the ensemble retrieval strategy leads to a modest improvement in faithfulness metrics, suggesting slightly enhanced relevance of retrieved content. However, pairwise comparisons show equal preference between the ensembled and baseline approaches, thereby leaving the superiority of ensembling open to debate.\n\n  \nIt is important to note that this ensembling methodology is not limited to variations in chunk size. It can also be extended to other dimensions of a RAG pipeline, including vector-based, keyword-based, and hybrid search strategies.\n\n\n\n\n\nEmbeddings\n\n\n\n\n  \nOnce you have your prompt chunked appropriately, the next step is to embed it. Embedding prompts and documents in RAG involves transforming both the user’s query (prompt) and the documents in the knowledge base into a format that can be effectively compared for relevance. This process is critical for RAG’s ability to retrieve the most relevant information from its knowledge base in response to a user query. Here’s how it typically works:\n\n  \nOne option to help pick which embedding model would be best suited for your task is to look at \nHuggingFace’s Massive Text Embedding Benchmark (MTEB) leaderboard\n. There is a question of whether a dense or sparse embedding can be used so let’s look into benefits of each below:\n\n  \nSparse embedding:\n Sparse embeddings such as TF-IDF are great for lexical matching the prompt with the documents. Best for applications where keyword relevance is crucial. It’s computationally less intensive but may not capture the deeper semantic meanings in the text.\n\n  \nSemantic embedding:\n Semantic embeddings, such as BERT or SentenceBERT lend themselves naturally to the RAG use-case.\n    \n\n      \nBERT:\n Suitable for capturing contextual nuances in both the documents and queries. Requires more computational resources compared to sparse embeddings but offers more semantically rich embeddings.\n\n      \nSentenceBERT:\n Ideal for scenarios where the context and meaning at the sentence level are important. It strikes a balance between the deep contextual understanding of BERT and the need for concise, meaningful sentence representations. This is usually the preferred route for RAG.\n\n    \n\n  \n\n\n\n\n\nNaive Chunking vs. Late Chunking vs. Late Interaction (\nColBERT\n and \nColPali\n)\n\n\n\n\n  \n\n    \nThe choice between naive chunking, late chunking, and late interaction (\nColBERT\n/\nColPali\n) depends on the specific requirements of the retrieval task:\n\n\n    \n\n      \nNaive Chunking\n is suitable for scenarios with strict resource constraints but where retrieval precision is less critical.\n\n      \nLate Chunking\n, introduced by \nJinaAI\n, offers an attractive middle ground, maintaining context and providing improved retrieval accuracy without incurring significant additional costs. Put simply, late chunking balances the trade-offs between cost and precision, making it an excellent option for building scalable and effective RAG systems, particularly in long-context retrieval scenarios.\n\n      \nLate Interaction (\nColBERT\n/\nColPali\n)\n is best suited for applications where retrieval precision is paramount and resource costs are less of a concern.\n\n    \n\n  \n\n  \n\n    \nLet’s explore the differences between three primary strategies: Naive Chunking, Late Chunking, and Late Interaction (\nColBERT\n and \nColPali\n), focusing on their methodologies, advantages, and trade-offs.\n\n  \n\n\n\n\n\nOverview\n\n\n\n\n  \nLong-context retrieval presents a challenge when balancing precision, context retention, and cost efficiency. Solutions range from simple and low-cost, like Naive Chunking, to more sophisticated and resource-intensive approaches, such as \nLate Interaction ([ColBERT](https://arxiv.org/abs/2004.12832))\n. \nLate Chunking\n, a novel approach by \nJinaAI\n, offers a middle ground, preserving context with efficiency comparable to Naive Chunking.\n\n\n\n\n\n\n\n\nNaive/Vanilla Chunking\n\n\n\nWhat is Naive/Vanilla Chunking?\n\n\n\n\n  \nAs discussed in the \nChunking\n section, naive/vanilla chunking divides a document into fixed-size chunks based on metrics like sentence boundaries or token count (e.g., 512 tokens per chunk).\n\n  \nEach chunk is independently embedded into a vector without considering the context of neighboring chunks.\n\n\n\n\n\nExample\n\n\n\n\n  \n\n    \nConsider the following paragraph: \nAlice went for a walk in the woods one day and on her walk, she spotted something. She saw a rabbit hole at the base of a large tree. She fell into the hole and found herself in a strange new world.\n\n  \n\n  \n\n    \nIf chunked by sentences:\n\n    \n\n      \nChunk 1\n: “Alice went for a walk in the woods one day and on her walk, she spotted something.”\n\n      \nChunk 2\n: “She saw a rabbit hole at the base of a large tree.”\n\n      \nChunk 3\n: “She fell into the hole and found herself in a strange new world.”\n\n    \n\n  \n\n\n\n\n\nAdvantages and Limitations\n\n\n\n\n  \nAdvantages\n:\n    \n\n      \nEfficient in terms of storage and computation.\n\n      \nSimple to implement and integrate with most retrieval pipelines.\n\n    \n\n  \n\n  \nLimitations\n:\n    \n\n      \nContext Loss\n: Each chunk is processed independently, leading to a loss of contextual relationships. For example, the connection between “she” and “Alice” would be lost, reducing retrieval accuracy for context-heavy queries like “Where did Alice fall?”.\n\n      \nFragmented Meaning\n: Splitting paragraphs or semantically related sections can dilute the meaning of each chunk, reducing retrieval precision.\n\n    \n\n  \n\n\n\n\n\nLate Chunking\n\n\n\nWhat is Late Chunking?\n\n\n\n\n  \nLate Chunking flips the order of vectorizing (i.e., embedding generation) and chunking compared to naive/vanilla chunking. In other words, it delays the chunking process until after the entire document has been embedded into token-level representations. This allows chunks to retain context from the full document, leading to richer, more contextually aware embeddings.\n\n\n\n\n\nHow Late Chunking Works\n\n\n\n\n  \nEmbedding First\n: The entire document is embedded into token-level representations using a long context model.\n\n  \nChunking After\n: After embedding, the token-level representations are pooled into chunks based on a predefined chunking strategy (e.g., 512-token chunks).\n\n  \nContext Retention\n: Each chunk retains contextual information from the full document, allowing for improved retrieval precision without increasing storage costs.\n\n\n\n\n\nExample\n\n\n\n\n  \nUsing the same paragraph:\n    \n\n      \nThe entire paragraph is first embedded as a whole, preserving the relationships between all sentences.\n\n      \nThe document is then split into chunks after embedding, ensuring that chunks like “She fell into the hole…” are contextually aware of the mention of “Alice” from earlier sentences.\n\n    \n\n  \n\n\n\n\n\nAdvantages and Trade-offs\n\n\n\n\n  \nAdvantages\n:\n    \n\n      \nContext Preservation\n: Late chunking ensures that the relationship between tokens across different chunks is maintained.\n\n      \nEfficiency\n: Late chunking requires the same amount of storage as naive chunking while significantly improving retrieval accuracy.\n\n    \n\n  \n\n  \nTrade-offs\n:\n    \n\n      \nRequires Long Context Models\n: To embed the entire document at once, a model with long-context capabilities (e.g., supporting up to 8192 tokens) is necessary.\n\n      \nSlightly Higher Compute Costs\n: Late chunking introduces an extra pooling step after embedding, although it’s more efficient than late interaction approaches like \nColBERT\n.\n\n    \n\n  \n\n\n\n\n\nLate Interaction\n\n\n\nWhat is Late Interaction?\n\n\n\n\n  \nLate Interaction refers to a retrieval approach where token embeddings for both the document and the query are computed separately and compared at the token level, without any pooling operation. The key advantage is fine-grained, token-level matching, which improves retrieval accuracy.\n\n\n\n\n\nColBERT\n: Late Interaction in Practice\n\n\n\n\n  \nColBERT\n (Contextualized Late Interaction over BERT) by Khattab et al. (2020) uses late interaction to compare individual token embeddings from the query and document using a MaxSim operator. This allows for granular, token-to-token comparisons, which results in highly precise matches but at a significantly higher storage cost.\n\n\n\n\n\nMaxSim: A Key Component of \nColBERT\n\n\n\n\n  \nMaxSim\n (Maximum Similarity) is a core component of the \nColBERT\n retrieval framework. It refers to a specific way of calculating the similarity between token embeddings of a query and document during retrieval.\n\n  \nHere’s a step-by-step breakdown of how MaxSim works:\n    \n\n      \nToken-level Embedding Comparisons\n:\n        \n\n          \nWhen a query is processed, it is tokenized and each token is embedded separately (e.g., “apple” and “sweet”).\n\n          \nThe document is already indexed at the token level, meaning that each token in the document also has its own embedding.\n\n        \n\n      \n\n      \nSimilarity Computation\n:\n        \n\n          \nAt query time, the system compares each query token embedding to every token embedding in the document. The similarity between two token embeddings is often measured using a dot product or cosine similarity.\n\n          \nFor example, given a query token \n\"apple\"\n and a document containing tokens like \n\"apple\"\n, \n\"banana\"\n, and \n\"fruit\"\n, the system computes the similarity of \n\"apple\"\n to each of these tokens.\n\n        \n\n      \n\n      \nSelecting Maximum Similarity (MaxSim)\n:\n        \n\n          \nThe system selects the highest similarity score between the query token and the document tokens. This is known as the MaxSim operation.\n\n          \nIn the above example, the system compares the similarity of \n\"apple\"\n (query token) with all document tokens and selects the highest similarity score, say between \n\"apple\"\n and the corresponding token \n\"apple\"\n in the document.\n\n        \n\n      \n\n      \nMaxSim Aggregation\n:\n        \n\n          \nThe MaxSim scores for each token in the query are aggregated (usually summed) to calculate a final relevance score for the document with respect to the query.\n\n        \n\n        \n\n          \nThis approach allows for token-level precision, capturing subtle nuances in the document-query matching that would be lost with traditional pooling methods.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nExample\n\n\n\n\n  \n\n    \nConsider the query \n\"sweet apple\"\n and two documents:\n\n\n    \n\n      \nDocument 1\n: “The apple is sweet and crisp.”\n\n      \nDocument 2\n: “The banana is ripe and yellow.”\n\n    \n\n  \n\n  \n\n    \nEach query token, \n\"sweet\"\n and \n\"apple\"\n, is compared with every token in both documents:\n\n\n    \n\n      \nFor \nDocument 1\n, \n\"sweet\"\n has a high similarity with \n\"sweet\"\n in the document, and \n\"apple\"\n has a high similarity with \n\"apple\"\n.\n\n      \nFor \nDocument 2\n, \n\"sweet\"\n does not have a strong match with any token, and \n\"apple\"\n does not appear.\n\n    \n\n  \n\n  \n\n    \nUsing MaxSim, Document 1 would have a higher relevance score for the query than Document 2 because the most similar tokens in Document 1 (i.e., \n\"sweet\"\n and \n\"apple\"\n) align more closely with the query tokens.\n\n  \n\n\n\n\n\nAdvantages and Trade-offs\n\n\n\n\n  \nAdvantages\n:\n    \n\n      \nHigh Precision\n: \nColBERT\n’s token-level comparisons, facilitated by MaxSim, lead to highly accurate retrieval, particularly for specific or complex queries.\n\n      \nFlexible Query Matching\n: By calculating similarity at the token level, \nColBERT\n can capture fine-grained relationships that simpler models might overlook.\n\n    \n\n  \n\n  \nTrade-offs\n:\n    \n\n      \nStorage Intensive\n: Storing all token embeddings for each document can be extremely costly. For example, storing token embeddings for a corpus of 100,000 documents could require upwards of 2.46 TB.\n\n      \nComputational Complexity\n: While precise, MaxSim increases computational demands at query time, as each token in the query must be compared to all tokens in the document.\n\n    \n\n  \n\n\n\n\n\nColPali\n: Expanding to Multimodal Retrieval\n\n\n\n\n  \nColPali\n by Faysse et al. (2024) integrates the late interaction mechanism from \nColBERT\n with a Vision Language Model (VLM) called PaliGemma to handle multimodal documents, such as PDFs with text, images, and tables. Instead of relying on OCR and layout parsing, \nColPali\n uses screenshots of PDF pages to directly embed both visual and textual content. This enables powerful multimodal retrieval in complex documents.\n\n\n\n\n\nExample\n\n\n\n\n  \nConsider a complex PDF with both text and images. \nColPali\n treats each page as an image and embeds it using a VLM. When a user queries the system, the query is matched with embedded screenshots via late interaction, improving the ability to retrieve relevant pages based on both visual and textual content.\n\n\n\n\n\n\n\n\nComparative Analysis\n\n\n\n\n\n\n \n\n\n\n\nMetric\n\n\nNaive Chunking\n\n\nLate Chunking\n\n\nLate Interaction ([ColBERT](https://arxiv.org/abs/2004.12832))\n\n\n\n\n\n\n\n\n\n\nStorage Requirements\n\n\nMinimal storage, ~4.9 GB for 100,000 documents\n\n\nSame as naive chunking, ~4.9 GB for 100,000 documents\n\n\nExtremely high storage, ~2.46 TB for 100,000 documents\n\n\n\n\n\n\nRetrieval Precision\n\n\nLower precision due to context fragmentation\n\n\nImproved precision by retaining context across chunks\n\n\nHighest precision with token-level matching\n\n\n\n\n\n\nComplexity and Cost\n\n\nSimple implementation, minimal resources\n\n\nModerately more complex, efficient in compute and storage\n\n\nHighly complex, resource-intensive in both storage and computation\n\n\n\n\n\n\n\n\n\n\n\nSentence Embeddings: The What and Why\n\n\n\nBackground: Differences compared to Token-Level Models like BERT\n\n\n\n\n  \nAs an overview, let’s look into how sentence transformers differ compared to token-level embedding models such as BERT.\n\n  \nSentence Transformers are a modification of the traditional BERT model, tailored specifically for generating embeddings of entire sentences (i.e., sentence embeddings). The key differences in their training approaches are:\n    \n\n      \nObjective\n: BERT is trained to predict masked words in a sentence and next sentence prediction. It’s optimized for understanding words and their context within a sentence. Sentence Transformers, on the other hand, are trained specifically to understand the meaning of entire sentences. They generate embeddings where sentences with similar meanings are close in the embedding space.\n\n      \nLevel of Embedding\n: The primary difference lies in the level of embedding. BERT provides embeddings for each token (word or subword) in a sentence, whereas sentence transformers provide a single embedding for the entire sentence.\n\n      \nTraining Data and Tasks\n: While BERT is primarily trained on large text corpora with tasks focused on understanding words in context, Sentence Transformers are often trained on data sets that include sentence pairs. This training focuses on similarity and relevance, teaching the model how to understand and compare the meanings of entire sentences.\n\n      \nSiamese and Triplet Network Structures\n: Sentence Transformers often use Siamese or Triplet network structures. These networks involve processing pairs or triplets of sentences and adjusting the model so that similar sentences have similar embeddings, and dissimilar sentences have different embeddings. This is different from BERT’s training, which does not inherently involve direct comparison of separate sentences.\n\n      \nFine-Tuning for Specific Tasks\n: Sentence Transformers are often fine-tuned on specific tasks like semantic similarity, paraphrase identification, or information retrieval. This fine-tuning is more focused on sentence-level understanding as opposed to BERT, which might be fine-tuned for a wider range of NLP tasks like question answering, sentiment analysis, etc., focusing on word or phrase-level understanding.\n\n      \nApplicability\n: BERT and similar models are more versatile for tasks that require understanding at the token level (like named entity recognition, question answering), whereas sentence transformers are more suited for tasks that rely on sentence-level understanding (like semantic search, sentence similarity).\n\n      \nEfficiency in Generating Sentence Embeddings or Similarity Tasks\n: In standard BERT, generating sentence embeddings usually involves taking the output of one of the hidden layers (often the first token, \n[CLS]\n) as a representation of the whole sentence. However, this method is not always optimal for sentence-level tasks. Sentence Transformers are specifically optimized to produce more meaningful and useful sentence embeddings and are thus more efficient for tasks involving sentence similarity computations. Since they produce a single vector per sentence, computing similarity scores between sentences is computationally less intensive compared to token-level models.\n\n    \n\n  \n\n  \nIn summary, while BERT is a general-purpose language understanding model with a focus on word-level contexts, Sentence Transformers are adapted specifically for understanding and comparing the meanings of entire sentences, making them more effective for tasks that require sentence-level semantic understanding.\n\n\n\n\n\nRelated: Training Process for Sentence Transformers vs. Token-Level Embedding Models\n\n\n\n\n  \nLet’s look into how sentence transformers trained differently compared to token-level embedding models such as BERT.\n\n  \nSentence transformers are trained to generate embeddings at the sentence level, which is a distinct approach from token-level embedding models like BERT. Here’s an overview of their training and how it differs from token-level models:\n    \n\n      \nModel Architecture\n: Sentence transformers often start with a base model similar to BERT or other transformer architectures. However, the focus is on outputting a single embedding vector for the entire input sentence, rather than individual tokens.\n\n      \nTraining Data\n: They are trained on a variety of datasets, often including pairs or groups of sentences where the relationship (e.g., similarity, paraphrasing) between the sentences is known.\n\n      \nTraining Objectives\n: BERT is pre-trained on objectives like masked language modeling (predicting missing words) and next sentence prediction, which are focused on understanding the context at the token level. Sentence transformers, on the other hand, are trained specifically to understand the context and relationships at the sentence level. Their training objective is typically to minimize the distance between embeddings of semantically similar sentences while maximizing the distance between embeddings of dissimilar sentences. This is achieved through contrastive loss functions like triplet loss, cosine similarity loss, etc.\n\n      \nOutput Representation\n: In BERT, the sentence-level representation is typically derived from the embedding of a special token (like \n[CLS]\n) or by pooling (i.e., averaging) token embeddings. Sentence transformers are designed to directly output a meaningful sentence-level representation.\n\n      \nFine-tuning for Downstream Tasks\n: Sentence transformers can be fine-tuned on specific tasks, such as semantic text similarity, where the model learns to produce embeddings that capture the nuanced meaning of entire sentences.\n\n    \n\n  \n\n  \nIn summary, sentence transformers are specifically optimized for producing representations at the sentence level, focusing on capturing the overall semantics of sentences, which makes them particularly useful for tasks involving sentence similarity and clustering. This contrasts with token-level models like BERT, which are more focused on understanding and representing the meaning of individual tokens within their wider context.\n\n\n\n\n\nApplying Sentence Transformers for RAG\n\n\n\n\n  \nNow, let’s look into why sentence transformers are the numero uno choice of models to generate embeddings for RAG.\n\n  \nRAG leverages Sentence Transformers for their ability to understand and compare the semantic content of sentences. This integration is particularly useful in scenarios where the model needs to retrieve relevant information before generating a response. Here’s how Sentence Transformers are useful in a RAG setting:\n    \n\n      \nImproved Document Retrieval\n: Sentence Transformers are trained to generate embeddings that capture the semantic meaning of sentences. In a RAG setting, these embeddings can be used to match a query (like a user’s question) with the most relevant documents or passages in a database. This is critical because the quality of the generated response often depends on the relevance of the retrieved information.\n\n      \nEfficient Semantic Search\n: Traditional keyword-based search methods might struggle with understanding the context or the semantic nuances of a query. Sentence Transformers, by providing semantically meaningful embeddings, enable more nuanced searches that go beyond keyword matching. This means that the retrieval component of RAG can find documents that are semantically related to the query, even if they don’t contain the exact keywords.\n\n      \nContextual Understanding for Better Responses\n: By using Sentence Transformers, the RAG model can better understand the context and nuances of both the input query and the content of potential source documents. This leads to more accurate and contextually appropriate responses, as the generation component of the model has more relevant and well-understood information to work with.\n\n      \nScalability in Information Retrieval\n: Sentence Transformers can efficiently handle large databases of documents by pre-computing embeddings for all documents. This makes the retrieval process faster and more scalable, as the model only needs to compute the embedding for the query at runtime and then quickly find the closest document embeddings.\n\n      \nEnhancing the Generation Process\n: In a RAG setup, the generation component benefits from the retrieval component’s ability to provide relevant, semantically-rich information. This allows the language model to generate responses that are not only contextually accurate but also informed by a broader range of information than what the model itself was trained on.\n\n    \n\n  \n\n  \nIn summary, Sentence Transformers enhance the retrieval capabilities of RAG models with LLMs by enabling more effective semantic search and retrieval of information. This leads to improved performance in tasks that require understanding and generating responses based on large volumes of text data, such as question answering, chatbots, and information extraction.\n\n\n\n\n\nRetrieval\n\n\n\n\n  \nLet’s look at three different types of retrieval: standard, sentence window, and auto-merging. Each of these approaches has specific strengths and weaknesses, and their suitability depends on the requirements of the RAG task, including the nature of the dataset, the complexity of the queries, and the desired balance between specificity and contextual understanding in the responses.\n\n\n\n\n\nStandard/Naive approach\n\n\n\n\n  \nAs we see in the image below \n(source)\n, the standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.\n\n\n\n\n\n\n\n\n\n  \nIn the context of RAG in LLMs, here are the advantages and disadvantages of the three approaches:\n\n\n\n\n\nAdvantages\n\n\n\n  \nSimplicity and Efficiency\n: This method is straightforward and efficient, using the same text chunk for both embedding and synthesis, simplifying the retrieval process.\n\n  \nUniformity in Data Handling\n: It maintains consistency in the data used across both retrieval and synthesis phases.\n\n\n\n\n\nDisadvantages\n\n\n\n  \nLimited Contextual Understanding\n: LLMs may require a larger window for synthesis to generate better responses, which this approach may not adequately provide.\n\n  \nPotential for Suboptimal Responses\n: Due to the limited context, the LLM might not have enough information to generate the most relevant and accurate responses.\n\n\n\n\n\nSentence-Window Retrieval / Small-to-Large Retrieval\n\n\n\n  \nThe sentence-window approach breaks down documents into smaller units, such as sentences or small groups of sentences.\n\n  \nIt decouples the embeddings for retrieval tasks (which are smaller chunks stored in a Vector DB), but for synthesis it adds back in the context around the retrieved chunks, as seen in the image below \n(source)\n.\n\n\n\n\n\n\n\n\n\n  \nDuring retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the sentence with the full surrounding context (using a static sentence-window around the context, implemented by retrieving sentences surrounding the one being originally retrieved) as shown in the figure below \n(source)\n.\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\n  \nEnhanced Specificity in Retrieval\n: By breaking documents into smaller units, it enables more precise retrieval of segments directly relevant to a query.\n\n  \nContext-Rich Synthesis\n: It reintroduces context around the retrieved chunks for synthesis, providing the LLM with a broader understanding to formulate responses.\n\n  \nBalanced Approach\n: This method strikes a balance between focused retrieval and contextual richness, potentially improving response quality.\n\n\n\n\n\nDisadvantages\n\n\n\n  \nIncreased Complexity\n: Managing separate processes for retrieval and synthesis adds complexity to the pipeline.\n\n  \nPotential Contextual Gaps\n: There’s a risk of missing broader context if the surrounding information added back is not sufficiently comprehensive.\n\n\n\n\n\nAuto-merging Retriever / Hierarchical Retriever\n\n\n\n\n  \nThe image below \n(source)\n, illustrates how auto-merging retrieval can work where it doesn’t retrieve a bunch of fragmented chunks as would happen with the naive approach.\n\n\n\n\n\n\n\n\n\n  \nThe fragmentation in the naive approach would be worse with smaller chunk sizes as shown below \n(source)\n.\n\n\n\n\n\n\n\n\n\n  \nAuto-merging retrieval aims to combine (or merge) information from multiple sources or segments of text to create a more comprehensive and contextually relevant response to a query. This approach is particularly useful when no single document or segment fully answers the query but rather the answer lies in combining information from multiple sources.\n\n  \nIt allows smaller chunks to be merged into bigger parent chunks. It does this via the following steps:\n    \n\n      \nDefine a hierarchy of smaller chunks linked to parent chunks.\n\n      \nIf the set of smaller chunks linking to a parent chunk exceeds some threshold (say, cosine similarity), then “merge” smaller chunks into the bigger parent chunk.\n\n    \n\n  \n\n  \nThe method will finally retrieve the parent chunk for better context.\n\n\n\n\n\nAdvantages\n\n\n\n  \nComprehensive Contextual Responses\n: By merging information from multiple sources, it creates responses that are more comprehensive and contextually relevant.\n\n  \nReduced Fragmentation\n: This approach addresses the issue of fragmented information retrieval, common in the naive approach, especially with smaller chunk sizes.\n\n  \nDynamic Content Integration\n: It dynamically combines smaller chunks into larger, more informative ones, enhancing the richness of the information provided to the LLM.\n\n\n\n\n\nDisadvantages\n\n\n\n  \nComplexity in Hierarchy and Threshold Management\n: The process of defining hierarchies and setting appropriate thresholds for merging is complex and critical for effective functioning.\n\n  \nRisk of Over-generalization\n: There’s a possibility of merging too much or irrelevant information, leading to responses that are too broad or off-topic.\n\n  \nComputational Intensity\n: This method might be more computationally intensive due to the additional steps in merging and managing the hierarchical structure of text chunks.\n\n\n\n\n\nContextual Retrieval\n\n\n\n\n  \nFor LLMs to deliver relevant and accurate responses, they must retrieve the right information from a knowledge base. Traditional RAG improves model accuracy by fetching relevant text chunks and appending them to the prompt. However, such methods often remove crucial context when encoding information, leading to failed retrievals and suboptimal outputs.\n\n  \nContextual Retrieval, introduced by \nAnthropic\n, is an advanced technique designed to improve this process by ensuring that retrieved chunks maintain their original context. It employs contextual embeddings – embeddings that incorporate chunk-specific background information and contextual BM25 – an enhanced BM25 ranking that considers the broader document context.\n\n  \n\n    \nBy prepending contextual metadata to each document chunk before embedding, Contextual Retrieval significantly enhances search accuracy. This approach reduces failed retrievals by 49% and, when combined with reranking, by 67%.\n\n  \n\n  \nWhy Context Matters in Retrieval\n:\n    \n\n      \nTraditional RAG solutions divide documents into small chunks for efficient retrieval. However, these fragments often lose critical context. For example, the statement “The company’s revenue grew by 3% over the previous quarter” lacks information about which company or quarter it refers to. Contextual Retrieval solves this by embedding relevant metadata into each chunk.\n\n    \n\n  \n\n  \nImplementation of Contextual Retrieval\n:\n    \n\n      \nTo implement Contextual Retrieval, a model like Claude 3 Haiku can generate concise context for each chunk. This context is then prepended before embedding and indexing, ensuring more precise retrieval. Developers can automate this process at scale using specialized retrieval pipelines.\n\n    \n\n  \n\n  \nPrompt Used for Contextual Retrieval\n:\n    \n\n      \nAnthropic’s method involves using Claude to generate a short, document-specific context for each chunk using the following prompt:\n        \n  <document>  \n          \n  </document>  \n\n  Here is the chunk we want to situate within the whole document:  \n\n  <chunk>  \n          \n  </chunk>  \n\n  Please give a short, succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n\n        \n\n      \n\n      \nThis process automatically generates a concise contextualized description that is prepended to the chunk before embedding and indexing.\n\n    \n\n  \n\n  \nCombining Contextual Retrieval with Reranking\n:\n    \n\n      \nFor maximum performance, Contextual Retrieval can be paired with reranking models, which filter and reorder retrieved chunks based on their relevance. This additional step enhances retrieval precision, ensuring only the most relevant chunks are passed to the LLM.\n\n    \n\n  \n\n  \nThe following flowchart from Anthropic’s \nblog\n shows the combined contextual retrieval and reranking stages which seek to maximize retrieval accuracy.\n\n\n\n\n\n\n\n\n\n  \nKey Takeaways\n:\n    \n\n      \nContextual Embeddings improve retrieval accuracy by preserving document meaning.\n\n      \nBM25 + Contextualization enhances exact-match retrieval.\n\n      \nCombining Contextual Retrieval with reranking further boosts retrieval effectiveness.\n\n      \nDevelopers can implement Contextual Retrieval using prompt-based preprocessing and automated pipelines.\n\n    \n\n  \n\n  \nWith Contextual Retrieval, LLM-powered knowledge systems can achieve greater accuracy, scalability, and relevance, unlocking new levels of performance in real-world applications.\n\n\n\n\n\nUsing Approximate Nearest Neighbors (ANN) for Retrieval\n\n\n\n\n  \nThe next step is to consider which approximate nearest neighbors (ANN) library to choose from indexing. One option to pick the best option is to look at \nANN-Benchmarks\n.\n\n  \nA detailed discourse on the concept of ANN can be found in our \nANN primer\n.\n\n\n\n\n\nRe-ranking\n\n\n\n\n  \nRe-ranking is an optional yet critical component in RAG pipelines, functioning as the refinement stage where an initially retrieved candidate set—usually limited to dozens of documents or passages—is reordered based on their relevance to the input query. This step ensures that the most pertinent content is prioritized for inclusion in the final prompt presented to the language model.\n\n  \nAs the candidate set is small, computationally intensive but accurate re-ranking techniques are feasible, with neural Learning-to-Rank (LTR) models being the most commonly used.\n\n\n\n\n\nNeural Re-rankers: Types and Architectures\n\n\n\n\n  \n\n    \nNeural re-rankers are broadly classified into three methodological paradigms based on how they evaluate relevance: pointwise, pairwise, and listwise. Each of these paradigms corresponds to specific models: monoBERT exemplifies the pointwise approach, duoBERT represents the pairwise method, and ListBERT along with ListT5 embody the listwise strategy, as detailed below:\n\n\n    \n\n      \n\n        \nmonoBERT\n, proposed by Nogueira et al. (2019) in \nMulti-Stage Document Ranking with BERT\n, scores each document-query pair independently using BERT as a cross-encoder. Each document is concatenated with the query, and a relevance score is predicted for that pair alone. This makes monoBERT a pointwise model, offering high-quality relevance estimation but at a high inference cost when applied to many pairs.\n\n      \n\n      \n\n        \nduoBERT\n, also proposed by Nogueira et al. (2019) in \nMulti-Stage Document Ranking with BERT\n, extends monoBERT by comparing pairs of documents relative to a given query. It predicts which of two documents is more relevant, allowing for more nuanced and direct ranking decisions. This pairwise approach helps resolve fine-grained distinctions in relevance that monoBERT might overlook.\n\n      \n\n      \n\n        \nListBERT\n, proposed by Kumar et al. (2022) in \nListBERT: Learning to Rank E-commerce products with Listwise BERT\n, brings a listwise learning paradigm to transformer-based ranking. Instead of scoring documents independently or in pairs, ListBERT considers a full list of documents simultaneously. It uses listwise loss functions tailored for ranking tasks (e.g., ListMLE, Softmax Cross Entropy) and was originally applied in the context of e-commerce to rank products effectively.\n\n      \n\n      \n\n        \nListT5\n, proposed by Yoon et al. (2024) in \nListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval\n, advances the listwise approach further by employing a Fusion-in-Decoder (FiD) architecture adapted from T5. This model jointly attends to multiple candidate documents during both training and inference, making it particularly suitable for zero-shot retrieval scenarios. It has shown state-of-the-art performance in contexts requiring the ranking of passages with minimal labeled data.\n\n      \n\n    \n\n  \n\n  \n\n    \nWhile \nmonoBERT\n and \nduoBERT\n utilize cross-encoder architectures, \nListBERT\n and \nListT5\n employ different mechanisms. ListBERT uses a listwise approach with BERT, and ListT5 is based on a Fusion-in-Decoder (FiD) architecture adapted from T5, which is not a traditional cross-encoder.\n\n  \n\n\n\n\n\nDomain-Specific Adaptations\n\n\n\n\n  \nReranking models can also be fine-tuned for specific domains to improve performance in specialized applications. For instance, \nLegal-BERT\n has been adapted for tasks like legal document classification and contract clause retrieval, demonstrating that domain-specific pretraining significantly boosts accuracy in re-ranking. Similar adaptations exist in finance, healthcare, and technical fields where vocabulary and relevance judgments differ markedly from general-purpose datasets.\n\n\n\n\n\nInstruction-Following Re-ranking: Precision and Control in RAG\n\n\n\n\n  \n\n    \nA growing frontier in reranking is instruction-following re-ranking, which introduces the ability to control ranking behavior using natural language instructions. This approach addresses the limitations of static relevance criteria by allowing dynamic, context-specific customization, which is particularly beneficial in enterprise RAG systems where documents may conflict or vary in trustworthiness and recency.\n\n  \n\n  \n\n    \nExamples of Natural Language Instructions\n:\n\n\n    \n\n      \n“Prioritize internal documentation over third-party sources. Favor the most recent information.”\n\n      \n“Disregard news summaries. Emphasize detailed technical reports from trusted analysts.”\n\n    \n\n  \n\n  \n\n    \nAdvantages\n:\n\n\n    \n\n      \nDynamic Relevance Modeling\n: Instructions enable runtime tuning of what “relevance” means, depending on the user’s intent or business context.\n\n      \nConflict Resolution\n: They allow systems to resolve contradictory or overlapping sources by enforcing prioritization rules.\n\n      \nPrompt Optimization\n: Ensuring higher-quality content appears earlier in the prompt helps maximize the utility of the limited context window, where the LLM’s attention is most focused.\n\n    \n\n  \n\n  \n\n    \nImplementation and Deployment\n:\n\n\n    \n\n      \nThese rerankers are often deployed as standalone APIs or integrated modules that rescore a shortlist of candidate documents after the initial retrieval phase. They can be combined with other techniques such as late chunking or retriever ensembling, effectively acting as the final curation layer before the documents are fed into the language model.\n\n      \nA notable example of this is Contextual AI’s system, presented in their post on \nthe world’s first instruction-following reranker\n, which demonstrates real-world integration of this technology.\n\n    \n\n  \n\n\n\n\n\nResponse Generation / Synthesis\n\n\n\n\n  \nThe last step of the RAG pipeline is to generate responses back to the user. In this step, the model synthesizes the retrieved information with its pre-trained knowledge to generate coherent and contextually relevant responses. This process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting a response that is not only informative but also aligns with the user’s original query, maintaining a natural and conversational tone.\n\n  \nNote that while creating the expanded prompt (with the retrieved top-\\(k\\) chunks) for an LLM to make an informed response generation, a strategic placement of vital information at the beginning or end of input sequences could enhance the RAG system’s effectiveness and thus make the system more performant. This is summarized in the paper below.\n\n\n\n\n\nLost in the Middle: How Language Models Use Long Contexts\n\n\n\n  \nWhile recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context.\n\n  \nThis paper by Liu et al. from Percy Liang’s lab at Stanford, UC Berkeley, and Samaya AI analyzes language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. Put simply, they analyze and evaluate how LLMs use the context by identifying relevant information within it.\n\n  \nThey tested open-source (MPT-30B-Instruct, LongChat-13B) and closed-source (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude 1.3) models. They used multi-document question-answering where the context included multiple retrieved documents and one correct answer, whose position was shuffled around. Key-value pair retrieval was carried out to analyze if longer contexts impact performance.\n\n  \nThey find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. In other words, their findings basically suggest that Retrieval-Augmentation (RAG) performance suffers when the relevant information to answer a query is presented in the middle of the context window with strong biases towards the beginning and the end of it.\n\n  \nA summary of their learnings is as follows:\n    \n\n      \nBest performance when the relevant information is at the beginning.\n\n      \nPerformance decreases with an increase in context length.\n\n      \nToo many retrieved documents harm performance.\n\n      \nImproving the retrieval and prompt creation step with a ranking stage could potentially boost performance by up to 20%.\n\n      \nExtended-context models (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) are not better if the prompt fits the original context.\n\n    \n\n  \n\n  \nConsidering that RAG retrieves information from an external database – which most commonly contains longer texts that are split into chunks. Even with split chunks, context windows get pretty large very quickly, at least much larger than a “normal” question or instruction. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Their analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.\n\n  \n“There is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it’s then how LLMs parameterize the attention weights during training.” \n(source)\n\n  \nIn other words, human text artifacts are often constructed in a way where the beginning and the end of a long text matter the most which could be a potential explanation to the characteristics observed in this work.\n\n  \nYou can also model this with the lens of two popular cognitive biases that humans face (primacy and recency bias), as in the following figure \n(source)\n.\n\n\n\n\n\n\n\n\n\n  \nThe final conclusion is that combining retrieval with ranking (as in recommender systems) should yield the best performance in RAG for question answering.\n\n  \nThe following figure \n(source)\n shows an overview of the idea proposed in the paper: “LLMs are better at using info at beginning or end of input context”.\n\n\n\n\n\n\n\n\n\n  \nThe following figure from the paper illustrates the effect of changing the position of relevant information (document containing the answer) on multidocument question answering performance. Lower positions are closer to the start of the input context. Performance is generally highest when relevant information is positioned at the very start or very end of the context, and rapidly degrades when models must reason over information in the middle of their input context.\n\n\n\n\n\n\n\n\nThe “Needle in a Haystack” Test\n\n\n\n\n  \nTo understand the in-context retrieval ability of long-context LLMs over various parts of their prompt, a simple ‘needle in a haystack’ analysis could be conducted. This method involves embedding specific, targeted information (the ‘needle’) within a larger, more complex body of text (the ‘haystack’). The purpose is to test the LLM’s ability to identify and utilize this specific piece of information amidst a deluge of other data.\n\n  \nIn practical terms, the analysis could involve inserting a unique fact or data point into a lengthy, seemingly unrelated text. The LLM would then be tasked with tasks or queries that require it to recall or apply this embedded information. This setup mimics real-world situations where essential details are often buried within extensive content, and the ability to retrieve such details is crucial.\n\n  \nThe experiment could be structured to assess various aspects of the LLM’s performance. For instance, the placement of the ‘needle’ could be varied—early, middle, or late in the text—to see if the model’s retrieval ability changes based on information location. Additionally, the complexity of the surrounding ‘haystack’ can be modified to test the LLM’s performance under varying degrees of contextual difficulty. By analyzing how well the LLM performs in these scenarios, insights can be gained into its in-context retrieval capabilities and potential areas for improvement.\n\n  \nThis can be accomplished using the \nNeedle In A Haystack\n library. The following plot shows OpenAI’s GPT-4-128K’s (top) and (bottom) performance with varying context length.\n\n\n\n\n\n\n\n\n\n\n\n\n  \nThe following figure \n(source)\n shows Claude 2.1’s long context question answering errors based on the areas of the prompt context length. On an average, Claude 2.1 demonstrated a 30% reduction in incorrect answers compared to Claude 2.\n\n\n\n\n\n\n\n\n\n  \nHowever, in Anthropic’s \nLong context prompting for Claude 2.1\n blog, Anthropic noted that adding “Here is the most relevant sentence in the context:” to the start of Claude’s response raised the score from 27% to 98% on the original evaluation! The figure below from the blog shows that Claude 2.1’s performance when retrieving an individual sentence across its full 200K token context window. This experiment uses the aforementioned prompt technique to guide Claude in recalling the most relevant sentence.\n\n\n\n\n\n\n\n\nRAG in Multi-Turn Chatbots: Embedding Queries for Retrieval\n\n\n\n\n  \n\n    \nIn multi-turn chatbot environments, RAG must extend beyond addressing isolated, single-turn queries. Conversations are inherently dynamic—context accumulates, user objectives evolve, and intent may shift subtly across multiple interactions. This dynamic nature renders one design decision particularly critical: determining which input text should be embedded during the retrieval phase. This decision has a direct impact on both the relevance of the retrieved content and the overall quality of the generated response.\n\n  \n\n  \n\n    \nIn contrast to single-turn systems, where embedding the current user input may suffice, multi-turn RAG systems face a more fluid and complex challenge. Limiting retrieval inputs to only the most recent user message is computationally efficient but often insufficient for capturing the nuances of ongoing discourse. Incorporating recent conversational turns offers improved contextual grounding, while advanced techniques such as summarization and query rewriting can significantly enhance retrieval precision.\n\n  \n\n  \n\n    \nThere is no universally optimal approach—the most suitable strategy depends on factors such as the application’s specific requirements, available computational resources, and tolerance for system complexity. Nevertheless, the most robust implementations often adopt a layered methodology: integrating recent dialogue context, monitoring evolving user intent, and utilizing reformulated or enriched queries. This composite approach typically results in more accurate, contextually appropriate retrieval and, consequently, more coherent and effective responses.\n\n  \n\n  \n\n    \nThe following sections outlines the key strategies and considerations for query embedding in multi-turn RAG chatbot systems.\n\n  \n\n\n\n\n\nEmbedding the Latest User Turn Only\n\n\n\n\n  \nThe simplest approach is to embed just the latest user message. For example, if a user says, “What are the symptoms of Lyme disease?”, that exact sentence is passed to the retriever for embedding.\n\n  \nPros\n:\n    \n\n      \nFast and computationally cheap.\n\n      \nReduces the risk of embedding irrelevant or stale context.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nIgnores conversational context and prior turns, which may contain critical disambiguating details (e.g., “Is it common in dogs?” following a discussion about pets).\n\n    \n\n  \n\n\n\n\n\nEmbedding Concatenated Recent Turns (Truncated Dialogue History)\n\n\n\n\n  \nA more nuanced approach involves embedding the current user message along with a sliding window of recent turns (usually alternating user and assistant messages).\n\n  \nFor example:\n    \nUser: My dog has been acting strange lately.\nAssistant: Can you describe the symptoms?\nUser: He’s tired, limping, and has a fever. Could it be Lyme disease?\n\n    \n\n    \n\n      \nThe retriever input would include all or part of the above.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nPreserves immediate context that can significantly improve retrieval relevance.\n\n      \nEspecially useful for resolving pronouns and follow-up queries.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nCan dilute the focus of the query if too many irrelevant prior turns are included.\n\n      \nRisk of exceeding input length limits for embedding models.\n\n    \n\n  \n\n\n\n\n\nEmbedding a Condensed or Summarized History\n\n\n\n\n  \nIn this strategy, prior turns are summarized into a condensed form before concatenation with the current turn. This reduces token count while preserving key context.\n\n  \nCan be achieved using simple heuristics, hand-written rules, or another lightweight LLM summarization pass.\n\n  \nFor example:\n    \nCondensed history: The user is concerned about their dog's health, showing signs of fatigue and limping.\nCurrent query: Could it be Lyme disease?\n\n    \n\n    \n\n      \nEmbed the concatenated string: “The user is concerned… Could it be Lyme disease?”\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nRetains relevant prior context while minimizing noise.\n\n      \nHelps improve retrieval accuracy for ambiguous follow-up questions.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nRequires additional processing and potential summarization latency.\n\n      \nSummarization quality can affect retrieval quality.\n\n    \n\n  \n\n\n\n\n\nEmbedding Structured Dialogue State\n\n\n\n\n  \nThis approach formalizes the conversation history into a structured format (like intent, entities, or user goals), which is then appended to the latest query before embedding.\n\n  \nFor instance:\n    \n[Intent: Diagnose pet illness] [Entity: Dog] [Symptoms: fatigue, limping, fever]\nQuery: Could it be Lyme disease?\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nAllows precision targeting of relevant documents, especially in domain-specific applications.\n\n      \nSupports advanced reasoning by aligning with KBs or ontology-driven retrieval.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nRequires reliable NLU and state-tracking pipelines.\n\n      \nAdds system complexity.\n\n    \n\n  \n\n\n\n\n\nTask-Optimized Embedding via Query Reformulation\n\n\n\n\n  \nSome systems apply a query rewriting model that reformulates the latest turn into a fully self-contained question, suitable for retrieval.\n\n  \nFor example, turning “What about dogs?” into “What are the symptoms of Lyme disease in dogs?”\n\n  \nThese reformulated queries are then embedded for retrieval.\n\n  \nPros\n:\n    \n\n      \nEnsures clarity and focus in queries passed to the retriever.\n\n      \nSignificantly boosts retrieval performance in ambiguous or shorthand follow-ups.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nIntroduces dependency on a high-quality rewrite model.\n\n      \nRisk of introducing hallucination or incorrect reformulations.\n\n    \n\n  \n\n\n\n\n\nBest Practices and Considerations\n\n\n\n\n  \nWindow Size\n: Most systems use a sliding window of 1-3 previous turns depending on token limits and task specificity.\n\n  \nQuery Length vs. Clarity Tradeoff\n: Longer queries with more context may capture nuance but risk introducing noise. Condensed or reformulated queries can help mitigate this.\n\n  \nPersonalization\n: In some advanced setups, user profiles or long-term memory can be injected into the retrieval query, but this must be carefully curated to avoid privacy or relevance pitfalls.\n\n  \nSystem Goals\n: If the chatbot is task-oriented (e.g., booking travel), structured state may be best. If it is open-domain (e.g., a virtual assistant), concatenated dialogue or rewrite strategies tend to perform better.\n\n\n\n\n\nComponent-Wise Evaluation\n\n\n\n\n  \nComponent-wise evaluation in RAG systems for LLMs involves assessing individual components of the system separately. This approach typically examines the performance of the retrieval component, which fetches relevant information from a database or corpus, and the generation component, which synthesizes responses based on the retrieved data. By evaluating these components individually, researchers can identify specific areas for improvement in the overall RAG system, leading to more efficient and accurate information retrieval and response generation in LLMs.\n\n  \nWhile metrics such as Context Precision, Context Recall, and Context Relevance provide insights into the performance of the retrieval component of the RAG system, Groundedness, and Answer Relevance offer a view into the quality of the generation.\n\n  \nSpecifically,\n    \n\n      \nMetrics to evaluate retrieval:\n Context Relevance, Context Recall, and Context Precision, which collectively assess the relevance, completeness, and accuracy of the information retrieved in response to a user’s query. Context Precision focuses on the system’s ability to rank relevant items higher, Context Recall evaluates how well the system retrieves all relevant parts of the context, and Context Relevance measures the alignment of retrieved information with the user’s query. These metrics ensure the effectiveness of the retrieval system in providing the most relevant and complete context for generating accurate responses.\n\n      \nMetrics to evaluate generation:\n Faithfulness and Answer Relevance, which measure the factual consistency of the generated answer with the given context and its relevance to the original question, respectively. Faithfulness focuses on the factual accuracy of the answer, ensuring all claims made can be inferred from the given context. Answer Relevance assesses how well the answer addresses the original question, penalizing incomplete or redundant responses. These metrics ensure the generation component produces contextually appropriate and semantically relevant answers.\n\n    \n\n  \n\n  \nThe harmonic mean of these four aspects gives you the overall score (also called ragas score) which is a single measure of the performance of your RAG system across all the important aspects.\n\n  \nMost of the measurements do not require any labeled data, making it easier for users to run it without worrying about building a human-annotated test dataset first. In order to run ragas all you need is a few questions and if your using context_recall, a reference answer.\n\n  \nOverall, these metrics offer a comprehensive view of the RAG system’s retrieval performance, which can be implemented using libraries for evaluating RAG pipelines such as \nRagas\n or \nTruLens\n and offer detailed insights about your RAG pipeline’s performance, focusing on the contextual and factual alignment of retrieved and generated content in response to user queries. Specifically, \nRagas\n, offers metrics tailored for evaluating each component of your RAG pipeline in isolation. This approach complements the broader, system-level end-to-end evaluation of your system (which is detailed in \nEnd-to-End Evaluation\n), allowing for a deeper understanding of how well a RAG system performs in real-world scenarios where the intricacies of context and factual accuracy are paramount. The figure below \n(source)\n shows the metrics that Ragas offers which are tailored for evaluating each component (retrieval, generation) of your RAG pipeline in isolation.\n\n\n\n\n\n\n\n\n\n  \nThe image below \n(source)\n, shows the “triad” of metrics that can be used to evaluate RAG: Groundedness (also known as Faithfulness), Answer Relevance, and Context Relevance. Note that Context Precision and Context Recall are also important and were more recently introduced in a newer version of \nRagas\n.\n\n\n\n\n\n\n\n\nRetrieval Metrics\n\n\n\n\n  \nEvaluating the retrieval component of RAG in the context of LLMs involves assessing how effectively the system retrieves relevant information to support the generation of accurate and contextually appropriate responses.\n\n\n\n\n\nContext Precision\n\n\n\n\n  \n\n    \nDefinition\n: Context Precision is a metric used to assess the accuracy of ranking ground-truth relevant items from the context higher in the results. It measures whether all the relevant chunks of information appear at the top ranks when responding to a query. Ideally all the relevant chunks must appear at the top ranks. The metric is scored between 0 and 1 using the question, ground truth, and the contexts, with higher scores indicating better precision.\n\n  \n\n  \nEvaluation Approach\n: Context Precision is calculated using the following steps:\n    \n\n      \nFor each chunk in the retrieved context, determine if it is relevant or not relevant based on the ground truth for the given question.\n\n      \n\n        \nCompute Precision@k for each chunk in the context using the formula:\n\n\n\\[\\text{Precision@k} = \\frac{\\text{true positives@k}}{\\text{true positives@k} + \\text{false positives@k}}\\]\n      \n\n      \n\n        \nCalculate the Context Precision@k by averaging the Precision@k values for all relevant items in the top \\(K\\) results:\n\n\n\\[\\text{Context Precision@k} = \\frac{\\sum_{k=1}^K (\\text{Precision@k} \\times v_k)}{\\text{Total number of relevant items in the top } K \\text{ results}}\\]\n\n        \n\n          \nwhere \\(K\\) is the total number of chunks in contexts and \\(v_k \\in \\{0,1\\}\\) is the relevance indicator at rank \\(k\\).\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nExample\n \n(source)\n: Let’s consider an example of calculating context precision using a question and its corresponding ground truth.\n\n\n    \n\n      \nQuestion\n: Where is France and what is its capital?\n\n      \nGround Truth\n: France is in Western Europe, and its capital is Paris.\n\n      \nHigh Context Precision Example\n:\n        \n\n          \nContexts: \n[\"France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower\", \"The country is also renowned for its wines and sophisticated cuisine. Lascaux's ancient cave drawings, Lyon's Roman theater and the vast Palace of Versailles attest to its rich history.\"]\n\n        \n\n      \n\n      \nLow Context Precision Example\n:\n        \n\n          \nContexts: \n[\"The country is also renowned for its wines and sophisticated cuisine. Lascaux's ancient cave drawings, Lyon's Roman theater and\", \"France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower\"]\n\n        \n\n      \n\n      \nIn this example, the calculation of context precision involves identifying relevant chunks related to the question and their ranking in the contexts. For the low context precision example:\n        \n\n          \nPrecision@1\n = \\(\\frac{0}{1}\\) = 0\n\n          \nPrecision@2\n = \\(\\frac{1}{2}\\) = 0.5\n\n          \nContext Precision\n = \\(\\frac{(0 + 0.5)}{1}\\) = 0.5\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nContext Recall\n\n\n\n\n  \n\n    \nDefinition\n: Context Recall measures how well the retrieved context aligns with the annotated answer, treated as the ground truth. This metric is essential for assessing the accuracy of the retrieval system in identifying and ranking relevant information. It evaluate the performance of the retrieval system in identifying relevant information based on a sample query and its corresponding ground truth answer. The context recall score helps in understanding how much of the ground truth information is accurately retrieved from the context. The context recall score ranges from 0 to 1, with higher values indicating better performance.\n\n  \n\n  \n\n    \nEvaluation Approach\n: To estimate context recall, each sentence in the ground truth answer is analyzed to determine whether it can be attributed to the retrieved context. The ideal scenario is when all sentences in the ground truth answer are attributable to the retrieved context. The formula used for calculating context recall is:\n\n\n\\[\\text{Context Recall} = \\frac{\\mid \\text{GT sentences attributable to context} \\mid}{\\mid \\text{Total sentences in GT} \\mid}\\]\n  \n\n  \n\n    \nExample\n \n(source)\n:\n\n    \n\n      \nGround Truth Question: “Where is France and what is its capital?”\n\n      \n\n        \nGround Truth Answer: “France is in Western Europe and its capital is Paris.”\n\n      \n\n      \nHigh Context Recall Example\n:\n        \n\n          \nRetrieved Context: “France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre, and monuments like the Eiffel Tower.”\n\n        \n\n      \n\n      \nLow Context Recall Example\n:\n        \n\n          \nRetrieved Context: “France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater, and the vast Palace of Versailles attest to its rich history.”\n\n        \n\n      \n\n      \nCalculation\n:\n        \n\n          \nStep 1\n: Break the ground truth answer into individual statements:\n            \n\n              \nStatement 1: “France is in Western Europe.”\n\n              \nStatement 2: “Its capital is Paris.”\n\n            \n\n          \n\n          \nStep 2\n: Verify if each ground truth statement can be attributed to the retrieved context:\n            \n\n              \nStatement 1: Yes (in both high and low context recall examples)\n\n              \nStatement 2: No (in the low context recall example)\n\n            \n\n          \n\n          \n\n            \nStep 3\n: Calculate context recall using the formula:\n\n\n\\[\\text{Context Recall} = \\frac{1}{2} = 0.5 \\quad \\text{(for the low context recall example)}\\]\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nContext Relevance\n\n\n\n\n  \nDefinition\n:\n    \n\n      \n“Is the passage returned relevant for answering the given query?”\n\n      \nMeasures how well the context retrieved by the RAG system aligns with the user’s query. It specifically evaluates whether the retrieved information is relevant and appropriate for the given query, ensuring that only essential information is included to address the query effectively.\n\n    \n\n  \n\n  \nEvaluation Approach\n: Involves a two-step procedure: first, the identification of relevant sentences using semantic similarity measures to produce a relevance score for each sentence. Can be measured with smaller BERT-style models, embedding distances, or with LLMs. The approach involves estimating the value of context relevance by identifying sentences within the retrieved context that are directly relevant for answering the given question. This is followed by the quantification of overall context relevance, where the final score is calculated using the formula:\n\n\n\n\n\\[\\text {Context Relevance} = \\frac{\\text { Number of sentences that are relevant to the query within the retrieved context}}{\\text { Total number of sentences in retrieved context}}\\]\n\n\n\n  \nExamples\n:\n    \n\n      \nHigh context relevance example\n: For a question like “What is the capital of France?”, a highly relevant context would be “France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.”\n\n      \nLow context relevance example\n: For the same question, a less relevant context would include additional, unrelated information such as “The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.”\n\n    \n\n  \n\n  \nThis metric ensures that the RAG system provides concise and directly related information, enhancing the efficiency and accuracy of the response given to a specific query.\n\n\n\n\n\nGeneration Metrics\n\n\n\n\n  \nEvaluating the generation component of RAG in the context of LLMs involves assessing the ability of the system to seamlessly integrate retrieved information into coherent, contextually relevant, and linguistically accurate responses, ensuring a harmonious blend of retrieved data and generative language skills. Put simply, these metrics collectively provide a nuanced and multidimensional approach to evaluating RAG systems, emphasizing not just the retrieval of information but its contextual relevance, factual accuracy, and semantic alignment with user queries.\n\n\n\n\n\nGroundedness (a.k.a. Faithfulness)\n\n\n\n\n  \nDefinition\n: Groundedness (also known as Faithfulness) evaluates the factual consistency of a generated answer against a given context. It is measured based on the alignment between the answer and the retrieved context, with scores ranging from 0 to 1. A higher score indicates better factual consistency.\n\n  \nEvaluation Approach\n:\n    \n\n      \nThe faithfulness of a generated answer is determined by checking if all the atomic (stand-alone) claims made in the answer can be inferred from the provided context. The process involves identifying a set of atomic claims from the answer and cross-referencing each claim with the context to confirm if it can be inferred. The faithfulness score is calculated using the formula:\n\n    \n\n\n\\[\\text{Faithfulness score} = \\frac{\\text{Number of claims in the generated answer that can be inferred from the given context}}{\\text{Total number of claims in the generated answer}}\\]\n  \n\n  \n\n    \nExample\n \n(source)\n: \n\nQuestion\n: Where and when was Einstein born?\n\nContext\n: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n\n\n    \n\n      \nHigh faithfulness answer\n: Einstein was born in Germany on 14th March 1879.\n\n      \n\n        \nLow faithfulness answer\n: Einstein was born in Germany on 20th March 1879.\n\n      \n\n      \nFor the low faithfulness answer:\n        \n\n          \nStep 1\n: Break the generated answer into individual statements.\n            \n\n              \nStatement 1: “Einstein was born in Germany.”\n\n              \nStatement 2: “Einstein was born on 20th March 1879.”\n\n            \n\n          \n\n          \nStep 2\n: Verify if each statement can be inferred from the given context.\n            \n\n              \nStatement 1: Yes\n\n              \nStatement 2: No\n\n            \n\n          \n\n          \n\n            \nStep 3\n: Calculate the faithfulness score using the formula.\n\n\n\\[\\text{Faithfulness} = \\frac{1}{2} = 0.5\\]\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nAnswer Relevance\n\n\n\n\n  \nDefinition\n:\n    \n\n      \nThe Answer Relevance metric evaluates how closely the generated answer aligns with the given query/prompt. This assessment focuses on the pertinence of the response, penalizing answers that are incomplete or contain redundant information. Higher scores indicate better relevance. The overarching concept behind answer relevance is that if the answer correctly addresses the question, it is likely that the original question can be accurately reconstructed from the answer alone.\n\n      \nAnswer relevance is reference free metric. If you’re looking to compare ground truth answer with generated answer refer to \nAnswer Correctness\n.\n\n      \nThe image below \n(source)\n shows the output format of Answer Relevance.\n\n    \n\n\n    \n\n  \n\n  \nEvaluation Approach\n:\n    \n\n      \nAnswer Relevance is quantified by calculating the mean cosine similarity between the original question and a set of generated questions based on the provided answer. Specifically, the metric is defined as follows:\n\n    \n\n\n\\[\\begin{aligned}\n& \\text{Answer Relevance} = \\frac{1}{N} \\sum_{i=1}^N \\cos (E_{g_i}, E_o) \\\\\n& \\text{Answer Relevance} = \\frac{1}{N} \\sum_{i=1}^N \\frac{E_{g_i} \\cdot E_o}{\\left\\|E_{g_i}\\right\\|\\left\\|E_o\\right\\|}\n\\end{aligned}\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(E_{g_i}\\) is the embedding of the generated question \\(i\\).\n\n          \n\\(E_o\\) is the embedding of the original question.\n\n          \n\\(N\\) is the number of generated questions, typically set to 3 by default.\n\n        \n\n      \n\n      \nIt is important to note that although the score generally ranges from 0 to 1, it is not strictly limited to this range due to the cosine similarity measure, which can range from -1 to 1. This metric does not rely on a reference answer and is purely focused on the relevance of the generated answer to the original question. If comparing the ground truth answer with the generated answer is required, one should refer to the “answer correctness” metric.\n\n      \nAn answer is considered relevant if it directly and appropriately responds to the original question. This metric does not consider the factual accuracy of the answer but rather penalizes cases where the answer is incomplete or contains unnecessary details. The process involves prompting a Large Language Model (LLM) to generate appropriate questions based on the provided answer and then measuring the mean cosine similarity between these questions and the original question. The idea is that a highly relevant answer should allow the LLM to generate questions that closely align with the original question.\n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nQuestion\n: Where is France and what is its capital?\n\n      \nLow relevance answer\n: France is in Western Europe.\n\n      \nHigh relevance answer\n: France is in Western Europe and Paris is its capital.\n\n    \n\n  \n\n  \nCalculation Steps\n:\n    \n\n      \nStep 1\n: Generate \\(n\\) variants of the question from the provided answer using an LLM. For example:\n        \n\n          \nQuestion 1: “In which part of Europe is France located?”\n\n          \nQuestion 2: “What is the geographical location of France within Europe?”\n\n          \nQuestion 3: “Can you identify the region of Europe where France is situated?”\n\n        \n\n      \n\n      \nStep 2\n: Calculate the mean cosine similarity between these generated questions and the original question.\n\n    \n\n  \n\n\n\n\n\nAnswer Semantic Similarity\n\n\n\n\n  \nCategory\n: Answer Quality and Semantic Alignment\n\n  \nRequirement\n: Access to ground truth answers is necessary to evaluate the semantic similarity of generated responses accurately.\n\n  \nDefinition\n: Evaluates the degree of semantic similarity between the generated answer by the RAG system and the ground truth. This metric specifically assesses how closely the meaning of the generated answer mirrors that of the ground truth.\n\n  \nMeasurement Methods\n: This metric is measured using cross-encoder models designed to calculate the semantic similarity score. These models analyze the semantic content of both the generated answer and the ground truth.\n\n  \n\n    \nEvaluation Approach\n: The approach involves comparing the generated answer with the ground truth to determine the extent of semantic overlap. The semantic similarity is quantified on a scale from 0 to 1, where higher scores indicate a greater alignment between the generated answer and the ground truth. The formula for Answer Semantic Similarity is implicitly based on the evaluation of semantic overlap rather than a direct formula.\n\n  \n\n  \nBERTScore\n:\n    \n\n      \nUses contextual embeddings from pre-trained BERT models to match tokens in the candidate and reference text.\n\n      \nComputes precision, recall, and F1 scores by aligning embeddings based on cosine similarity, capturing nuanced semantic overlap.\n\n    \n\n  \n\n  \nMoverScore\n:\n    \n\n      \nExtends BERTScore by incorporating Earth Mover’s Distance (EMD) to assess the minimal semantic “effort” needed to transform one text into another.\n\n      \nLeverages both contextual embeddings and IDF weighting to emphasize important content over common filler words.\n\n    \n\n  \n\n  \nAdvantages of MoverScore over BERTScore\n:\n    \n\n      \nBetter captures the global semantic flow between texts by considering word importance and distribution, not just local alignment.\n\n      \nMore robust in handling paraphrased or reordered sentences, where BERTScore may undervalue semantic similarity due to token-level matching.\n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nGround truth\n: Albert Einstein’s theory of relativity revolutionized our understanding of the universe.\n\n      \nHigh similarity answer\n: Einstein’s groundbreaking theory of relativity transformed our comprehension of the cosmos.\n\n      \nLow similarity answer\n: Isaac Newton’s laws of motion greatly influenced classical physics.\n\n    \n\n  \n\n  \nIn this metric, a higher score reflects a better quality of the generated response in terms of its semantic closeness to the ground truth, indicating a more accurate and contextually relevant answer.\n\n\n\n\n\nBLEU Score\n\n\n\n\n  \nCategory\n: N-gram Precision-Based Evaluation\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate the BLEU score.\n\n  \nDefinition\n: BLEU (Bilingual Evaluation Understudy) is a metric that evaluates the quality of text by comparing a candidate translation to one or more reference translations. It measures the precision of n-grams in the candidate text that appear in the reference texts, with a brevity penalty to penalize overly short translations.\n\n  \nMeasurement Methods\n: BLEU calculates modified n-gram precision for n-grams up to a specified length (commonly 4). It also applies a brevity penalty to account for short candidate translations that might otherwise score artificially high.\n\n  \n\n    \nEvaluation Approach\n: The BLEU score is computed using the formula:\n\n\n\\[\\text{BLEU} = \\text{BP} \\times \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(\\text{BP}\\) is the brevity penalty.\n\n          \n\\(p_n\\) is the modified n-gram precision.\n\n          \n\\(w_n\\) is the weight for each n-gram (typically uniform).\n\n        \n\n      \n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nReference\n: The cat is on the mat.\n\n      \nCandidate\n: The cat is on mat.\n\n      \nUnigram Precision\n: 5 matches out of 5 words = 1.0\n\n      \nBigram Precision\n: 4 matches out of 4 bigrams = 1.0\n\n      \nTrigram Precision\n: 3 matches out of 3 trigrams = 1.0\n\n      \n4-gram Precision\n: 2 matches out of 2 four-grams = 1.0\n\n      \nBrevity Penalty\n: Applied due to shorter length.\n\n      \nBLEU Score\n: Calculated by combining precisions and brevity penalty.\n\n      \nIn this example, despite high n-gram precision, the brevity penalty reduces the BLEU score to account for the missing word “the” before “mat.”\n\n    \n\n  \n\n\n\n\n\nROUGE Score\n\n\n\n\n  \nCategory\n: Recall-Oriented N-gram Evaluation\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate the ROUGE score.\n\n  \nDefinition\n: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic summarization and machine translation by comparing the overlap of n-grams between the candidate and reference texts.\n\n  \nMeasurement Methods\n: Common variants include:\n    \n\n      \nROUGE-N\n: Measures overlap of n-grams.\n\n      \nROUGE-L\n: Measures the longest common subsequence.\n\n      \nROUGE-S\n: Measures skip-bigram overlap.\n\n    \n\n  \n\n  \n\n    \nEvaluation Approach\n: For ROUGE-N, the recall is calculated as:\n\n\n\\[\\text{ROUGE-N} = \\frac{\\text{Number of matching n-grams}}{\\text{Total number of n-grams in reference}}\\]\n  \n\n  \nExample\n:\n    \n\n      \nReference\n: “The cat is on the mat.”\n\n      \nCandidate\n: “The cat is on mat.”\n\n      \nROUGE-1 (Unigram) Recall\n: 5 matches out of 6 unigrams = 0.833\n\n      \nROUGE-2 (Bigram) Recall\n: 4 matches out of 5 bigrams = 0.8\n\n      \nIn this example, the candidate misses the unigram “the” before “mat,” affecting the recall scores.\n\n    \n\n  \n\n\n\n\n\nString Presence\n\n\n\n\n  \nCategory\n: Keyword or Phrase Matching\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate string presence.\n\n  \nDefinition\n: The String Presence metric checks if the generated response contains specific reference text, such as certain keywords or phrases. It is useful in scenarios where ensuring the inclusion of particular content is essential.\n\n  \nMeasurement Methods\n: This is a binary metric that returns 1 if the reference string is present in the response and 0 otherwise.\n\n  \nEvaluation Approach\n: The presence of the reference string is verified within the candidate response.\n\n  \nExample\n:\n    \n\n      \nReference\n: “climate change”\n\n      \nCandidate\n: The recent study highlights the impacts of climate change on polar bears.\n\n      \nString Presence Score\n: 1 (since “climate change” is present in the candidate).\n\n    \n\n  \n\n\n\n\n\nExact Match\n\n\n\n\n  \nCategory\n: Strict Matching Evaluation\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate exact matches.\n\n  \nDefinition\n: The Exact Match metric assesses whether the generated response is identical to the reference text. It is particularly useful in scenarios requiring precise outputs, such as predefined answers or specific commands.\n\n  \nMeasurement Methods\n: This binary metric returns 1 if the candidate text matches the reference text exactly and 0 otherwise.\n\n  \nEvaluation Approach\n: A direct comparison is made between the candidate and reference texts.\n\n  \nExample\n:\n    \n\n      \nReference\n: \\(E=mc^2\\)\n\n      \nCandidate\n: \\(E=mc^2\\)\n\n      \nExact Match Score\n: 1 (since the candidate matches the reference exactly).\n\n    \n\n  \n\n\n\n\n\nContext Entities Recall\n\n\n\n\n  \n\n    \nDefinition\n: Context Entities Recall is a metric that measures the recall of entities from the retrieved context compared to the ground truth. It calculates the fraction of entities in the ground truth that are also present in the context. This metric is crucial for scenarios where accurate entity retrieval is essential, such as tourism help desks or historical question answering.\n\n  \n\n  \nEvaluation Approach\n:\n    \n\n      \nTo compute this metric, two sets are used:\n        \n\n          \n\\(GE\\) (Ground Truth Entities): The set of entities present in the ground truth.\n\n          \n\\(CE\\) (Context Entities): The set of entities present in the retrieved context.\n\n        \n\n      \n\n      \n\n        \nThe Context Entities Recall is calculated using the formula:\n\n\n\\[\\text{Context Entity Recall} = \\frac{|CE \\cap GE|}{|GE|}\\]\n\n        \n\n          \nwhere, \\(\\mid CE \\cap GE\\mid\\) represents the number of entities common to both the context and the ground truth, while \\(\\mid GE\\mid\\) is the total number of entities in the ground truth.\n\n        \n\n      \n\n    \n\n  \n\n  \nExample\n \n(source)\n:\n    \n\n      \n\n        \nGround Truth\n: The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\n\n      \n\n      \n\n        \nHigh Entity Recall Context\n: The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it.\n\n      \n\n      \n\n        \nLow Entity Recall Context\n: The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.\n\n      \n\n      \nCalculation\n:\n        \n\n          \nEntities in Ground Truth (GE)\n: \n['Taj Mahal', 'Yamuna', 'Agra', '1631', 'Shah Jahan', 'Mumtaz Mahal']\n\n          \nEntities in High Entity Recall Context (CE1)\n: \n['Taj Mahal', 'Agra', 'Shah Jahan', 'Mumtaz Mahal', 'India']\n\n          \nEntities in Low Entity Recall Context (CE2)\n: \n['Taj Mahal', 'UNESCO', 'India']\n\n        \n\n      \n\n      \n\n        \nContext Entity Recall - 1\n:\n\n\n\\[\\text{Context Entity Recall - 1} = \\frac{|CE1 \\cap GE|}{|GE|} = \\frac{4}{6} = 0.666\\]\n      \n\n      \n\n        \nContext Entity Recall - 2\n:\n\n\n\\[\\text{Context Entity Recall - 2} = \\frac{|CE2 \\cap GE|}{|GE|} = \\frac{1}{6} = 0.166\\]\n      \n\n      \nThe first context demonstrates a higher entity recall, indicating better entity coverage in comparison to the ground truth. If these contexts were generated by different retrieval mechanisms, the first mechanism would be deemed superior for applications where entity accuracy is crucial.\n\n    \n\n  \n\n\n\n\n\n\n\n\nMultimodal Input Handling\n\n\n\n\n  \nRAG traditionally focuses on textual inputs. However, real-world scenarios frequently involve multimodal inputs, particularly text combined with images. Consider queries such as “What brand are the shoes in this image?”, “Describe the issue shown in the screenshot and suggest how to fix it”, and “Provide nutritional details for the meal shown here.” Addressing these queries requires handling both text and visual elements simultaneously.\n\n  \nIntegrating multimodal embeddings in RAG systems enables robust and precise handling of queries containing both visual and textual elements, significantly enhancing retrieval accuracy and the overall quality of generated responses.\n\n\n\n\n\nFlow of Multimodal Input\n\n\n\n\n  \nQuery Input\n:\n    \n\n      \nThe user submits a query comprising text and an image. For example, a user might upload a picture of a jacket alongside the text query, “Is this jacket available in waterproof material?”\n\n    \n\n  \n\n  \nEmbedding Multimodal Input\n:\n    \n\n      \nBoth text and image inputs need to be converted into embeddings to capture their semantic essence. This typically involves:\n        \n\n          \nText Embedding\n: Utilizing models like Sentence-BERT or GPT embeddings to create dense vectors representing the semantic meaning of the textual query.\n\n          \nImage Embedding\n: Using visual embedding models such as CLIP (Contrastive Language-Image Pre-training), ViT (Vision Transformer), or ResNet variants. These models process images to create dense vector representations capturing visual features.\n\n        \n\n      \n\n      \nThe resulting embeddings are then concatenated or fused into a single multimodal embedding vector. This fusion captures both the textual semantics and the visual features coherently.\n\n    \n\n  \n\n  \nStorage and Retrieval from Vector Database\n:\n    \n\n      \nThe multimodal embeddings are stored in a vector database, similar to text-only scenarios.\n\n      \nDuring retrieval, multimodal embeddings derived from user queries are compared against the stored embeddings in the database.\n\n    \n\n  \n\n  \nSimilarity Matching via Cosine Similarity\n:\n    \n\n      \nRetrieval involves computing cosine similarity between the multimodal query embedding and the embeddings stored in the vector database.\n\n      \nCosine similarity effectively measures semantic and visual similarity, ensuring retrieved items closely align with both textual context and visual content of the query.\n\n    \n\n  \n\n  \nRanked Results and Response Generation\n:\n    \n\n      \nItems with the highest similarity scores – provide specific product details (e.g., material information, waterproof ratings) – are retrieved and ranked according to relevance.\n\n      \nThese ranked results are then fed into an LLM to synthesize contextually accurate and visually informed responses. The final response leverages the multimodal context to precisely answer queries such as material specifications or availability in different sizes or colors, with a coherent respons such ase: “Yes, this particular jacket model is made from Gore-Tex, which is fully waterproof.”\n\n    \n\n  \n\n\n\n\n\nBenefits of Multimodal Embeddings in RAG\n\n\n\n\n  \nEnhanced User Experience\n: Allows users to naturally query using images, which often conveys more information than text alone.\n\n  \nPrecision and Relevance\n: Combining textual semantics and visual features significantly enhances retrieval accuracy.\n\n  \nScalable Solution\n: Multimodal embeddings can seamlessly integrate with existing vector databases, offering scalability and performance optimization.\n\n\n\n\n\nMultimodal RAG\n\n\n\n\n  \nMany documents contain a mixture of content types, including text and images. Yet, information captured in images is lost in most RAG applications. With the emergence of multimodal LLMs, like GPT-4V, it is worth considering how to utilize images in RAG.\n\n  \nHere are three ways to use images in RAG:\n    \n\n      \nOption 1:\n\n        \n\n          \nUse multimodal embeddings (such as CLIP) to embed images and text.\n\n          \nRetrieve both using similarity search.\n\n          \nPass raw images and text chunks to a multimodal LLM for answer synthesis.\n\n        \n\n      \n\n      \nOption 2:\n\n        \n\n          \nUse a multimodal LLM (such as GPT-4V, LLaVA, or Fuyu-8b) to produce text summaries from images.\n\n          \nEmbed and retrieve text.\n\n          \nPass text chunks to an LLM for answer synthesis.\n\n        \n\n      \n\n      \nOption 3:\n\n        \n\n          \nUse a multimodal LLM (such as GPT-4V, LLaVA, or Fuyu-8b) to produce text summaries from images.\n\n          \nEmbed and retrieve image summaries with a reference to the raw image. You can use a \nmulti-vector retriever\n with a Vector DB such as \nChroma\n to store raw text and images along with their summaries for retrieval.\n\n          \nPass raw images and text chunks to a multimodal LLM for answer synthesis.\n\n        \n\n      \n\n    \n\n  \n\n  \nOption 2 is appropriate for cases when a multi-modal LLM cannot be used for answer synthesis (e.g., cost, etc).\n\n  \nThe following figure \n(source)\n offers an overview of all three aforementioned options.\n\n\n\n\n\n\n\n\n\n  \nLangChain offers cookbooks for \nOption 1\n and \nOption 3\n.\n\n  \nThe following infographic \n(source)\n also offers a top-level overview of Multimodal RAG:\n\n\n\n\n\n\n\n\nAgentic Retrieval-Augmented Generation\n\n\n\n\n  \n\n    \nAgent-based Retrieval-Augmented Generation (RAG), or Agentic RAG, represents an advanced approach in AI that enhances the traditional RAG pipeline with intelligent agents. In conventional RAG systems, an AI model queries a knowledge base to retrieve relevant information and generate responses. However, Agentic RAG extends beyond this by employing AI agents capable of orchestrating multi-step retrieval processes, utilizing external tools, and dynamically adapting to the query. This added layer of autonomy enables advanced reasoning, decision-making, and adaptability, allowing the system to handle complex queries and diverse data sources with greater precision and responsiveness.\n\n  \n\n  \n\n    \nBy integrating AI agents, Agentic RAG transforms traditional RAG, providing a flexible, intelligent solution for nuanced, real-world inquiries. This shift enables organizations to deploy AI systems with a higher degree of accuracy, flexibility, and intelligence, allowing them to tackle intricate tasks and deliver more precise results across a wide range of applications.\n\n  \n\n\n\n\n\nHow Agentic RAG Works\n\n\n\n\n  \n\n    \nIn an agentic RAG system, AI agents play key roles in the retrieval process, using specialized tools to retrieve context-sensitive information. Unlike traditional RAG, where retrieval functions are static, agentic RAG allows dynamic selection and operation of tools based on query requirements. Retrieval agents may utilize tools such as:\n\n\n    \n\n      \nVector Search Engines\n: Retrieve information from vectorized data in databases.\n\n      \nWeb Search Tools\n: Access live web data for up-to-date, contextually relevant information.\n\n      \nCalculators\n: Perform computations for queries that require accurate calculation.\n\n      \nAPIs for Software Programs\n: Programmatically retrieve information from applications like email or chat programs to access user-specific data.\n\n    \n\n  \n\n  \n\n    \nIn the context of Agentic RAG, the retrieval process is “agentic,” meaning agents are capable of reasoning and decision-making regarding which sources and tools to use, based on the specific requirements of the query. This flexibility elevates their tool usage beyond simple retrieval, allowing for a more dynamic and adaptive response.\n\n  \n\n\n\n\n\nAgentic Decision-Making in Retrieval\n\n\n\n\n  \n\n    \nThe decision-making process of retrieval agents encompasses several key actions, including:\n\n\n    \n\n      \nDeciding Whether to Retrieve\n: Assessing if additional information is necessary for the query.\n\n      \nChoosing the Appropriate Tool\n: Selecting the most suitable tool (e.g., a vector search engine or web search) based on the query.\n\n      \nQuery Formulation\n: Refining or rephrasing the query to enhance retrieval accuracy.\n\n      \nEvaluating Retrieved Results\n: Reviewing the retrieved information to determine sufficiency, and whether further retrieval is needed.\n\n    \n\n  \n\n\n\n\n\nAgentic RAG Architectures: Single-Agent vs. Multi-Agent Systems\n\n\n\n\n  \nAgentic RAG can be implemented with a single agent or multiple agents, each offering unique strengths.\n\n\n\n\n\nSingle-Agent RAG (Router)\n\n\n\n\n  \nThe simplest implementation of agentic RAG involves a single agent functioning as a “router.” This agent determines the appropriate source or tool for retrieving information based on the query. The single agent toggles between different options, such as a vector database, web search, or an API. This setup provides a versatile retrieval process, enabling access to multiple data sources beyond a single vector search tool.\n\n  \nAs shown in the figure below (\nsource\n), the single-agent RAG system (router) architecture involves a single agent serving as a “router,” dynamically selecting the best tool or source based on the query, enabling efficient information retrieval across multiple data channels.\n\n\n\n\n\n\n\n\nMulti-Agent RAG Systems\n\n\n\n\n  \n\n    \nFor more complex queries, multi-agent RAG systems provide additional flexibility. These systems feature a “master agent” that coordinates several specialized retrieval agents, such as:\n\n\n    \n\n      \nInternal Data Retrieval Agent\n: Retrieves information from proprietary, internal databases.\n\n      \nPersonal Data Retrieval Agent\n: Accesses user-specific information, such as emails or chat history.\n\n      \nPublic Data Retrieval Agent\n: Conducts web searches for up-to-date public information.\n\n    \n\n  \n\n  \n\n    \nBy utilizing multiple agents tailored to specific sources or tasks, multi-agent RAG systems can deliver comprehensive, accurate responses across diverse channels.\n\n  \n\n  \n\n    \nAs shown in the figure below (\nsource\n), the multi-agent RAG system architecture utilizes multiple specialized retrieval agents to access different sources and tools, offering a flexible and comprehensive approach to complex queries.\n\n  \n\n\n\n\n\n\n\n\nBeyond Retrieval: Expanding Agentic RAG’s Capabilities\n\n\n\n\n  \n\n    \nAgentic RAG systems can incorporate agents for tasks beyond retrieval, including:\n\n\n    \n\n      \nValidating Information\n: Cross-referencing data across sources to ensure accuracy.\n\n      \nPerforming Multi-step Reasoning\n: Following logical steps to address complex queries before generating responses.\n\n      \nUpdating System Memory\n: Tracking and retaining user-specific preferences or past queries, enabling personalized and context-aware responses.\n\n    \n\n  \n\n  \n\n    \nBy expanding its capabilities beyond simple retrieval, Agentic RAG delivers a powerful, context-sensitive AI solution capable of handling intricate, real-world applications.\n\n  \n\n\n\n\n\nAgentic RAG vs. Vanilla RAG: Key Differences\n\n\n\n\n  \nWhile both vanilla and agentic RAG systems aim to retrieve information and generate responses, agentic RAG introduces several significant enhancements:\n\n\n\n\n\n\n\n\n \n\n\n\n\nFeature\n\n\nVanilla RAG\n\n\nAgentic RAG\n\n\n\n\n\n\n\n\n\n\nAccess to External Tools\n\n\nNo\n\n\nYes – Utilizes external tools like vector search engines, web search tools, calculators, and APIs.\n\n\n\n\n\n\nQuery Pre-processing\n\n\nNo\n\n\nYes – Agents dynamically refine, rephrase, and adapt queries for optimized retrieval.\n\n\n\n\n\n\nDecision-making in Retrieval\n\n\nLimited to direct retrieval from knowledge base\n\n\nAgents autonomously decide if retrieval is needed, select tools, and adapt based on query complexity and source type.\n\n\n\n\n\n\nMulti-step Retrieval Process\n\n\nNo\n\n\nYes – Agents perform multi-step, adaptive retrieval processes involving various sources or tool combinations.\n\n\n\n\n\n\nData Validation\n\n\nNo\n\n\nYes – Information is cross-referenced across sources to validate accuracy, supporting complex, real-world responses.\n\n\n\n\n\n\nDynamic Tool Selection\n\n\nStatic retrieval tools only\n\n\nDynamic – Agents choose specific tools (e.g., vector search, APIs) based on query needs.\n\n\n\n\n\n\nAdaptability to Query\n\n\nLimited\n\n\nHighly adaptive – Agents select and operate tools based on real-time assessment of query requirements.\n\n\n\n\n\n\nTypes of Agents\n\n\nNot applicable\n\n\nMultiple specialized agents, such as internal data retrieval, personal data retrieval, public data retrieval.\n\n\n\n\n\n\nSingle-Agent vs. Multi-Agent System\n\n\nNot applicable\n\n\nSingle-agent router or multi-agent systems, with “master” and specialized agents for complex queries.\n\n\n\n\n\n\nReasoning and Logic Capability\n\n\nNo\n\n\nYes – Supports multi-step reasoning, allowing logical sequence handling before generating responses.\n\n\n\n\n\n\nMemory and Personalization\n\n\nLimited to immediate query\n\n\nYes – Capable of updating memory to retain user preferences or history, allowing personalized responses.\n\n\n\n\n\n\nReal-world Applications\n\n\nPrimarily static responses from a fixed database\n\n\nSupports a wide range of real-world applications by responding to complex, nuanced inquiries with context sensitivity.\n\n\n\n\n\n\n\n\n\n\n\n\n  \nDrawing a parallel with problem-solving, agentic RAG offers capabilities akin to having a smartphone in hand—equipped with multiple apps and tools to help answer a question—whereas vanilla RAG is akin to being in a library with limited resources.\n\n\n\n\n\nImplementing Agentic RAG: Key Approaches\n\n\n\n\n  \nTo implement agentic RAG, developers can use either language models with function calling or agent frameworks, each providing specific advantages in terms of flexibility and control.\n\n  \nBoth methods—function calling in language models and agent frameworks—enable agentic RAG, though each has unique benefits:\n    \n\n      \nFunction Calling\n provides control over each tool interaction, suitable for cases with specific tool chains or simple agent setups.\n\n      \nAgent Frameworks\n offer pre-built integrations and routing logic, ideal for larger, multi-agent architectures.\n\n    \n\n  \n\n  \nUsing these implementations, developers can build flexible and adaptive agentic RAG pipelines, enhancing retrieval, reasoning, and response generation capabilities for AI-driven applications.\n\n\n\n\n\nLanguage Models with Function Calling\n\n\n\n\n  \nFunction calling allows language models to interact directly with external tools. For example, OpenAI’s function calling for GPT-4 or Cohere’s connectors API lets developers connect language models to databases, calculators, and other services. This interaction involves defining a function (such as querying a database), passing it to the model via a schema, and routing the model’s queries through the defined functions. This approach enables the model to leverage specific tools as needed, based on the query.\n\n\n\n\n\nAgent Frameworks\n\n\n\n\n  \nSeveral agent frameworks—such as LangChain, LlamaIndex, CrewAI—simplify agentic RAG implementation by providing pre-built templates and tool integrations. Key features include:\n    \n\n      \nLangChain\n: Offers support for language model tools, and its LCEL and LangGraph frameworks integrate these tools seamlessly.\n\n      \nLlamaIndex\n: Provides a QueryEngineTool to streamline retrieval tasks.\n\n      \nCrewAI\n: A leading framework for multi-agent setups, which supports shared tool access among agents.\n\n    \n\n  \n\n\n\n\n\nEnterprise-driven Adoption\n\n\n\n\n  \nOrganizations are increasingly transitioning to agentic RAG to gain more autonomous and accurate AI-driven systems. Enterprises such as Microsoft and Replit have introduced agents to enhance task completion and software development assistance. With agentic RAG, companies can build AI applications capable of handling diverse, real-time data sources, providing robust and adaptable responses for complex queries and tasks.\n\n\n\n\n\nBenefits\n\n\n\n\n  \nThe primary benefits of agentic RAG include:\n    \n\n      \nEnhanced Retrieval Accuracy\n: By routing queries through specialized agents, agentic RAG can provide more accurate responses.\n\n      \nAutonomous Task Performance\n: Agents can perform multi-step reasoning, independently solving complex problems.\n\n      \nImproved Collaboration\n: These systems can better assist users by handling more varied and personalized queries.\n\n    \n\n  \n\n\n\n\n\nLimitations\n\n\n\n\n  \nAgentic RAG does present challenges, such as:\n    \n\n      \nIncreased Latency\n: Running multiple agents and interacting with tools can add delays to the response.\n\n      \nReliability of Agents\n: Depending on the LLM’s reasoning capabilities, agents may fail to complete certain tasks accurately.\n\n      \nComplexity in Error Handling\n: Systems need robust fallback mechanisms to recover if an agent fails to retrieve or process data.\n\n    \n\n  \n\n\n\n\n\nCode\n\n\n\n\n  \nImplementing agentic RAG requires setting up an agent framework capable of handling tool integrations and coordinating retrieval processes. This section walks through an example code setup, demonstrating both language models with function calling and agent frameworks for building an agentic RAG pipeline.\n\n\n\n\n\nImplementing Agentic RAG with Function Calling\n\n\n\n\n  \n\n    \nFunction calling in language models allows them to interact with tools by defining functions that retrieve data from external sources. This method leverages API calls, database queries, and computation tools to enrich the response with dynamic data.\n\n  \n\n  \n\n    \nHere’s an example implementation using a function for retrieval from a database via the Weaviate vector search API.\n\n  \n\n\n\n\n\nDefine the Function for Retrieval\n\n\n\n\n  \nTo start, we define a function that uses Weaviate’s hybrid search to query a database and retrieve relevant results.\n\n\n\n\n\ndef\n \nget_search_results\n(\nquery\n:\n \nstr\n)\n \n->\n \nstr\n:\n\n    \n\"\"\"Sends a query to Weaviate's Hybrid Search. Parses the response into a formatted string.\"\"\"\n\n\n    \nresponse\n \n=\n \nblogs\n.\nquery\n.\nhybrid\n(\nquery\n,\n \nlimit\n=\n5\n)\n  \n# Retrieve top 5 results based on the query\n\n    \nstringified_response\n \n=\n \n\"\"\n\n    \nfor\n \nidx\n,\n \no\n \nin\n \nenumerate\n(\nresponse\n.\nobjects\n):\n\n        \nstringified_response\n \n+=\n \nf\n\"Search Result \n{\nidx\n+\n1\n}\n:\n\\n\n\"\n\n        \nfor\n \nprop\n \nin\n \no\n.\nproperties\n:\n\n            \nstringified_response\n \n+=\n \nf\n\"\n{\nprop\n}\n: \n{\no\n.\nproperties\n[\nprop\n]\n}\n\\n\n\"\n\n        \nstringified_response\n \n+=\n \n\"\n\\n\n\"\n\n\n    \nreturn\n \nstringified_response\n\n\n\n\n\nDefine the Tools Schema\n\n\n\n\n  \nNext, we define a tools schema that connects the function to the language model. This schema tells the model how to use the function for retrieving data.\n\n\n\n\n\ntools_schema\n \n=\n \n[{\n\n    \n'type'\n:\n \n'function'\n,\n\n    \n'function'\n:\n \n{\n\n        \n'name'\n:\n \n'get_search_results'\n,\n\n        \n'description'\n:\n \n'Get search results for a provided query.'\n,\n\n        \n'parameters'\n:\n \n{\n\n          \n'type'\n:\n \n'object'\n,\n\n          \n'properties'\n:\n \n{\n\n            \n'query'\n:\n \n{\n\n              \n'type'\n:\n \n'string'\n,\n\n              \n'description'\n:\n \n'The search query.'\n,\n\n            \n},\n\n          \n},\n\n          \n'required'\n:\n \n[\n'query'\n],\n\n        \n},\n\n    \n},\n\n\n}]\n\n\n\n\n\nSetting Up the Interaction Loop\n\n\n\n\n  \nTo ensure the model can call the tool multiple times (if needed), we set up a loop that enables the model to interact with tools and retrieve data iteratively until it has all necessary information.\n\n\n\n\n\ndef\n \nollama_generation_with_tools\n(\nuser_message\n:\n \nstr\n,\n \ntools_schema\n:\n \nlist\n,\n \ntool_mapping\n:\n \ndict\n,\n \nmodel_name\n:\n \nstr\n \n=\n \n\"llama3.1\"\n)\n \n->\n \nstr\n:\n\n    \nmessages\n \n=\n \n[{\n\"role\"\n:\n \n\"user\"\n,\n \n\"content\"\n:\n \nuser_message\n}]\n\n    \nresponse\n \n=\n \nollama\n.\nchat\n(\nmodel\n=\nmodel_name\n,\n \nmessages\n=\nmessages\n,\n \ntools\n=\ntools_schema\n)\n\n    \n    \n# Check if the model needs to use a tool\n\n    \nif\n \nnot\n \nresponse\n[\n\"message\"\n].\nget\n(\n\"tool_calls\"\n):\n\n        \nreturn\n \nresponse\n[\n\"message\"\n][\n\"content\"\n]\n\n    \n    \n# Handle tool calls and retrieve information\n\n    \nfor\n \ntool\n \nin\n \nresponse\n[\n\"message\"\n][\n\"tool_calls\"\n]:\n\n        \nfunction_to_call\n \n=\n \ntool_mapping\n[\ntool\n[\n\"function\"\n][\n\"name\"\n]]\n\n        \nfunction_response\n \n=\n \nfunction_to_call\n(\ntool\n[\n\"function\"\n][\n\"arguments\"\n][\n\"query\"\n])\n\n        \nmessages\n.\nappend\n({\n\"role\"\n:\n \n\"tool\"\n,\n \n\"content\"\n:\n \nfunction_response\n})\n\n    \n    \n# Generate final response after tool calls\n\n    \nfinal_response\n \n=\n \nollama\n.\nchat\n(\nmodel\n=\nmodel_name\n,\n \nmessages\n=\nmessages\n)\n\n    \nreturn\n \nfinal_response\n[\n\"message\"\n][\n\"content\"\n]\n\n\n\n\n\nExecuting the Agentic RAG Query\n\n\n\n\n  \nFinally, we run the function, allowing the language model to interact with the \nget_search_results\n tool.\n\n\n\n\n\ntool_mapping\n \n=\n \n{\n\"get_search_results\"\n:\n \nget_search_results\n}\n  \n# Maps tool name to function\n\nresponse\n \n=\n \nollama_generation_with_tools\n(\n\n    \n\"How is HNSW different from DiskANN?\"\n,\n\n    \ntools_schema\n=\ntools_schema\n,\n\n    \ntool_mapping\n=\ntool_mapping\n\n\n)\n\n\nprint\n(\nresponse\n)\n\n\n\n\n\n\n  \nThis setup enables the language model to retrieve dynamic information and perform tool-based retrievals as needed.\n\n\n\n\n\nImplementing Agentic RAG with Agent Frameworks\n\n\n\n\n  \nUsing agent frameworks streamlines the implementation process by providing templates and pre-built modules for multi-agent orchestration. Here’s how to set up an agentic RAG pipeline using LangChain as an example.\n\n\n\n\n\nStep 1: Define Agents and Tools\n\n\n\n\n  \nLangChain simplifies agentic RAG by managing tools and routing tasks. First, define the agents and register the tools they will use.\n\n\n\n\n\nfrom\n \nlangchain.tools\n \nimport\n \nWebSearchTool\n,\n \nDatabaseTool\n,\n \nCalculatorTool\n\n\nfrom\n \nlangchain.agents\n \nimport\n \nAgent\n\n\n\n# Define tools for retrieval\n\nweb_search_tool\n \n=\n \nWebSearchTool\n(\napi_key\n=\n\"YOUR_WEB_SEARCH_API_KEY\"\n)\n\n\ndatabase_tool\n \n=\n \nDatabaseTool\n(\ndb_client\n=\n\"your_database_client\"\n)\n\n\ncalculator_tool\n \n=\n \nCalculatorTool\n()\n\n\n\n# Set up an agent with a routing function\n\nretrieval_agent\n \n=\n \nAgent\n(\n\n    \ntools\n=\n[\nweb_search_tool\n,\n \ndatabase_tool\n,\n \ncalculator_tool\n],\n\n    \nrouting_function\n=\n\"retrieve_and_select_tool\"\n\n\n)\n\n\n\n\n\nStep 2: Configure Agent Routing\n\n\n\n\n  \nSet up the routing function to let the agent decide which tool to use based on the input query.\n\n\n\n\n\ndef\n \nretrieve_and_select_tool\n(\nquery\n):\n\n    \nif\n \n\"calculate\"\n \nin\n \nquery\n:\n\n        \nreturn\n \ncalculator_tool\n\n    \nelif\n \n\"web\"\n \nin\n \nquery\n:\n\n        \nreturn\n \nweb_search_tool\n\n    \nelse\n:\n\n        \nreturn\n \ndatabase_tool\n\n\n\n\n\nStep 3: Chain Agents for Multi-Agent RAG\n\n\n\n\n  \nIn multi-agent RAG, you might have a “master agent” that routes queries to specialized agents based on query type. Here’s how to set up a master agent to coordinate multiple agents.\n\n\n\n\n\nfrom\n \nlangchain.agents\n \nimport\n \nMultiAgent\n\n\n\n# Define specialized agents\n\ninternal_agent\n \n=\n \nAgent\n(\ntools\n=\n[\ndatabase_tool\n],\n \nrouting_function\n=\n\"database_retrieval\"\n)\n\n\npublic_agent\n \n=\n \nAgent\n(\ntools\n=\n[\nweb_search_tool\n],\n \nrouting_function\n=\n\"web_retrieval\"\n)\n\n\n\n# Create a master agent to coordinate retrieval\n\nmaster_agent\n \n=\n \nMultiAgent\n(\nagents\n=\n[\ninternal_agent\n,\n \npublic_agent\n])\n\n\n\n# Function to handle a query using master agent\n\ndef\n \nhandle_query_with_master_agent\n(\nquery\n):\n\n    \nreturn\n \nmaster_agent\n.\nhandle_query\n(\nquery\n)\n\n\n\n\n\nRunning the Multi-Agent Query\n\n\n\n\n  \nFinally, to test the system, input a query and let the master agent route it appropriately:\n\n\n\n\n\nresponse\n \n=\n \nhandle_query_with_master_agent\n(\n\"Find recent studies on neural networks\"\n)\n\n\nprint\n(\nresponse\n)\n\n\n\n\n\nDisadvantages of Agentic RAG\n\n\n\n\n  \n\n    \nDespite its advantages, agentic RAG comes with several limitations that should be carefully considered, particularly for time-sensitive applications:\n\n\n    \n\n      \n\n        \nIncreased Latency\n: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.\n\n      \n\n      \n\n        \nHigher Computational Cost\n: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.\n\n      \n\n      \n\n        \nProduction Feasibility\n: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.\n\n      \n\n    \n\n  \n\n  \n\n    \nWhile these drawbacks limit agentic RAG’s use in certain scenarios, its capability to generate high-quality, well-researched responses can make it worthwhile in contexts where response time is less critical and information accuracy is paramount.\n\n  \n\n\n\n\n\nSummary\n\n\n\n\n  \nAgentic RAG refers to an agent-based implementation of RAG. AI agents are entities tasked with accomplishing specific objectives. These agents are often equipped with memory and tools, which they can utilize to carry out their tasks effectively. Among these tools, one significant capability is the ability to retrieve information from various sources, such as web searches or internal documents.\n\n  \nIn the context of agentic RAG, the “retrieval becomes agentic.” This implies that the AI agent is capable of reasoning and making decisions regarding which sources are most appropriate for retrieving the required information. The agent’s tool usage evolves beyond simple information retrieval, becoming more flexible and dynamic.\n\n  \nThe distinction between standard and agentic RAG can be summarized as follows:\n    \n\n      \nCommon RAG\n: The user input prompts a single call to a database, retrieving additional information in response to the query.\n\n      \nAgentic RAG\n: The agent is able to deliberate on which source is the most suitable for retrieving information based on the query, providing a more sophisticated and adaptable approach.\n\n    \n\n  \n\n  \nThe following figure (\nsource\n) offers a visual summary of Agentic RAG:\n\n\n\n\n\n\n\n\nRAG vs. Long Context Windows\n\n\n\nComputational Cost\n\n\n\n\n  \nProcessing extremely long contexts incurs substantial computational overhead. For instance, utilizing a 10 million token context window with state-of-the-art models like Llama 4 demands considerable hardware resources—approximately 32 H100 GPUs—which translates to over $100 per hour in inference costs. The key-value (KV) cache alone can exceed 1 terabyte of VRAM. These requirements pose a significant barrier to the practical deployment of long-context inference systems at scale, especially for organizations with limited infrastructure budgets.\n\n\n\n\n\nInference Latency and Throughput\n\n\n\n\n  \nAs the number of tokens increases, the latency of inference rises proportionally, often leading to a considerable decline in throughput. Even when hardware resources are available, this degradation in response time can negatively impact user experience in latency-sensitive applications such as virtual assistants, search engines, or real-time analytics systems.\n\n\n\n\n\nContextual Comprehension and Model Training Limitations\n\n\n\n\n  \nAlthough large context windows are theoretically capable of accommodating vast amounts of input data, current LLMs are typically trained on much smaller maximum context lengths—commonly up to 128,000 tokens. Consequently, performance across the full extent of a 10 million token context window is unproven and likely suboptimal. Empirical studies suggest that retrieval accuracy tends to diminish for information placed in the middle of a long context due to a phenomenon informally referred to as the “Lost in the Middle” effect. Therefore, while long-context architectures offer the promise of expanded capacity, their practical utility is constrained by training regimes and architectural bottlenecks.\n\n\n\n\n\nRAG as a Targeted, Cost-Efficient Solution\n\n\n\n\n  \nIn contrast, RAG provides a principled and efficient mechanism for narrowing down relevant content from a large corpus before conditioning the model’s generative process. By introducing a retrieval stage that identifies and ranks the most pertinent information, RAG minimizes unnecessary context, optimizes for response accuracy, and reduces memory and compute demands. This retrieval-first approach allows RAG systems to operate effectively within the token limitations of current LLMs, while maintaining scalability and affordability.\n\n\n\n\n\nImproving RAG Systems\n\n\n\n\n  \nTo enhance and refine RAG systems, consider the following three structured methods, each accompanied by comprehensive guides and practical implementations:\n    \n\n      \nRe-ranking Retrieved Results\n: A fundamental and effective method involves employing a Re-ranking Model to refine the results obtained through initial retrieval. This approach prioritizes more relevant results, thereby improving the overall quality of the generated content. MonoT5, MonoBERT, DuoBERT, etc. are examples of deep models that can be used as re-rankers. For a detailed exploration of this technique, refer to the \nguide and code example\n provided by Mahesh Deshwal. A detailed discourse on re-ranking is available in the \nRe-ranking\n section.\n\n      \nFLARE Technique\n: Subsequent to re-ranking, one should explore the FLARE methodology. This technique dynamically queries the internet (could also be a local knowledge base) whenever the confidence level of a segment of the generated content falls below a specified threshold. This overcomes a significant limitation of conventional RAG systems, which typically query the knowledge base only at the outset and subsequently produce the final output. Akash Desai’s \nguide and code walkthrough\n offer an insightful understanding and practical application of this technique. More on the FLARE technique in the \nActive Retrieval Augmented Generation\n section.\n\n      \nHyDE Approach\n: Finally, the HyDE technique introduces an innovative concept of generating a hypothetical document in response to a query. This document is then converted into an embedding vector. The uniqueness of this method lies in using the vector to identify a similar neighborhood within the corpus embedding space, thereby retrieving analogous real documents based on vector similarity. To delve into this method, refer to Akash Desai’s \nguide and code implementation\n. More on the HyDE technique in the \nPrecise Zero-Shot Dense Retrieval Without Relevance Labels\n section.\n\n    \n\n  \n\n  \nEach of these methods offers a unique approach to refining RAG systems, contributing to more accurate and contextually relevant results.\n\n\n\n\n\nRAG 2.0\n\n\n\n\n  \nRAG 2.0\n, unveiled by \nContextual AI\n, represents a significant advancement in robust AI systems for enterprise use, optimizing the entire system end-to-end unlike its predecessor. This new generation introduces Contextual Language Models (CLMs) which not only surpass the original RAG benchmarks but also outperform the strongest available models based on GPT-4, across various industry benchmarks, demonstrating superior performance in open domain question-answering and specialized tasks like truth verification.\n\n  \nThe introduction of RAG 2.0 marks a departure from the use of off-the-shelf models and disjointed components, which characterized previous systems as brittle and suboptimal for production environments. Instead, RAG 2.0 end-to-end optimizes the language model and retriever as a single system.\n\n  \nKey improvements are evident in real-world applications where RAG 2.0 CLMs have been deployed. Using Google Cloud’s latest ML infrastructure, these models have shown significant accuracy enhancements, particularly in sectors like finance and law, highlighting their potential in specialized domains.\n\n  \nFurther comparisons reveal that RAG 2.0 significantly outperforms traditional long-context models, providing higher accuracy with less computational demand. This makes RAG 2.0 particularly appealing for scaling in production environments.\n\n  \nOverall, RAG 2.0’s innovative approach not only pushes the boundaries of generative AI in production settings but also demonstrates its superiority through extensive benchmarks and real-world deployments, inviting enterprises to join in its ongoing development and application.\n\n\n\n\n\nSelected Papers\n\n\n\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\n\n\n\n\n  \nThe paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.\n\n  \nAddressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.\n\n  \nThe RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.\n\n  \nThe retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.\n\n  \nRAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.\n\n  \nA summary of the methods and models used for query/document embedding and retrieval, as well as the end-to-end structure of the RAG framework is as below:\n    \n\n      \nQuery/Document Embedding:\n\n        \n\n          \nThe retrieval component, Dense Passage Retriever (DPR), follows a bi-encoder architecture.\n\n          \nDPR uses BERTBASE as the foundation for both document and query encoders.\n\n          \nFor a document \\(z\\), a dense representation \\(d(z)\\) is produced by a document encoder, \\(BERT_d\\).\n\n          \nFor a query \\(x\\), a query representation \\(q(x)\\) is produced by a query encoder, \\(BERT_q\\).\n\n          \nThe embeddings are created such that relevant documents for a given query are close in the embedding space, allowing effective retrieval.\n\n        \n\n      \n\n      \nRetrieval Process:\n\n        \n\n          \nThe retrieval process involves calculating the top-\\(k\\) documents with the highest prior probability, which is essentially a Maximum Inner Product Search (MIPS) problem.\n\n          \nThe MIPS problem is solved approximately in sub-linear time to efficiently retrieve relevant documents.\n\n        \n\n      \n\n      \nEnd-to-End Structure:\n\n        \n\n          \nThe RAG model uses the input sequence \\(x\\) to retrieve text documents \\(z\\), which are then used as additional context for generating the target sequence \\(y\\).\n\n          \nThe generator component is modeled using BART-large, a pre-trained seq2seq transformer with 400M parameters. BART-large combines the input \\(x\\)with the retrieved content \\(z\\) for generation.\n\n          \nThe RAG-Sequence model uses the same retrieved document for generating the complete sequence, while the RAG-Token model can use different passages per token.\n\n          \nThe training process involves jointly training the retriever and generator components without direct supervision on what document should be retrieved. The training minimizes the negative marginal log-likelihood of each target using stochastic gradient descent with Adam.\n\n          \nNotably, the document encoder BERTd is kept fixed during training, avoiding the need for periodic updates of the document index.\n\n        \n\n      \n\n    \n\n  \n\n  \nThe following figure from the paper illustrates an overview of the proposed approach. They combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query \\(x\\), they use Maximum Inner Product Search (MIPS) to find the top-\\(K\\) documents \\(z_i\\). For final prediction \\(y\\), they treat \\(z\\) as a latent variable and marginalize over seq2seq predictions given different documents.\n\n\n\n\n\n\n\n\n\n  \nIn open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.\n\n  \nRAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.\n\n  \nOn the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.\n\n  \nThis study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.\n\n  \nCode\n; \ninteractive demo\n.\n\n\n\n\n\nActive Retrieval Augmented Generation\n\n\n\n\n  \nDespite the remarkable ability of large language models (LLMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.\n\n  \nAugmenting LLMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LLMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries.\n\n  \nThis paper from Jiang et al. at CMU, Sea AI Lab, and Meta AI in EMNLP 2023 presents Forward-Looking Active REtrieval augmented generation (FLARE), a method addressing the tendency of large language models (LLMs) to produce factually inaccurate content.\n\n  \nFLARE iteratively uses predictions of upcoming sentences to actively decide when and what to retrieve across the generation process, enhancing LLMs with dynamic, multi-stage external information retrieval.\n\n  \nUnlike traditional retrieve-and-generate models that use fixed intervals or input-based retrieval, FLARE targets continual information gathering for long text generation, reducing hallucinations and factual inaccuracies.\n\n  \nThe system triggers retrieval when generating low-confidence tokens, determined by a probability threshold. This anticipates future content, forming queries to retrieve relevant documents for regeneration.\n\n  \nThe following figure from the paper illustrates FLARE. Starting with the user input \\(x\\) and initial retrieval results \\(D_x\\), FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence.\n\n\n\n\n\n\n\n\n\n  \nFLARE was tested on four long-form, knowledge-intensive generation tasks/datasets, exhibiting superior or competitive performance, demonstrating its effectiveness in addressing the limitations of existing retrieval-augmented LLMs.\n\n  \nThe model is adaptable to existing LLMs, as shown with its implementation on GPT-3.5, and employs off-the-shelf retrievers and the Bing search engine.\n\n  \nCode\n.\n\n\n\n\n\nMuRAG: Multimodal Retrieval-Augmented Generator\n\n\n\n\n  \nThis paper by Chen et al. from Google Research proposes Multimodal Retrieval-Augmented Transformer (MuRAG), which looks to extend the retrieval process beyond text to include other modalities like images or structured data, which can then be used alongside textual information to inform the generation process.\n\n  \nMuRAG’s magic lies in its two-phase training approach: pre-training and fine-tuning, each carefully crafted to build the model’s ability to tap into a vast expanse of multimodal knowledge.\n\n  \nThe key goal of MuRAG is to incorporate both visual and textual knowledge into language models to improve their capability for multimodal question answering.\n\n  \nMuRAG is distinct in its ability to access an external non-parametric multimodal memory (images and texts) to enhance language generation, addressing the limitations of text-only retrieval in previous models.\n\n  \nMuRAG has a dual-encoder architecture combines pre-trained visual transformer (ViT) and a text encoder (T5) models to create a backbone encoder, enabling the encoding of image-text pairs, image-only, and text-only inputs into a unified/joint multimodal representation.\n\n  \nMuRAG is pre-trained on a mixture of image-text data (LAION, Conceptual Captions) and text-only data (PAQ, VQA). It uses a contrastive loss for retrieving relevant knowledge and a generation loss for answer prediction. It employs a two-stage training pipeline: initial training with small in-batch memory followed by training with a large global memory.\n\n  \nDuring the retriever stage, MuRAG takes a query \\(q\\) of any modality as input and retrieves from a memory \\(\\mathcal{M}\\) of image-text pairs. Specifically, we apply the backbone encoder \\(f_\\theta\\) to encode a query \\(q\\), and use maximum inner product search (MIPS) over all of the memory candidates \\(m \\in \\mathcal{M}\\) to find the top-\\(k\\) nearest neighbors \\(\\operatorname{Top}_K(\\mathcal{M} \\mid q)=\\left[m_1, \\cdots, m_k\\right]\\). Formally, we define \\(\\operatorname{Top}_K(\\mathcal{M} \\mid q)\\) as follows:\n\n\n\n\n\\[\\operatorname{Top}_K(\\mathcal{M} \\mid q)=\\underset{m \\in \\mathcal{M}}{\\operatorname{Top}} \\quad f_\\theta(q)_{[\\mathrm{CLS}]} \\cdot f_\\theta(m)_{[\\mathrm{CLS}]}\\]\n\n\n\n  \n\n    \nDuring the reader stage, the retrievals (the raw image patches) are combined with the query \\(q\\) as an augmented input \\(\\left[m_1, \\cdots, m_k, q\\right]\\), which is fed to the backbone encoder \\(f_\\theta\\) to produce retrievalaugmented encoding. The decoder model \\(g_\\theta\\) uses attention over this representation to generate textual outputs \\(\\mathbf{y}=y_1, \\cdots, y_n\\) token by token.\n\n\n\\[p\\left(y_i \\mid y_{i-1}\\right)=g_\\theta\\left(y_i \\mid f_\\theta\\left(\\operatorname{Top}_K(\\mathcal{M} \\mid q) ; q\\right) ; y_{1: i-1}\\right)\\]\n\n    \n\n      \nwhere \\(y\\) is decoded from a given vocabulary \\(\\mathcal{V}\\).\n\n    \n\n  \n\n  \n\n    \nThe figure below from the original paper \n(source)\n shows how the model taps into an external repository to retrieve a diverse range of knowledge encapsulated within both images and textual fragments. This multimodal information is then employed to enhance the generative process. The upper section outlines the setup for the pre-training phase, whereas the lower section specifies the framework for the fine-tuning phase.\n\n  \n\n\n\n\n\n\n\n\n\n  \nThe process can be summarized as follows:\n    \n\n      \nFor retrieval, MuRAG uses maximum inner product search to find the top-\\(k\\) most relevant image-text pairs from the memory given a question. The “memory” here refers to the external knowledge base that the model can retrieve information from. Specifically, the memory contains a large collection of image-text pairs that are encoded offline by the backbone encoder prior to training.\n\n      \nDuring training and inference, given a question, MuRAG’s retriever module will search through this memory to find the most relevant image-text pairs using maximum inner product search.\n\n      \nThe memory serves as the knowledge source and can contain various types of multimodal data like images with captions, passages of text, tables, etc. that are related to the downstream task.\n\n      \nFor example, when fine-tuning on the WebQA dataset, the memory contains 1.1 million image-text pairs extracted from Wikipedia that the model can retrieve from to answer questions.\n\n      \nSo in summary, the memory is the large non-parametric external knowledge base encoded in a multimodal space that MuRAG learns to retrieve relevant knowledge from given a question, in order to augment its language generation capabilities. The memory provides the world knowledge to complement what is stored implicitly in the model’s parameters.\n\n      \nFor reading, the retrieved multimodal context is combined with the question embedding and fed into the decoder to generate an answer.\n\n    \n\n  \n\n  \nMuRAG achieves state-of-the-art results on two multimodal QA datasets - WebQA and MultimodalQA, outperforming text-only methods by 10-20% accuracy. It demonstrates the value of incorporating both visual and textual knowledge.\n\n  \nKey limitations are the reliance on large-scale pre-training data, computational costs, and issues in visual reasoning like counting objects. But overall, MuRAG represents an important advance in building visually-grounded language models.\n\n\n\n\n\nHypothetical Document Embeddings (HyDE)\n\n\n\n\n  \nPublished in \nPrecise Zero-Shot Dense Retrieval without Relevance Labels\n by Gao et al. from CMU and University of Waterloo, proposes an innovative approach called Hypothetical Document Embeddings (HyDE) for effective zero-shot dense retrieval in the absence of relevance labels. HyDE leverages an instruction-following language model, such as InstructGPT, to generate a hypothetical document that captures relevance patterns, although it may contain factual inaccuracies. An unsupervised contrastive encoder, like Contriever, then encodes this document into an embedding vector to identify similar real documents in the corpus embedding space, effectively filtering out incorrect details.\n\n  \nThe implementation of HyDE combines InstructGPT (a GPT-3 model) and Contriever models, utilizing OpenAI playground’s default temperature setting for generation. For English retrieval tasks, the English-only Contriever model was used, while for non-English tasks, the multilingual mContriever was employed.\n\n  \nThe following image from the paper illustrates the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever/mContriever models.\n\n\n\n\n\n\n\n\n\n  \nExperiments were conducted using the Pyserini toolkit. The results demonstrate HyDE’s significant improvement over the state-of-the-art unsupervised dense retriever Contriever, with strong performance comparable to fine-tuned retrievers across various tasks and languages. Specifically, in web search and low-resource tasks, HyDE showed sizable improvements in precision and recall-oriented metrics. It remained competitive even compared to fine-tuned models, particularly in terms of recall. In multilingual retrieval, HyDE improved the mContriever model and outperformed non-Contriever models fine-tuned on MS-MARCO. However, there were some performance gaps with fine-tuned mContrieverFT, likely due to under-training in non-English languages.\n\n  \nFurther analysis explored the effects of using different generative models and fine-tuned encoders with HyDE. Larger language models brought greater improvements, and the use of fine-tuned encoders with HyDE showed that less powerful instruction language models could impact the performance of the fine-tuned retriever.\n\n  \nOne possible pitfall of HyDE is that it can potentially “hallucinate” in the sense that it generates hypothetical documents that may contain invented or inaccurate details. This phenomenon occurs because HyDE uses an instruction-following language model, like InstructGPT, to generate a document based on a query. The generated document is intended to capture the relevance patterns of the query, but since it’s created without direct reference to real-world data, it can include false or fictional information. This aspect of HyDE is a trade-off for its ability to operate in zero-shot retrieval scenarios, where it creates a contextually relevant but not necessarily factually accurate document to guide the retrieval process.\n\n  \nIn conclusion, the paper introduces a new paradigm of interaction between language models and dense encoders/retrievers, showing that relevance modeling and instruction understanding can be effectively handled by a powerful and flexible language model. This approach eliminates the need for relevance labels, offering practical utility in the initial stages of a search system’s life, and paving the way for further advancements in tasks like multi-hop retrieval/QA and conversational search.\n\n\n\n\n\nRAGAS: Automated Evaluation of Retrieval Augmented Generation\n\n\n\n\n  \nThis paper by Es et al. from Exploding Gradients, Cardiff University, and AMPLYFI introduces RAGAS, a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) systems.\n\n  \nRAGAS focuses on evaluating the performance of RAG systems in dimensions such as the effectiveness of the retrieval system in providing relevant context, the LLM’s ability to utilize this context, and the overall quality of generation.\n\n  \nThe framework proposes a suite of metrics to evaluate these dimensions without relying on ground truth human annotations.\n\n  \nRAGAS focuses on three quality aspects: Faithfulness, Answer Relevance, and Context Relevance.\n    \n\n      \nFaithfulness\n: Defined as the extent to which the generated answer is grounded in the provided context. It’s measured using the formula:\n\\(F = \\frac{|V|}{|S|}\\)\nwhere, \\(|V|\\) is the number of statements supported by the context and \\(|S|\\) is the total number of statements extracted from the answer.\n\n      \nAnswer Relevance\n: This metric assesses how well the answer addresses the given question. It’s calculated by generating potential questions from the answer and measuring their similarity to the original question using the formula:\n\\(AR = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sim}(q, q_i)\\)\nwhere \\(q\\) is the original question, \\(q_i\\) are the generated questions, and sim denotes the cosine similarity between their embeddings.\n\n      \nContext Relevance\n: Measures the extent to which the retrieved context contains only the information necessary to answer the question. It is quantified using the proportion of extracted relevant sentences to the total sentences in the context:\n\\(CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of sentences in } c(q)}\\)\n\n    \n\n  \n\n  \nThe paper validates RAGAS using the WikiEval dataset, demonstrating its alignment with human judgments in evaluating these aspects.\n\n  \nThe authors argue that RAGAS contributes to faster and more efficient evaluation cycles for RAG systems, which is vital due to the rapid adoption of LLMs.\n\n  \nRAGAS is validated using the WikiEval dataset, which includes question-context-answer triples annotated with human judgments for faithfulness, answer relevance, and context relevance.\n\n  \nThe evaluation shows that RAGAS aligns closely with human judgments, particularly in assessing faithfulness and answer relevance.\n\n  \nCode\n.\n\n\n\n\n\nFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\n\n\n\n\n  \nThis paper by Ovadia et al. from Microsoft presents an insightful comparison of knowledge injection methods in large language models (LLMs). The core question addressed is whether unsupervised fine-tuning (USFT) is more effective than retrieval-augmented generation (RAG) for improving LLM performance on knowledge-intensive tasks.\n\n  \nThe researchers focus on LLMs’ ability to memorize, understand, and retrieve factual data, using a knowledge base scraped from Wikipedia and a dataset of current events questions created with GPT-4. The study employs models like Llama2-7B, Mistral-7B, and Orca2-7B, evaluating them on tasks from the Massively Multitask Language Understanding Evaluation (MMLU) benchmark and a current events dataset.\n\n  \nTwo methods of knowledge injection are explored: fine-tuning, which continues the model’s pre-training process using task-specific data, and retrieval-augmented generation (RAG), which uses external knowledge sources to enhance LLMs’ responses. The paper also delves into supervised, unsupervised, and reinforcement learning-based fine-tuning methods.\n\n  \nThe key finding is that RAG outperforms unsupervised fine-tuning in knowledge injection. RAG, which uses external knowledge sources, is notably more effective in terms of knowledge injection than USFT alone and even more so than a combination of RAG and fine-tuning, particularly in scenarios where questions directly corresponded to the auxiliary dataset. This suggests that USFT may not be as efficient in embedding new knowledge into the model’s parameters.\n\n  \nThe figure below from the paper shows a visualization of the knowledge injection framework.\n\n\n\n\n\n\n\n\n\n  \nNote that USFT in this context is a direct continuation of pre-training (hence also called continued pre-training in literature), predicting the next token on the dataset. Interestingly, fine-tuning with multiple paraphrases of the same fact significantly improves the baseline performance, indicating the importance of repetition and varied presentation of information for effective knowledge assimilation.\n\n  \nThe authors created a knowledge base by scraping Wikipedia articles relevant to various topics, which was used for both fine-tuning and RAG. Additionally, a dataset of multiple-choice questions about current events was generated using GPT-4, with paraphrases created to augment this dataset.\n\n  \nLimitations of the study include the exclusive focus on unsupervised fine-tuning, without exploring supervised fine-tuning or reinforcement learning from human feedback (RLHF). The study also notes a high variance in accuracy performance across experiments, making it challenging to ascertain the statistical significance of the results.\n\n  \nThe paper also questions why baseline models don’t achieve a 25% accuracy rate for multiple-choice questions with four options, suggesting that the tasks may not represent truly “unseen” knowledge. Moreover, the research primarily assesses straightforward knowledge or fact tasks, without delving into reasoning capabilities.\n\n  \nIn summary, while fine-tuning can be beneficial, RAG is identified as a superior method for knowledge injection in LLMs, especially for tasks involving new information. The results highlight the potential of using diverse fine-tuning techniques and auxiliary knowledge bases for further research in this domain.\n\n\n\n\n\nDense X Retrieval: What Retrieval Granularity Should We Use?\n\n\n\n\n  \nOne crucial choice in RAG pipeline design is chunking: should it be sentence level, passage level, or chapter level? This choice significantly impacts your retrieval and response generation performance.\n\n  \nThis paper by Chen et al. from the University of Washington, Tencent AI Lab, University of Pennsylvania, Carnegie Mellon University introduces a novel approach to dense retrieval in open-domain NLP tasks by using “propositions” as retrieval units, instead of the traditional document passages or sentences. A proposition is defined as an atomic expression within text, encapsulating a distinct factoid in a concise, self-contained natural language format. This change in retrieval granularity has a significant impact on both retrieval and downstream task performances.\n\n  \nPropositions follow three key principles:\n    \n\n      \nEach proposition encapsulates a distinct meaning, collectively representing the semantics of the entire text.\n\n      \nThey are minimal and indivisible, ensuring precision and clarity.\n\n      \nEach proposition is contextualized and self-contained, including all necessary text context (like coreferences) for full understanding.\n\n    \n\n  \n\n  \nThe authors developed a text generation model, named “Propositionizer,” to segment Wikipedia pages into propositions. This model was fine-tuned in two steps, starting with prompting GPT-4 for paragraph-to-propositions pairs generation, followed by fine-tuning a Flan-T5-large model.\n\n  \nThe effectiveness of propositions as retrieval units was evaluated using the FACTOIDWIKI dataset, a processed English Wikipedia dump segmented into passages, sentences, and propositions. Experiments were conducted on five open-domain QA datasets: Natural Questions (NQ), TriviaQA (TQA), Web Questions (WebQ), SQuAD, and Entity Questions (EQ). Six different dense retriever models were compared: SimCSE, Contriever, DPR, ANCE, TAS-B, and GTR.\n\n  \nThe figure below from the paper illustrates the fact that that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet effective strategy to increase dense retrievers’ generalization performance at inference time \\((A, B)\\). We empirically compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with Wikipedia indexed at the level of 100-word passage, sentence or proposition \\((C, D)\\).\n\n\n\n\n\n\n\n\n\n  \nResults:\n    \n\n      \nPassage Retrieval Performance:\n Proposition-based retrieval consistently outperformed sentence and passage-level retrieval across all datasets and models. This was particularly evident with unsupervised retrievers like SimCSE and Contriever, which showed an average Recall@5 improvement of 12.0% and 9.3%, respectively.\n\n      \nCross-Task Generalization:\n The advantage of proposition retrieval was most pronounced in cross-task generalization settings, especially for queries about less common entities. It showed significant improvement over other granularities in datasets not seen during the training of the retriever models.\n\n      \nDownstream QA Performance:\n In the retrieve-then-read setting, proposition-based retrieval led to stronger downstream QA performance. This was true for both unsupervised and supervised retrievers, with notable improvements in exact match (EM) scores.\n\n      \nDensity of Question-Related Information:\n Propositions proved to offer a higher density of relevant information, resulting in the correct answers appearing more frequently within the top-l retrieved words. This was a significant advantage over sentence and passage retrieval, particularly in the range of 100-200 words.\n\n      \nError Analysis:\n The study also highlighted the types of errors typical to each retrieval granularity. For example, passage-level retrieval often struggled with entity ambiguity, while proposition retrieval faced challenges in multi-hop reasoning tasks.\n\n    \n\n  \n\n  \nThe figure plot from the paper shows that retrieving by propositions yields the best retrieval performance in both passage retrieval task and downstream open-domain QA task, e.g. with Contriever or GTR as the backbone retriever.\n\n\n\n\n\n\n\n\n\n  \nThe research demonstrates that using propositions as retrieval units significantly improves dense retrieval performance and downstream QA task accuracy, outperforming traditional passage and sentence-based methods. The introduction of FACTOIDWIKI, with its 250 million propositions, is expected to facilitate future research in information retrieval.\n\n\n\n\n\nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n\n\n\n\n  \nThis paper by Saad-Falcon et al. from Stanford University and UC Berkeley, the paper introduces ARES (Automated RAG Evaluation System) for evaluating Retrieval-Augmented Generation (RAG) systems in terms of context relevance, answer faithfulness, and answer relevance.\n\n  \nARES generates synthetic training data using a language model and fine-tunes lightweight LM judges to assess individual RAG components. It utilizes a small set of human-annotated data points for prediction-powered inference (PPI), enabling statistical guarantees for its predictions.\n\n  \nThe framework has three stages:\n    \n\n      \nLLM Generation of Synthetic Dataset\n: ARES uses generative LLMs (like FLAN-T5 XXL) to create synthetic datasets of question-answer pairs derived from target corpus passages. This stage includes both positive and negative examples for training.\n\n      \nPreparing LLM Judges\n: Separate lightweight LM models are fine-tuned for three classification tasks - context relevance, answer faithfulness, and answer relevance - using the synthetic dataset. These models are tuned using a contrastive learning objective.\n\n      \nRanking RAG Systems with Confidence Intervals\n:\n        \n\n          \nAfter preparing the LLM judges, the next step involves using them to score and rank various RAG systems. This process begins with ARES sampling in-domain query-document-answer triples from each RAG approach. The judges then label each triple, assessing context relevance, answer faithfulness, and answer relevance. These labels are averaged for each in-domain triple to evaluate the performance of the RAG systems across the three metrics.\n\n          \nWhile average scores could be reported as quality metrics for each RAG system, these scores are based on unlabeled data and predictions from synthetically-trained LLM judges, which may introduce noise. An alternative is to rely solely on a small human preference validation set for evaluation, examining the extent to which each RAG system aligns with human annotations. However, this method requires labeling outputs from each RAG system separately, which can be time-consuming and expensive.\n\n          \nTo enhance the precision of the evaluation, ARES employs prediction-powered inference (PPI). PPI is a statistical method that narrows the confidence interval of predictions on a small annotated dataset by utilizing predictions on a larger, non-annotated dataset. It combines labeled datapoints and ARES judge predictions on non-annotated datapoints to construct tighter confidence intervals for RAG system performance.\n\n          \nPPI involves using LLM judges on the human preference validation set to learn a rectifier function. This function constructs a confidence set of the ML model’s performance, taking into account each ML prediction in the larger non-annotated dataset. The confidence set helps create a more precise confidence interval for the average performance of the ML model (e.g., its context relevance, answer faithfulness, or answer relevance accuracy). By integrating the human preference validation set with a larger set of datapoints with ML predictions, PPI develops reliable confidence intervals for ML model performance, outperforming traditional inference methods.\n\n          \nThe PPI rectifier function addresses errors made by the LLM judge and generates confidence bounds for the success and failure rates of the RAG system. It estimates performances in context relevance, answer faithfulness, and answer relevance. PPI also allows for estimating confidence intervals with a specified probability level; in these experiments, a standard 95% alpha is used.\n\n          \nFinally, the accuracy confidence interval for each component of the RAG is determined, and the midpoints of these intervals are used to rank the RAG systems. This ranking enables a comparison of different RAG systems and configurations within the same system, aiding in identifying the optimal approach for a specific domain.\n            \n\n              \nIn summary, ARES employs PPI to score and rank RAG systems, using human preference validation sets to calculate confidence intervals. PPI operates by first generating predictions for a large sample of data points, followed by human annotation of a small subset. These annotations are used to calculate confidence intervals for the entire dataset, ensuring accuracy in the system’s evaluation capabilities.\n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \nTo implement ARES for scoring a RAG system and comparing to other RAG configurations, three components are needed:\n    \n\n      \nA human preference validation set of annotated query, document, and answer triples for the evaluation criteria (e.g. context relevance, answer faithfulness, and/or answer relevance). There should be at least 50 examples but several hundred examples is ideal.\n\n      \nA set of few-shot examples for scoring context relevance, answer faithfulness, and/or answer relevance in your system.\n\n      \nA much larger set of unlabeled query-document-answer triples outputted by your RAG system for scoring.\n\n    \n\n  \n\n  \nThe figure below from the paper shows an overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference validation set of 150 annotated datapoints or more, and five few-shot examples of in-domain queries and answers, which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation, we first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a constrastive learning framework, we fine-tune an LLM to classify query–passage–answer triples across three criteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judge to evaluate RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n\n\n\n\n\n\n\n\n\n  \nExperiments conducted on datasets from KILT and SuperGLUE demonstrate ARES’s accuracy in evaluating RAG systems, outperforming existing automated evaluation approaches like RAGAS. ARES is effective across various domains, maintaining accuracy even with domain shifts in queries and documents.\n\n  \nThe paper highlights the strengths of ARES in cross-domain applications and its limitations, such as its inability to generalize across drastic domain shifts (e.g., language changes, text-to-code). It also explores the potential of using GPT-4 for generating labels as a replacement for human annotations in the PPI process.\n\n  \nARES code and datasets are available for replication and deployment at \nGitHub\n.\n\n  \nCode\n\n\n\n\n\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n\n\n\n  \nThis technical report by Barnett et al. from the Applied Artificial Intelligence Institute, Deakin University, Australia, explores failure points in the implementation of Retrieval Augmented Generation (RAG) systems. based on three case studies in diverse domains: research, education, and biomedical.\n\n  \nRAG systems, which integrate retrieval mechanisms with Large Language Models (LLMs) to generate contextually relevant responses, are scrutinized for their operational challenges. The paper identifies seven key failure points in RAG systems:\n    \n\n      \nFP1 Missing Relevant Content:\n The first failure case is when asking a question that cannot be answered from the available documents. In the happy case the RAG system will respond with something like “Sorry, I don’t know”. However, for questions that are related to the content but don’t have answers the system could be fooled into giving a response.\n\n      \nFP2 Missed the Top Ranked Documents:\n The answer to the question is in the document but did not rank highly enough to be returned to the user. In theory, all documents are ranked and used in the next steps. However, in practice only the top \\(K\\) documents are returned where \\(K\\) is a value selected based on performance.\n\n      \nFP3 Not in Context - Consolidation Strategy Limitations:\n Documents with the answer were retrieved from the database but did not make it into the context for generating an answer. This occurs when many documents are returned from the database and a consolidation process takes place to retrieve the answer.\n\n      \nFP4 Not Extracted Here:\n the answer is present in the context, but the large language model failed to extract out the correct answer. Typically, this occurs when there is too much noise or contradicting information in the context.\n\n      \nFP5 Wrong Format:\n The question involved extracting information in a certain format such as a table or list and the large language model ignored the instruction.\n\n      \nFP6 Incorrect Specificity:\n The answer is returned in the response but is not specific enough or is too specific to address the user’s need. This occurs when the RAG system designers have a desired outcome for a given question such as teachers for students. In this case, specific educational content should be provided with answers not just the answer. Incorrect specificity also occurs when users are not sure how to ask a question and are too general.\n\n      \nFP7 Incomplete Responses:\n Incomplete answers are not incorrect but miss some of the information even though that information was in the context and available for extraction. An example question such as “What are the key points covered in documents A, B and C?” A better approach is to ask these questions separately.\n\n    \n\n  \n\n  \nThe study also emphasizes the importance of real-time validation and the evolving robustness of RAG systems. It concludes with suggestions for future research directions, highlighting the significance of chunking, embeddings, and the trade-offs between RAG systems and fine-tuning LLMs.\n\n  \nThe following image from the paper shows the Indexing and Query processes required for creating a Retrieval Augmented Generation (RAG) system. The indexing\nprocess is typically done at development time and queries at runtime. Failure points identified in this study are shown in red boxes. All required stages are underlined.\n\n\n\n\n\n\n\n\n\n  \nMoreover, the paper provides insights into the challenges faced in implementing RAG systems, such as handling diverse document types, query preprocessing, and the need for continuous calibration and monitoring of these systems. These findings are derived from practical experiences and offer valuable guidance for practitioners in the field.\n\n\n\n\n\nRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n\n\n\n\n  \nThis paper by Sarthi et al. from Manning’s Lab at Stanford, published in ICLR 2024, introduces RAPTOR, a novel approach for retrieval-augmented language models. RAPTOR addresses the limitation of existing retrieval methods that primarily fetch short text chunks, hindering comprehensive document understanding. It constructs a tree by recursively embedding, clustering, and summarizing text chunks, offering multi-level summarization and facilitating efficient information retrieval from extensive documents.\n\n  \nAt its core, RAPTOR employs a tree structure starting from leaf nodes (text chunks) and builds up to the root through successive clustering and summarization. This method allows the model to access information at various abstraction levels, significantly enhancing performance on complex, multi-step reasoning tasks. When combined with GPT-4, RAPTOR achieved a 20% absolute accuracy improvement on the QuALITY benchmark over previous state-of-the-art models.\n\n  \nSome key insights into why using a tree-structure lets your RAG pipeline handle more complex questions:\n    \n\n      \nCluster semantically related chunks to dynamically identify distinct topics within your documents.\n\n      \nCreate new chunks by summarizing clusters.\n\n      \nMix high-level and low-level chunks during retrieval, to dynamically surface relevant information depending on the query.\n\n    \n\n  \n\n  \nThe model utilizes SBERT for embedding text chunks and Gaussian Mixture Models (GMMs) for clustering, allowing flexible groupings of related content. Summarization is performed by a language model (GPT-3.5-turbo), producing summaries that guide the construction of higher tree levels. This recursive process creates a scalable and computationally efficient system that linearly scales in both token expenditure and build time, as detailed in the scalability analysis.\n\n  \nQuerying within RAPTOR’s tree employs two strategies: tree traversal and collapsed tree, with the latter showing superior flexibility and effectiveness in preliminary tests on the QASPER dataset. The model’s innovative clustering mechanism, highlighted in an ablation study, proves essential for capturing thematic content and outperforms standard retrieval methods.\n\n  \nThe figure below from the paper shows the tree construction process: RAPTOR recursively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that cluster.\n\n\n\n\n\n\n\n\n\n  \nExperimental results across various datasets (NarrativeQA, QASPER, QuALITY) demonstrate RAPTOR’s effectiveness, setting new benchmarks and outperforming existing retrieval-augmented models. The paper’s qualitative analysis illustrates RAPTOR’s ability to retrieve relevant information for thematic questions, showcasing its superiority over Dense Passage Retrieval (DPR) methods in handling complex queries.\n\n  \nThe paper includes a comprehensive reproducibility statement, detailing the use of publicly available language models and datasets, ensuring that the community can replicate and extend upon RAPTOR’s findings.\n\n\n\n\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n\n\n\n  \nThis paper by Cuconasu et al. from Sapienza University of Rome, Technology Innovation Institute, and University of Pisa introduces a comprehensive study on Retrieval-Augmented Generation (RAG) systems, highlighting the significant influence of Information Retrieval (IR) components on RAG’s performance, beyond the generative abilities of Large Language Models (LLMs).\n\n  \nTheir research investigates the characteristics required in a retriever for optimal RAG prompt formulation, emphasizing the balance between relevant, related, and irrelevant documents.\n\n  \nThe study reveals that including irrelevant documents surprisingly enhances RAG system performance by over 30% in accuracy, challenging the assumption that only relevant and related documents should be retrieved. This finding underscores the potential of integrating seemingly noise-adding strategies to improve RAG system outputs, thereby laying the groundwork for future research in IR and language model integration.\n\n  \nThe experimental methodology employed involves a detailed examination of the Natural Questions dataset, testing various configurations of document relevance and placement within the RAG prompt. This methodological rigor allows the researchers to dissect the impact of document type (gold, relevant, related, irrelevant) and position on the accuracy of RAG system responses, with attention to how these factors influence LLM’s generative performance.\n\n  \nInsights from the experiments led to the formulation of strategies for optimizing RAG systems, proposing a nuanced approach to document retrieval that includes a mix of relevant and intentionally irrelevant documents. This approach aims to maximize system performance within the context size constraints of LLMs, offering a novel perspective on the integration of retrieval processes with generative language models for enhanced factual accuracy and context awareness.\n\n  \nThe study’s findings challenge traditional IR strategies and suggest a paradigm shift towards the inclusion of controlled noise in the retrieval process for language generation tasks. The researchers advocate for further exploration into the mechanisms by which irrelevant documents improve RAG system performance, highlighting the need for new IR techniques tailored to the unique demands of language generation models.\n\n\n\n\n\nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n\n\n\n\n  \nThis paper by Tang et al. from the Hong Kong University of Science and Technology introduces MultiHop-RAG, a novel dataset and benchmark for evaluating Retrieval-Augmented Generation (RAG) systems on multi-hop queries. These queries necessitate retrieving and reasoning over multiple pieces of evidence, a challenge not adequately addressed by existing RAG systems.\n\n  \nMultiHop-RAG consists of a knowledge base derived from English news articles, multi-hop queries, their answers, and the supporting evidence required for those answers. This dataset aims to mimic real-world applications where complex queries involving multiple pieces of information are common.\n\n  \nThe figure below from the paper shows the RAG flow with a multi-hop query.\n\n\n\n\n\n\n\n\n\n  \nThe authors categorize multi-hop queries into four types: Inference, Comparison, Temporal, and Null queries. The first three types — Inference, Comparison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encompassing tasks like inferring relationships, comparing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance. Each type requires a distinct retrieval and reasoning strategy over the evidence, with Null queries designed to test the model’s ability to refrain from generating an answer when the query cannot be resolved with the available knowledge.\n\n  \nThey define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query \\(q\\), the chunks in the retrieval set \\(\\mathcal{R}_q\\) collectively provide an answer to \\(q\\). For example, the query “Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?” requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a singlehop query such as “What is Google’s profit margin in the third-quarter reports for 2023 ,” where the answer can be directly derived from a single piece of evidence.\n\n  \nBased on the queries commonly used in realworld RAG systems, they identify four types of multi-hop queries. For each type, they present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\n    \n\n      \nInference query:\n For such a query \\(q\\), the answer is deduced through reasoning from the retrieval set \\(\\mathcal{R}_q\\). An example of an inference query might be: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\n\n      \nComparison query:\n For such a query \\(q\\), the answer requires a comparison of evidence within the retrieval set \\(\\mathcal{R}_q\\). For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?”\n\n      \nTemporal query:\n For such a query \\(q\\), the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\n\n      \nNull query:\n For such as query \\(q\\), the answer cannot be derived from the retrieved set \\(\\mathcal{R}_q\\). They include the null query to assess the generation quality, especially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assuming ABCS is a non-existent company, a null query might ask: What are the sales of company ABCS as reported in its 2022 and 2023 annual reports?\n\n    \n\n  \n\n  \nThe dataset was created using GPT-4 to generate multi-hop queries from a pool of factual sentences extracted from news articles. The queries were then validated for quality and relevance. This process ensures the dataset’s utility in benchmarking the capability of RAG systems to handle complex queries beyond the capacity of current systems.\n\n  \nExperimental results demonstrate that existing RAG methods struggle with multi-hop query retrieval and answering, underscoring the necessity for advancements in this area. The benchmarking also explores the effectiveness of different embedding models for evidence retrieval and the reasoning capabilities of various state-of-the-art Large Language Models (LLMs) including GPT-4, PaLM, and Llama2-70B, revealing significant room for improvement.\n\n  \nThe authors hope that MultiHop-RAG will encourage further research and development in RAG systems, particularly those capable of sophisticated multi-hop reasoning, thereby enhancing the practical utility of LLMs in complex information-seeking tasks.\n\n  \nCode\n\n\n\n\n\nRAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture\n\n\n\n\n  \nThis paper by Balaguer et al. from Microsoft, delves into two prevalent approaches for incorporating proprietary and domain-specific data into Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments prompts with external data, whereas Fine-Tuning embeds additional knowledge directly into the model. The paper outlines a comprehensive pipeline for both approaches, evaluating their effectiveness on multiple popular LLMs including Llama2-13B, GPT-3.5, and GPT-4.\n\n  \nThe research particularly focuses on agriculture, an industry with relatively limited AI penetration, proposing a disruptive application: providing location-specific insights to farmers. The pipeline stages include data acquisition, PDF information extraction, question and answer generation using this data, and leveraging GPT-4 for result evaluation. Metrics are introduced to assess the performance of the RAG and Fine-Tuning pipeline stages.\n\n  \nThe figure below from the paper shows the methodology pipeline. Domain-specific datasets are collected, and the content and structure of the documents are extracted. This information is then fed to the Q&A generation step. Synthesized question-answer pairs are used to fine-tune the LLMs. Models are evaluated with and without RAG under different GPT-4-based metrics.\n\n\n\n\n\n\n\n\n\n  \nExperimental results from an agricultural dataset highlight the pipeline’s capability in capturing geography-specific knowledge. Fine-Tuning demonstrated a significant accuracy increase of over 6 percentage points, a benefit that accumulates with RAG, further enhancing accuracy by 5 percentage points. One experiment showcased the fine-tuned model’s ability to leverage information across geographies to answer specific questions, boosting answer similarity from 47% to 72%.\n\n  \nThe paper presents an in-depth comparison of answers from GPT-4, Bing Chat, and agronomist experts to the same query across different U.S. states, revealing the models’ generic responses versus the experts’ nuanced, location-specific answers. This comparative analysis underscores the potential of fine-tuning and RAG in producing more contextually appropriate responses for industry-specific applications.\n\n  \nThe proposed methodology aims at generating domain-specific questions and answers to create a valuable knowledge resource for industries requiring specific contextual and adaptive responses. Through an extensive evaluation involving benchmarks from major agriculture-producing countries, the study establishes a baseline understanding of model performance in the agricultural context and explores the impact of spatial shift on knowledge encoding and the benefits of spatially-scoped fine-tuning.\n\n  \nAdditionally, the research investigates the implications of retrieval techniques and fine-tuning on LLM performance. It identifies RAG as particularly effective in contexts requiring domain-specific knowledge and fine-tuning as beneficial for imparting new skills to models, albeit at a higher initial cost. This work serves as a foundation for applying RAG and fine-tuning techniques across industries, demonstrating their utility in enhancing model efficiency from the Q&A generation process onwards.\n\n\n\n\n\nRAFT: Adapting Language Model to Domain Specific RAG\n\n\n\n\n  \nThis paper by Zhang et al. from UC Berkeley introduces Retrieval Augmented Fine Tuning (RAFT) as a method to adapt pre-trained Large Language Models (LLMs) for domain-specific Retrieval Augmented Generation (RAG), focusing on “open-book” in-domain settings. By training the model to identify and ignore distractor documents while citing relevant information from pertinent documents, RAFT enhances the model’s reasoning capability and its ability to answer questions based on a specific set of documents.\n\n  \nThe concept draws an analogy to preparing for an open-book exam, where RAFT simulates the conditions of such an exam by incorporating both relevant and irrelevant (distractor) documents during training. This contrasts with existing methods that either do not leverage the opportunity to learn from domain-specific documents or fail to prepare the model for the dynamics of RAG in an open-book test setting.\n\n  \nThe figure below from the paper draws an analogy to how best to prepare for an exam? (a) Fine-tuning based approaches implement “studying” by either directly “memorizing” the input documents or answering practice QA without referencing the documents. (b) Alternatively, incontext retrieval methods fail to leverage the learning opportunity afforded by the fixed domain and are equivalent to taking an open-book exam without studying. While these approaches leverage in-domain learning, they fail to prepare for open-book tests. In contrast, (c) RAFT leverages fine-tuning with question-answer pairs while referencing the documents in a simulated imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.\n\n\n\n\n\n\n\n\n\n  \nThe methodology involves creating training data that includes a question, a set of documents (with one or more being relevant to the question), and a CoT-style answer derived from the relevant document(s). The paper explores the impact of including distractor documents in the training set and the proportion of training data that should contain the oracle document.\n\n  \nThe figure below from the paper shows an overview of RAFT. The top-left figure depicts our approach of adapting LLMs to reading solution from a set of positive and negative documents in contrast to standard RAG setup where models are trained based on the retriever outputs, which is a mixture of both memorization and reading. At test time, all methods follow the standard RAG setting, provided with a top-k retrieved documents in the context.\n\n\n\n\n\n\n\n\n\n  \nExperiments conducted across PubMed, HotpotQA, and Gorilla datasets demonstrate RAFT’s effectiveness. It consistently outperforms both supervised fine-tuning and RAG across these datasets, particularly highlighting the importance of the chain-of-thought (CoT) style responses in improving model performance.\n\n  \nResults from various experiments indicate that mixing a fraction of the training data without the oracle document in its context is beneficial for in-domain RAG tasks. Moreover, training with a balance of relevant and irrelevant documents at test time shows that RAFT can generalize well to different numbers of retrieved documents, enhancing robustness against inaccuracies in retrieval.\n\n  \nRAFT’s approach is compared against several baselines, including LLaMA-7B with and without RAG, domain-specific fine-tuning with 0-shot prompting (DSF), and DSF with RAG. Across different datasets, RAFT demonstrates significant improvements, underscoring its potential in domain-specific applications.\n\n  \nThe paper also discusses related works, highlighting advancements in retrieval-augmented language models, memorization versus generalization in LLMs, and fine-tuning strategies for adapting LLMs to specific tasks. RAFT’s contribution lies in its focus on preparing LLMs for domain-specific RAG by effectively leveraging both relevant and distractor documents during training.\n\n  \nThe study posits RAFT as a valuable strategy for adapting pre-trained LLMs to domain-specific tasks, especially where leveraging external documents is crucial. By training models to discern relevant information from distractors and generating CoT-style answers, RAFT significantly enhances the model’s ability to perform in open-book exam settings, paving the way for more nuanced and effective domain-specific applications of LLMs.\n\n  \nProject page\n; \nCode\n\n\n\n\n\nCorrective Retrieval Augmented Generation\n\n\n\n\n  \nThe paper by Yan et al. from the University of Science and Technology of China, UCLA, and Google Research, proposed Corrective Retrieval Augmented Generation (CRAG) which addresses the challenge of hallucinations and inaccuracies in large language models (LLMs) by proposing a novel framework that enhances the robustness of retrieval-augmented generation (RAG) methods.\n\n  \nCRAG introduces a lightweight retrieval evaluator that assesses the quality of documents retrieved for a query and triggers actions based on a confidence degree, aiming to correct or enhance the retrieval process. The framework also incorporates large-scale web searches to augment the pool of retrieved documents, ensuring a broader spectrum of relevant and accurate information.\n\n  \nA key feature of CRAG is its decompose-then-recompose algorithm, which processes the retrieved documents to highlight crucial information while discarding irrelevant content. This method significantly improves the model’s ability to utilize the retrieved documents effectively, enhancing the quality and accuracy of the generated text.\n\n  \nThe figure below from the paper shows an overview of CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the\nretrieved documents to the input, and estimate a confidence degree based on which different knowledge retrieval actions of \n{Correct, Incorrect, Ambiguous}\n can be triggered.\n\n\n\n\n\n\n\n\n\n  \nCRAG is designed to be plug-and-play, allowing seamless integration with various RAG-based approaches. Extensive experiments across four datasets demonstrate CRAG’s ability to significantly enhance the performance of RAG-based methods in both short- and long-form generation tasks, showcasing its adaptability and generalizability.\n\n  \nThe study identifies scenarios where conventional RAG approaches may falter due to inaccurate retrievals. CRAG addresses this by enabling self-correction and efficient utilization of retrieved documents, marking a significant step towards improving the reliability and effectiveness of RAG methods.\n\n  \nLimitations acknowledged include the ongoing challenge of accurately detecting and correcting erroneous knowledge. The necessity of fine-tuning a retrieval evaluator and the potential biases introduced by web searches are highlighted as areas for future improvement.\n\n  \nCode\n\n\n\n\n\nFine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\n\n\n\n\n  \nThis paper by Soudani et al. from Radboud University and the University of Amsterdam investigates the efficacy of Retrieval Augmented Generation (RAG) and fine-tuning (FT) on enhancing the performance of large language models (LLMs) for question answering (QA) tasks involving low-frequency factual knowledge. The authors conducted a comprehensive comparison to determine which approach is more beneficial for customizing LLMs to handle less popular entities, using a dataset characterized by a wide range of entity popularity levels. They found that fine-tuning significantly improves performance across entities of varying popularity, with notable gains in the most and least popular groups. Conversely, RAG was observed to surpass other methods, particularly when combined with FT in smaller models, although its advantage diminishes in base models and is non-existent in larger models.\n\n  \nThe evaluation setup included a diverse range of factors such as model size, retrieval models, quality of synthetic data generation, and fine-tuning method (PEFT vs. full fine-tuning). The findings underscored the importance of advancements in retrieval and data augmentation techniques for the success of both RAG and FT strategies. For FT, two data augmentation methods were used to generate synthetic training data: an End-to-End approach utilizing a model trained for paragraph-level QA generation and a Prompt method using LLMs for QA generation.\n\n  \nFor RAG, various retrieval models were employed to enhance the LLM’s response generation by providing additional context from a document corpus. The performance of the retrieval models played a significant role in the effectiveness of the RAG approach. The study also highlighted the role of synthetic data quality over quantity, with models trained on prompt-generated data outperforming those trained on E2E-generated data.\n\n  \nThe figure below from the paper shows a correlation between subject entity popularity in a question and the effects of RAG and FT on FlanT5-\nsmall performance in open-domain question answering. FT markedly improves accuracy in the initial and final buckets relative to others (indicated by the pink line).\n\n\n\n\n\n\n\n\nHGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation\n\n\n\n\n  \nThis paper by Fang et al. from Queen’s University introduce a novel structured, multi-layered graph approach named Hierarchical Graph of Thoughts (HGOT). This framework aims to mitigate hallucinations in large language models (LLMs) by enhancing the retrieval of relevant information for in-context learning. HGOT uses emergent planning capabilities of LLMs to decompose complex queries into manageable sub-queries. The divide-and-conquer strategy simplifies problem-solving and improves the relevance and accuracy of retrieved information.\n\n  \nHGOT incorporates a unique self-consistency majority voting mechanism for answer selection. This mechanism uses citation recall and precision metrics to evaluate the quality of thoughts, thus directly linking the credibility of an answer to the thought’s quality. The approach employs a scoring mechanism for evaluating retrieved passages, considering citation frequency and quality, self-consistency confidence, and the retrieval module’s ranking.\n\n  \nThe figure below from the paper shows an illustrative example of HGOT in answering a factual question. (The abbreviations employed are as\nfollows: Instr.: Instructions, Q: Question, Ctx.: Context or References, Resp.: ChatGPT’s Response, PL: Plan, D: Dependencies, CI: Confidence, Ans.: Answer, Thot.: Thought)\n\n\n\n\n\n\n\n\n\n  \nThe effectiveness of HGOT is validated against several other retrieval-augmented methods like Demonstrate-Search-Predict (DSP) and ReAct, showing an improvement of up to 7% on datasets such as FEVER, Open-SQuAD, and HotPotQA. This demonstrates HGOT’s enhanced capability for factuality in LLM responses.\n\n  \nIn terms of implementation, HGOT utilizes emergent planning abilities of LLMs to create hierarchical graphs, which organizes the thought process more efficiently and reduces the likelihood of error propagation across multiple reasoning layers. The framework adjusts majority voting by weighting responses based on the quality of their associated citations, and employs a scoring system that factors in multiple qualities of retrieved passages to ensure high-quality, relevant informational support for LLM responses.\n\n\n\n\n\nHow faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior\n\n\n\n\n  \nThis paper by Wu et al. from from Stanford investigates the effectiveness of Retrieval Augmented Generation (RAG) frameworks in moderating the behavior of Large Language Models (LLMs) when confronted with conflicting information. It centers on the dynamic between an LLM’s pre-existing knowledge and the information retrieved via RAG, particularly when discrepancies arise.\n\n  \nThe authors conducted a systematic study using models like GPT-4 and GPT-3.5, simulating scenarios where the models were provided with both accurate and deliberately perturbed information across six distinct datasets. The paper confirms that while correct information typically corrects LLM outputs (with a 94% accuracy rate), incorrect data leads to errors if the model’s internal prior is weak.\n\n  \nThe study introduces a novel experimental setup where documents are systematically modified to test LLM reliance on prior knowledge versus retrieved content. Changes ranged from numerical modifications (e.g., altering drug dosages or dates by specific multipliers or intervals) to categorical shifts in names and locations, assessing model response variations.\n\n  \nThe figure below from the paper shows a schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. They then observe whether the LLM prefers the modified information or its own prior answer.\n\n\n\n\n\n\n\n\n\n  \nKey findings include an inverse correlation between the likelihood of an LLM adhering to retrieved information and its internal confidence, quantified through token probabilities. Models with stronger priors demonstrated greater resistance to misleading RAG content, reverting to their initial responses.\n\n  \nAdditionally, the paper discusses the influence of different prompting strategies on RAG adherence. The ‘strict’ prompting led to higher reliance on retrieved content, whereas ‘loose’ prompting allowed more independent reasoning from the models, highlighting the importance of prompt design in RAG systems.\n\n  \nResults across the datasets illustrated varying degrees of RAG effectiveness, influenced by the model’s confidence level. This nuanced exploration of RAG dynamics provides insights into improving the reliability of LLMs in practical applications, emphasizing the delicate balance needed in integrating RAG to mitigate errors and hallucinations in model outputs.\n\n\n\n\n\nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\n\n\n\n\n  \nThis paper by Jeong et al. from KAIST presents a novel framework named Adaptive-RAG for dynamic adjustment of retrieval strategies in Large Language Models (LLMs) based on the complexity of incoming queries. This allows for efficient and accurate responses across different query complexities.\n\n  \nThe system categorizes queries into simple, moderate, and complex, each requiring different retrieval strategies: non-retrieval, single-step retrieval, and multi-step retrieval, respectively. The determination of query complexity is facilitated by a classifier trained on automatically labeled data.\n\n  \nThe figure below from the paper shows a conceptual comparison of different retrieval-augmented LLM approaches to question answering. (A) In response to a query, this single-step approach retrieves relevant documents and then generates an answer. However, it may not be sufficient for complex queries that require multi-step reasoning. (B) This multi-step approach iteratively retrieves documents and generates intermediate answers, which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both LLMs and retrievers. (C) Their adaptive approach can select the most suitable strategy for retrieval-augmented LLMs, ranging from iterative, to single, to even no retrieval approaches, based on the complexity of given queries determined by our classifier.\n\n\n\n\n\n\n\n\n\n  \nAdaptive-RAG was validated across multiple open-domain QA datasets, showing significant improvements in both efficiency and accuracy over existing models. It employs a blend of iterative and single-step retrieval processes tailored to the specific needs of a query, which optimizes resource use and response time.\n\n  \nThe implementation utilizes a secondary smaller language model as a classifier to predict query complexity. The classifier is trained on datasets synthesized without human labeling, using model predictions and inherent dataset biases to automatically generate training labels.\n\n  \nExperimental results demonstrate that Adaptive-RAG efficiently allocates resources, handling complex queries with detailed retrieval while effectively answering simpler queries directly through the LLM, thus avoiding unnecessary computation.\n\n  \nAdditionally, Adaptive-RAG’s flexibility is highlighted in its ability to interchange between different retrieval strategies without altering the underlying model architecture or parameters, providing a scalable solution adaptable to varied query complexities.\n\n\n\n\n\nRichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation\n\n\n\n\n  \nThis paper by Wang et al. from the Gaoling School of Artificial Intelligence, Renmin University of China, and Baichuan Intelligent Technology, addresses the need for rich and comprehensive responses to broad, open-ended queries in retrieval-augmented generation (RAG).\n\n  \nThe authors propose a novel framework, RichRAG, to handle complex user queries that have multiple sub-intents. RichRAG consists of three main components: a sub-aspect explorer, a multi-faceted retriever, and a generative list-wise ranker.\n\n  \nThe sub-aspect explorer identifies potential sub-aspects of the input queries. This module leverages large language models (LLMs) for their extensive world knowledge and language understanding capabilities. It generates sub-aspects by fine-tuning on training queries using a next token prediction (NTP) loss function.\n\n  \nThe multi-faceted retriever builds a candidate pool of external documents related to the identified sub-aspects. It retrieves top-N documents for each sub-aspect and combines these into a diverse candidate pool, ensuring broad coverage of the query’s various aspects.\n\n  \nThe generative list-wise ranker sorts the top-k most valuable documents from the candidate pool. Built on a seq-to-seq model structure (T5), it models global interactions among candidates and sub-aspects, using a parallel encoding process and a pooling operation to extract relevance representations. The ranker generates a list of document IDs optimized through supervised fine-tuning and reinforcement learning stages.\n\n  \nThe supervised fine-tuning stage uses a greedy algorithm to build silver target ranking lists based on a coverage utility function, ensuring the ranker can generate comprehensive lists.\n\n  \nThe reinforcement learning stage aligns the ranker’s output with LLM preferences by using a reward function based on the quality and coverage of the generated responses. The Direct Preference Optimization (DPO) algorithm is employed, with training pairs created through a unilateral significance sampling strategy (US3) to ensure valuable and reliable training data.\n\n  \nThe figure below from the paper illustrates the overall framework of RichRAG. We describe the training stages of our ranker at the bottom.\n\n\n\n\n\n\n\n\n\n  \nExperimental results on WikiPassageQA and WikiAsp datasets demonstrate RichRAG’s effectiveness in generating comprehensive responses. The framework shows superior performance in terms of Rouge and Com-Rouge scores compared to existing methods.\n\n  \nRichRAG significantly improves the quality of responses to multi-faceted queries by explicitly modeling sub-aspects and aligning ranking lists with LLM preferences. The efficiency and robustness of the ranker are validated through various experiments, confirming its advantage in handling complex search scenarios.\n\n\n\n\n\nHiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA\n\n\n\n\n  \nThis paper by Chen et al. from introduces HiQA, an advanced multi-document question-answering (MDQA) framework to tackle the challenge of retrieving accurate information from extensive, indistinguishable documents. It incorporates cascading metadata and a multi-route retrieval mechanism to enhance the precision and relevance of knowledge retrieval.\n\n  \nThe paper outlines the methodology comprising three main components: Markdown Formatter (MF), Hierarchical Contextual Augmentor (HCA), and Multi-Route Retriever (MRR). MF converts documents into markdown format, enriching them with structured metadata. HCA further augments document segments with hierarchical metadata, and MRR utilizes a combination of vector similarity, Elasticsearch, and keyword matching for improved retrieval accuracy.\n\n  \nThe following figure from the paper illustrates of the proposed contextual text enhancement. The contextual structure can improve text alignment with the query for better matching in multi-documents scenarios.\n\n\n\n\n\n\n\n\n\n  \nA novel dataset, MasQA, is introduced to evaluate the performance of MDQA systems, highlighting the framework’s superiority in handling massive documents through extensive experiments.\n\n  \nAblation studies demonstrate the individual contribution of each component to the system’s overall effectiveness, with a focus on the HCA’s role in improving retrieval precision.\n\n  \nTheoretical exploration into the impact of HCA on the distribution of document segments within the embedding space supports the framework’s approach, indicating enhanced retrieval accuracy and the avoidance of information loss associated with hard partitioning methods.\n\n\n\n\n\nReferences\n\n\n\n\n  \nDocument Similarity Search with [ColPali](https://arxiv.org/abs/2407.01449)\n\n  \nLate Chunking: Balancing Precision and Cost in Long Context Retrieval\n\n  \nLate Chunking in Long-Context Embedding Models\n\n  \nWeaviate Blog: What is Agentic RAG?\n\n  \nAnthropic: Introducing Contextual Retrieval\n\n\n\n\n\nCitation\n\n\n\n@article{Chadha2020DistilledRAG,\n  title   = {Retrieval Augmented Generation},\n  author  = {Chadha, Aman and Jain, Vinija},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/RAG/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nNLP • Retrieval Augmented Generation\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nMotivation\n\n  \nLexical Retrieval\n\n  \nSemantic Retrieval\n\n  \nHybrid Retrieval (Lexical + Semantic)\n\n  \nThe Retrieval Augmented Generation (RAG) Pipeline\n\n  \nBenefits of RAG\n    \n\n      \nRAG vs. Fine-tuning\n\n    \n\n  \n\n  \nEnsemble of RAG\n\n  \nChoosing a Vector DB using a Feature Matrix\n\n  \nBuilding a RAG pipeline\n    \n\n      \nIngestion\n        \n\n          \nChunking\n            \n\n              \nFiguring out the ideal chunk size\n                \n\n                  \nRetriever Ensembling and Reranking\n\n                \n\n              \n\n            \n\n          \n\n          \nEmbeddings\n\n          \nNaive Chunking vs. Late Chunking vs. Late Interaction (ColBERT and ColPali)\n            \n\n              \nOverview\n\n              \nNaive/Vanilla Chunking\n                \n\n                  \nWhat is Naive/Vanilla Chunking?\n\n                  \nExample\n\n                  \nAdvantages and Limitations\n\n                \n\n              \n\n              \nLate Chunking\n                \n\n                  \nWhat is Late Chunking?\n\n                  \nHow Late Chunking Works\n\n                  \nExample\n\n                  \nAdvantages and Trade-offs\n\n                \n\n              \n\n              \nLate Interaction\n                \n\n                  \nWhat is Late Interaction?\n\n                  \nColBERT: Late Interaction in Practice\n\n                  \nMaxSim: A Key Component of ColBERT\n\n                  \nExample\n\n                  \nAdvantages and Trade-offs\n\n                \n\n              \n\n              \nColPali: Expanding to Multimodal Retrieval\n                \n\n                  \nExample\n\n                \n\n              \n\n              \nComparative Analysis\n\n            \n\n          \n\n          \nSentence Embeddings: The What and Why\n            \n\n              \nBackground: Differences compared to Token-Level Models like BERT\n\n              \nRelated: Training Process for Sentence Transformers vs. Token-Level Embedding Models\n\n              \nApplying Sentence Transformers for RAG\n\n            \n\n          \n\n        \n\n      \n\n      \nRetrieval\n        \n\n          \nStandard/Naive approach\n            \n\n              \nAdvantages\n\n              \nDisadvantages\n\n            \n\n          \n\n          \nSentence-Window Retrieval / Small-to-Large Retrieval\n            \n\n              \nAdvantages\n\n              \nDisadvantages\n\n            \n\n          \n\n          \nAuto-merging Retriever / Hierarchical Retriever\n            \n\n              \nAdvantages\n\n              \nDisadvantages\n\n            \n\n          \n\n          \nContextual Retrieval\n\n          \nUsing Approximate Nearest Neighbors (ANN) for Retrieval\n\n        \n\n      \n\n      \nRe-ranking\n        \n\n          \nNeural Re-rankers: Types and Architectures\n            \n\n              \nDomain-Specific Adaptations\n\n            \n\n          \n\n          \nInstruction-Following Re-ranking: Precision and Control in RAG\n\n        \n\n      \n\n      \nResponse Generation / Synthesis\n        \n\n          \nLost in the Middle: How Language Models Use Long Contexts\n\n          \nThe “Needle in a Haystack” Test\n\n        \n\n      \n\n    \n\n  \n\n  \nRAG in Multi-Turn Chatbots: Embedding Queries for Retrieval\n    \n\n      \nEmbedding the Latest User Turn Only\n\n      \nEmbedding Concatenated Recent Turns (Truncated Dialogue History)\n\n      \nEmbedding a Condensed or Summarized History\n\n      \nEmbedding Structured Dialogue State\n\n      \nTask-Optimized Embedding via Query Reformulation\n\n      \nBest Practices and Considerations\n\n    \n\n  \n\n  \nComponent-Wise Evaluation\n    \n\n      \nRetrieval Metrics\n        \n\n          \nContext Precision\n\n          \nContext Recall\n\n          \nContext Relevance\n\n        \n\n      \n\n      \nGeneration Metrics\n        \n\n          \nGroundedness (a.k.a. Faithfulness)\n\n          \nAnswer Relevance\n\n          \nAnswer Semantic Similarity\n\n          \nBLEU Score\n\n          \nROUGE Score\n\n          \nString Presence\n\n          \nExact Match\n\n          \nContext Entities Recall\n\n        \n\n      \n\n    \n\n  \n\n  \nMultimodal Input Handling\n    \n\n      \nFlow of Multimodal Input\n\n      \nBenefits of Multimodal Embeddings in RAG\n\n    \n\n  \n\n  \nMultimodal RAG\n\n  \nAgentic Retrieval-Augmented Generation\n    \n\n      \nHow Agentic RAG Works\n\n      \nAgentic Decision-Making in Retrieval\n\n      \nAgentic RAG Architectures: Single-Agent vs. Multi-Agent Systems\n        \n\n          \nSingle-Agent RAG (Router)\n\n          \nMulti-Agent RAG Systems\n\n        \n\n      \n\n      \nBeyond Retrieval: Expanding Agentic RAG’s Capabilities\n\n      \nAgentic RAG vs. Vanilla RAG: Key Differences\n\n      \nImplementing Agentic RAG: Key Approaches\n        \n\n          \nLanguage Models with Function Calling\n\n          \nAgent Frameworks\n\n        \n\n      \n\n      \nEnterprise-driven Adoption\n\n      \nBenefits\n\n      \nLimitations\n\n      \nCode\n        \n\n          \nImplementing Agentic RAG with Function Calling\n            \n\n              \nDefine the Function for Retrieval\n\n              \nDefine the Tools Schema\n\n              \nSetting Up the Interaction Loop\n\n              \nExecuting the Agentic RAG Query\n\n            \n\n          \n\n          \nImplementing Agentic RAG with Agent Frameworks\n            \n\n              \nStep 1: Define Agents and Tools\n\n              \nStep 2: Configure Agent Routing\n\n              \nStep 3: Chain Agents for Multi-Agent RAG\n\n              \nRunning the Multi-Agent Query\n\n            \n\n          \n\n        \n\n      \n\n      \nDisadvantages of Agentic RAG\n\n      \nSummary\n\n    \n\n  \n\n  \nRAG vs. Long Context Windows\n    \n\n      \nComputational Cost\n\n      \nInference Latency and Throughput\n\n      \nContextual Comprehension and Model Training Limitations\n\n      \nRAG as a Targeted, Cost-Efficient Solution\n\n    \n\n  \n\n  \nImproving RAG Systems\n\n  \nRAG 2.0\n\n  \nSelected Papers\n    \n\n      \nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n      \nActive Retrieval Augmented Generation\n\n      \nMuRAG: Multimodal Retrieval-Augmented Generator\n\n      \nHypothetical Document Embeddings (HyDE)\n\n      \nRAGAS: Automated Evaluation of Retrieval Augmented Generation\n\n      \nFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\n\n      \nDense X Retrieval: What Retrieval Granularity Should We Use?\n\n      \nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n\n      \nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n      \nRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n\n      \nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n      \nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n\n      \nRAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture\n\n      \nRAFT: Adapting Language Model to Domain Specific RAG\n\n      \nCorrective Retrieval Augmented Generation\n\n      \nFine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\n\n      \nHGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation\n\n      \nHow faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior\n\n      \nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\n\n      \nRichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation\n\n      \nHiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA\n\n    \n\n  \n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \nRetrieval-Augmented Generation (RAG) is an advanced technique designed to enhance the output of Language Models (LMs) by incorporating external knowledge sources.\n\n  \nRAG is achieved by retrieving relevant information from a large corpus of documents and utilizing that information to guide and inform the generative process of the model. The subsequent sections provide a detailed examination of this methodology.\n\n\n\n\n\nMotivation\n\n\n\n\n  \nIn many real-world scenarios, organizations maintain extensive collections of proprietary documents, such as technical manuals, from which precise information must be extracted. This challenge is often analogous to locating a needle in a haystack, given the sheer volume and complexity of the content.\n\n  \nWhile recent advancements, such as OpenAI’s introduction of GPT-4 Turbo, offer improved capabilities for processing lengthy documents, they are not without limitations. Notably, these models exhibit a tendency known as the “Lost in the Middle” phenomenon, wherein information positioned near the center of the context window is more likely to be overlooked or forgotten. This issue is akin to reading a comprehensive text such as the Bible, yet struggling to recall specific content from its middle chapters.\n\n  \nTo address this shortcoming, the RAG approach has been introduced. This method involves segmenting documents into discrete units—typically paragraphs—and creating an index for each. Upon receiving a query, the system efficiently identifies and retrieves the most relevant segments, which are then supplied to the language model. By narrowing the input to only the most pertinent information, this strategy mitigates cognitive overload within the model and substantially improves the relevance and accuracy of its responses.\n\n\n\n\n\nLexical Retrieval\n\n\n\n\n  \n\n    \nLexical retrieval is the traditional approach to information retrieval based on exact word matches and term frequency. Two commonly used methods in this category are TF-IDF and BM25.\n\n\n    \n\n      \nTF-IDF (Term Frequency-Inverse Document Frequency):\n\n        \n\n          \nTF-IDF evaluates the importance of a word in a document relative to a corpus. It increases proportionally with the number of times a word appears in the document but is offset by how frequently the word appears across all documents.\n\n          \nWhile TF-IDF is simple and effective for many scenarios, it does not take into account the saturation of term frequency and lacks the ability to differentiate between rare and common words beyond the basic IDF scaling.\n\n        \n\n      \n\n      \nBM25 (Best Matching 25):\n\n        \n\n          \nBM25 is a more refined version of TF-IDF. It introduces term frequency saturation and document length normalization, improving relevance scoring.\n\n          \nOne of the key advantages of BM25 over TF-IDF is its ability to handle multiple occurrences of a term in a more nuanced way. It prevents overly frequent terms from dominating the score, making retrieval results more balanced.\n\n          \nBM25 also scales better with document length, giving fair chances to both short and long documents.\n\n        \n\n      \n\n    \n\n  \n\n  \nAdvantages of Lexical Retrieval:\n\n    \n\n      \nFast and computationally efficient.\n\n      \nEasy to interpret and implement.\n\n      \nWorks well when exact keyword matching is important.\n\n    \n\n  \n\n  \nLimitations:\n\n    \n\n      \nCannot handle synonyms or paraphrased queries effectively.\n\n      \nLimited ability to capture semantic meaning.\n\n    \n\n  \n\n\n\n\n\nSemantic Retrieval\n\n\n\n\n  \n\n    \nSemantic retrieval, previously referred to as neural retrieval, is a more recent approach that relies on machine learning models to understand the meaning behind queries and documents.\n\n  \n\n  \n\n    \nThese systems use neural networks to embed both queries and documents into a shared vector space, where semantic similarity can be calculated using metrics like cosine similarity.\n\n  \n\n  \nHow it works:\n\n    \n\n      \nVector Encoding:\n        \n\n          \nBoth queries and documents are transformed into dense vectors using pre-trained or fine-tuned encoders.\n\n          \nThese encoders are typically trained on large datasets, enabling them to capture semantic nuances beyond surface-level keyword overlap.\n\n        \n\n      \n\n      \nSemantic Matching:\n        \n\n          \nVectors are compared to identify the most semantically relevant documents, even if they don’t share explicit terms with the query.\n\n        \n\n      \n\n    \n\n  \n\n  \nAdvantages of Semantic Retrieval:\n\n    \n\n      \nHandles paraphrasing, synonyms, and conceptual similarity effectively.\n\n      \nSupports more natural and conversational queries.\n\n      \nMultilingual capabilities are often built-in.\n\n    \n\n  \n\n  \nChallenges and Considerations:\n\n    \n\n      \nRequires significant computational resources.\n\n      \nRetrieval quality is sensitive to training data and may reflect biases.\n\n      \nUpdating document embeddings for dynamic content can be complex.\n\n    \n\n  \n\n\n\n\n\nHybrid Retrieval (Lexical + Semantic)\n\n\n\n\n  \n\n    \nA hybrid retrieval system combines the strengths of lexical and semantic methods to deliver more accurate and robust results.\n\n  \n\n  \n\n    \nOne popular technique for hybrid retrieval is Reciprocal Rank Fusion (RRF). RRF merges the rankings from different retrieval models (e.g., BM25 and a neural retriever) by assigning higher scores to documents that consistently rank well across systems.\n\n  \n\n  \nHow RRF works:\n\n    \n\n      \nEach document receives a score based on its position in the ranked lists from multiple retrieval methods.\n\n      \nThe scores are combined using the following reciprocal formula:\n\n    \n\n\n\\[\\text{RRF Score}(d) = \\sum_{i=1}^{n} \\frac{1}{k + \\text{rank}_i(d)}\\]\n\n    \nwhere:\n\n    \n\n      \n\\(d\\) is the document,\n\n      \n\\(\\text{rank}_i(d)\\) is the rank position of document \\(d\\) in the \\(i^{\\text{th}}\\) ranked list,\n\n      \n\\(k\\) is a constant (typically set to 60),\n\n      \n\\(n\\) is the number of retrieval systems.\n\n    \n\n  \n\n  \nExample:\n\n    \n\n      \n\n        \nSuppose two retrieval systems return the following top-5 rankings for a given query:\n\n      \n\n      \nBM25:\n [DocA, DocB, DocC, DocD, DocE]\n\n      \n\n        \nNeural Retriever:\n [DocF, DocC, DocA, DocG, DocB]\n\n      \n\n      \n\n        \nRRF scores are calculated as follows:\n\n\n        \n\n          \n\n            \nFor DocA (rank 1 in BM25, rank 3 in Neural Retriever):\n\n\n\\[\\text{RRF Score}(\\text{DocA}) = \\frac{1}{60 + 1} + \\frac{1}{60 + 3}\\]\n          \n\n          \n\n            \nFor DocC (rank 3 in BM25, rank 2 in Neural Retriever):\n\n\n\\[\\text{RRF Score}(\\text{DocC}) = \\frac{1}{60 + 3} + \\frac{1}{60 + 2}\\]\n          \n\n          \n\n            \nFor DocB (rank 2 in BM25, rank 5 in Neural Retriever):\n\n\n\\[\\text{RRF Score}(\\text{DocB}) = \\frac{1}{60 + 2} + \\frac{1}{60 + 5}\\]\n          \n\n          \n\n            \nAfter computing scores for all documents, the final RRF ranking is determined by sorting them in descending order of their cumulative scores.\n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \nBenefits of Hybrid Retrieval:\n\n    \n\n      \nIncreases recall by retrieving relevant documents that either lexical or semantic methods might miss individually.\n\n      \nBalances precision and coverage.\n\n      \nMakes the retrieval system more resilient to query variations and noise.\n\n    \n\n  \n\n\n\n\n\nThe Retrieval Augmented Generation (RAG) Pipeline\n\n\n\n\n  \nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge sources such as databases.\n\n  \nIt leverages a retriever to find relevant contexts to condition the LLM, in this way, RAG is able to augment the knowledge-base of an LLM with relevant documents.\n\n  \nThe retriever here could be any of the following depending on the need for semantic retrieval or not:\n    \n\n      \nVector database:\n Typically, queries are embedded using models like BERT for generating dense vector embeddings. Alternatively, traditional methods like TF-IDF can be used for sparse embeddings. The search is then conducted based on term frequency or semantic similarity.\n\n      \nGraph database:\n Constructs a knowledge base from extracted entity relationships within the text. This approach is precise but may require exact query matching, which could be restrictive in some applications.\n\n      \nRegular SQL database:\n Offers structured data storage and retrieval but might lack the semantic flexibility of vector databases.\n\n    \n\n  \n\n  \nThe image below from \nDamien Benveniste, PhD\n talks a bit about the difference between using Graph vs Vector database for RAG.\n\n\n\n\n\n\n\n\n\n  \nIn his post linked above, Damien states that Graph Databases are favored for Retrieval Augmented Generation (RAG) when compared to Vector Databases. While Vector Databases partition and index data using LLM-encoded vectors, allowing for semantically similar vector retrieval, they may fetch irrelevant data.\n\n  \nGraph Databases, on the other hand, build a knowledge base from extracted entity relationships in the text, making retrievals concise. However, it requires exact query matching which can be limiting.\n\n  \n\n    \nA potential solution could be to combine the strengths of both databases: indexing parsed entity relationships with vector representations in a graph database for more flexible information retrieval. It remains to be seen if such a hybrid model exists.\n\n  \n\n  \nAfter retrieving, you may want to look into filtering the candidates further by adding ranking and/or fine ranking layers that allow you to filter down candidates that do not match your business rules, are not personalized for the user, current context, or response limit.\n\n  \nLet’s succinctly summarize the process of RAG and then delve into its pros and cons:\n    \n\n      \nVector Database Creation\n: RAG starts by converting an internal dataset into vectors and storing them in a vector database (or a database of your choosing).\n\n      \nUser Input\n: A user provides a query in natural language, seeking an answer or completion.\n\n      \nInformation Retrieval\n: The retrieval mechanism scans the vector database to identify segments that are semantically similar to the user’s query (which is also embedded). These segments are then given to the LLM to enrich its context for generating responses.\n\n      \nCombining Data\n: The chosen data segments from the database are combined with the user’s initial query, creating an expanded prompt.\n\n      \nGenerating Text\n: The enlarged prompt, filled with added context, is then given to the LLM, which crafts the final, context-aware response.\n\n    \n\n  \n\n  \nThe image below \n(source)\n displays the high-level working of RAG.\n\n\n\n\n\n\nBenefits of RAG\n\n\n\n\n  \nSo why should you use RAG for your application?\n    \n\n      \nWith RAG, the LLM is able to leverage knowledge and information that is not necessarily in its weights by providing it access to external knowledge bases.\n\n      \nRAG doesn’t require model retraining, saving time and computational resources.\n\n      \nIt’s effective even with a limited amount of labeled data.\n\n      \nHowever, it does have its drawbacks, namely RAG’s performance depends on the comprehensiveness and correctness of the retriever’s knowledge base.\n\n      \nRAG is best suited for scenarios with abundant unlabeled data but scarce labeled data and is ideal for applications like virtual assistants needing real-time access to specific information like product manuals.\n        \n\n          \nScenarios with abundant unlabeled data but scarce labeled data: RAG is useful in situations where there is a lot of data available, but most of it is not categorized or labeled in a way that’s useful for training models. As an example, the internet has vast amounts of text, but most of it isn’t organized in a way that directly answers specific questions.\n\n          \nFurthermore, RAG is ideal for applications like virtual assistants: Virtual assistants, like Siri or Alexa, need to pull information from a wide range of sources to answer questions in real-time. They need to understand the question, retrieve relevant information, and then generate a coherent and accurate response.\n\n          \nNeeding real-time access to specific information like product manuals: This is an example of a situation where RAG models are particularly useful. Imagine you ask a virtual assistant a specific question about a product, like “How do I reset my XYZ brand thermostat?” The RAG model would first retrieve relevant information from product manuals or other resources, and then use that information to generate a clear, concise answer.\n\n          \nIn summary, RAG models are well-suited for applications where there’s a lot of information available, but it’s not neatly organized or labeled.\n\n        \n\n      \n\n    \n\n  \n\n  \nBelow, let’s take a look at the publication that introduced RAG and how the original paper implemented the framework.\n\n\n\n\n\nRAG vs. Fine-tuning\n\n\n\n\n  \nThe table below \n(source)\n compares RAG vs. fine-tuning.\n\n\n\n\n\n\n\n\n\n  \nTo summarize the above table:\n    \n\n      \nRAG offers Large Language Models (LLMs) access to factual, access-controlled, timely information. This integration enables LLMs to fetch precise and verified facts directly from relevant databases and knowledge repositories in real-time. While fine-tuning can address some of these aspects by adapting the model to specific data, RAG excels at providing up-to-date and specific information without the substantial costs associated with fine-tuning. Moreover, RAG enhances the model’s ability to remain current and relevant by dynamically accessing and retrieving the latest data, thus ensuring the responses are accurate and contextually appropriate. Additionally, RAG’s approach to leveraging external sources can be more flexible and scalable, allowing for easy updates and adjustments without the need for extensive retraining.\n\n      \nFine-tuning adapts the style, tone, and vocabulary of LLMs so that your linguistic “paint brush” matches the desired domain and style. RAG does not provide this level of customization in terms of linguistic style and vocabulary.\n\n      \nFocus on RAG first. A successful LLM application typically involves connecting specialized data to the LLM workflow. Once you have a functional application, you can add fine-tuning to enhance the style and vocabulary of the system.\n\n    \n\n  \n\n\n\n\n\nEnsemble of RAG\n\n\n\n\n  \nLeveraging an ensemble of RAG systems offers a substantial upgrade to the model’s ability to produce rich and contextually accurate text. Here’s an enhanced breakdown of how this procedure could work:\n    \n\n      \nKnowledge sources:\n RAG models retrieve information from external knowledge stores to augment their knowledge in a particular domain. These can include passages, tables, images, etc. from domains like Wikipedia, books, news, databases.\n\n      \nCombining sources:\n At inference time, multiple retrievers can pull relevant content from different corpora. For example, one retriever searches Wikipedia, another searches news sources. Their results are concatenated into a pooled set of candidates.\n\n      \nRanking:\n The model ranks the pooled candidates by their relevance to the context.\n\n      \nSelection:\n Highly ranked candidates are selected to condition the language model for generation.\n\n      \nEnsembling:\n Separate RAG models specialized on different corpora can be ensembled. Their outputs are merged, ranked, and voted on.\n\n    \n\n  \n\n  \nMultiple knowledge sources can augment RAG models through pooling and ensembles. Careful ranking and selection helps integrate these diverse sources for improved generation.\n\n  \nOne thing to keep in mind when using multiple retrievers is to rank the different outputs from each retriever before merging them to form a response. This can be done in a variety of ways, using LTR algorithms, multi-armed bandit framework, multi-objective optimization, or according to specific business use cases.\n\n\n\n\n\nChoosing a Vector DB using a Feature Matrix\n\n\n\n\n  \nTo compare the plethora of Vector DB offerings, a feature matrix that highlights the differences between Vector DBs and which to use in which scenario is essential.\n\n  \nVector DB Comparison by VectorHub\n offers a great comparison spanning 37 vendors and 29 features (as of this writing).\n\n\n\n\n\n\n\n\n\n  \nAs a secondary resource, the following table (\nsource\n) shows a comparison of some of the prevalent Vector DB offers along various feature dimensions:\n\n\n\n\n\n\n\n\n\n  \nAccess the full spreadsheet \nhere\n.\n\n\n\n\n\nBuilding a RAG pipeline\n\n\n\n\n  \nThe image below \n(source)\n, gives a visual overview of the three different steps of RAG: Ingestion, Retrieval, and Synthesis/Response Generation.\n\n\n\n\n\n\n\n\n\n  \nIn the sections below, we will go over these key areas.\n\n\n\n\n\nIngestion\n\n\n\nChunking\n\n\n\n\n  \nChunking is the process of dividing the prompts and/or the documents to be retrieved, into smaller, manageable segments or chunks. These chunks can be defined either by a fixed size, such as a specific number of characters, sentences, or paragraphs. The choice of chunking strategy plays a critical role in determining both the performance and efficiency of your RAG system.\n\n  \nEach chunk is encoded into an embedding vector for retrieval. Smaller, more precise chunks lead to a finer match between the user’s query and the content, enhancing the accuracy and relevance of the information retrieved.\n\n  \nLarger chunks might include irrelevant information, introducing noise and potentially reducing the retrieval accuracy. By controlling the chunk size, RAG can maintain a balance between comprehensiveness and precision.\n\n  \nSo the next natural question that comes up is, how do you choose the right chunk size for your use case? The choice of chunk size in RAG is crucial. It needs to be small enough to ensure relevance and reduce noise but large enough to maintain the context’s integrity. Let’s look at a few methods below referred from \nPinecone\n:\n    \n\n      \nFixed-size Chunking:\n Simply decide the number of tokens in our chunk along with whether there should be overlap between them or not. Overlap between chunks guarantees there to be minimal semantic context loss between chunks. This option is computationally cheap and simple to implement.\n        \ntext\n \n=\n \n\"...\"\n \n# your text\n\nfrom\n \nlangchain.text_splitter\n \nimport\n \nCharacterTextSplitter\n\n\ntext_splitter\n \n=\n \nCharacterTextSplitter\n(\n\n    \nseparator\n \n=\n \n\"\n\\n\\n\n\"\n,\n\n    \nchunk_size\n \n=\n \n256\n,\n\n    \nchunk_overlap\n  \n=\n \n20\n\n\n)\n\n\ndocs\n \n=\n \ntext_splitter\n.\ncreate_documents\n([\ntext\n])\n\n\n        \n\n      \n\n      \nContext-aware Chunking:\n Content-aware chunking leverages the intrinsic structure of the text to create chunks that are more meaningful and contextually relevant. Here are several approaches to achieving this:\n        \n\n          \nSentence Splitting\n: This method aligns with models optimized for embedding sentence-level content. Different tools and techniques can be used for sentence splitting:\n            \n\n              \nNaive Splitting:\n A basic method where sentences are split using periods and new lines. Example:\n                \n   \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\n   \ndocs\n \n=\n \ntext\n.\nsplit\n(\n\".\"\n)\n\n\n                \n\n                \n\n                  \nThis method is quick but may overlook complex sentence structures.\n\n                \n\n              \n\n              \nNLTK (Natural Language Toolkit):\n A comprehensive Python library for language processing. NLTK includes a sentence tokenizer that effectively splits text into sentences. Example:\n                \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\nfrom\n \nlangchain.text_splitter\n \nimport\n \nNLTKTextSplitter\n\n\ntext_splitter\n \n=\n \nNLTKTextSplitter\n()\n\n\ndocs\n \n=\n \ntext_splitter\n.\nsplit_text\n(\ntext\n)\n\n\n                \n\n              \n\n              \nspaCy:\n An advanced Python library for NLP tasks, spaCy offers efficient sentence segmentation. Example:\n                \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\nfrom\n \nlangchain.text_splitter\n \nimport\n \nSpacyTextSplitter\n\n\ntext_splitter\n \n=\n \nSpacyTextSplitter\n()\n\n\ndocs\n \n=\n \ntext_splitter\n.\nsplit_text\n(\ntext\n)\n\n\n                \n\n              \n\n            \n\n          \n\n          \nRecursive Chunking:\n Recursive chunking is an iterative method that splits text hierarchically using various separators. It adapts to create chunks of similar size or structure by recursively applying different criteria. Example using LangChain:\n            \n   \ntext\n \n=\n \n\"...\"\n  \n# Your text\n\n   \nfrom\n \nlangchain.text_splitter\n \nimport\n \nRecursiveCharacterTextSplitter\n\n   \ntext_splitter\n \n=\n \nRecursiveCharacterTextSplitter\n(\n\n       \nchunk_size\n \n=\n \n256\n,\n\n       \nchunk_overlap\n \n=\n \n20\n\n   \n)\n\n   \ndocs\n \n=\n \ntext_splitter\n.\ncreate_documents\n([\ntext\n])\n\n\n            \n\n          \n\n          \nStructure-based Chunking:\n For formatted content like Markdown or LaTeX, specialized chunking can be applied to maintain the original structure:\n            \n\n              \nMarkdown Chunking:\n Recognizes Markdown syntax and divides content based on structure. Example:\n                \nfrom\n \nlangchain.text_splitter\n \nimport\n \nMarkdownTextSplitter\n\n\nmarkdown_text\n \n=\n \n\"...\"\n\n\nmarkdown_splitter\n \n=\n \nMarkdownTextSplitter\n(\nchunk_size\n=\n100\n,\n \nchunk_overlap\n=\n0\n)\n\n\ndocs\n \n=\n \nmarkdown_splitter\n.\ncreate_documents\n([\nmarkdown_text\n])\n\n\n                \n\n              \n\n              \nLaTeX Chunking:\n Parses LaTeX commands and environments to chunk content while preserving its logical organization.\n\n            \n\n          \n\n          \nSemantic Chunking:\n Segment text based on semantic similarity. This means that sentences with the strongest semantic connections are grouped together, while sentences that move to another topic or theme are separated into distinct chunks. \nNotebook\n.\n            \n\n              \nSemantic chunking can be summarized in four steps:\n                \n\n                  \nSplit the text into sentences, paragraphs, or other rule-based units.\n\n                  \nVectorize a window of sentences or other units.\n\n                  \nCalculate the cosine distance between the embedded windows.\n\n                  \nMerge sentences or units until the cosine similarity value reaches a specific threshold.\n\n                \n\n              \n\n              \nThe following figure (\nsource\n) visually summarizes the overall process:\n \n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \n“As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.” \n(source)\n\n\n\n\n\nFiguring out the ideal chunk size\n\n\n\n\n  \nChoosing the right chunk size is foundational to building an effective RAG system. It directly influences retrieval quality, model efficiency, and how well the system captures relevant context for downstream tasks. Poor chunking can lead to fragmented information or excessive context loss, undermining overall performance.\n\n  \n\n    \nBuilding a RAG system involves determining the ideal chunk sizes for the documents that the retriever component will process. The ideal chunk size depends on several factors:\n\n\n    \n\n      \n\n        \nData Characteristics\n: The nature of your data is crucial. For text documents, consider the average length of paragraphs or sections. If the documents are well-structured with distinct sections, these natural divisions might serve as a good basis for chunking.\n\n      \n\n      \n\n        \nRetriever Constraints\n: The retriever model you choose (like BM25, TF-IDF, or a neural retriever like DPR) might have limitations on the input length. It’s essential to ensure that the chunks are compatible with these constraints.\n\n      \n\n      \n\n        \nMemory and Computational Resources\n: Larger chunk sizes can lead to higher memory usage and computational overhead. Balance the chunk size with the available resources to ensure efficient processing.\n\n      \n\n      \n\n        \nTask Requirements\n: The nature of the task (e.g., question answering, document summarization) can influence the ideal chunk size. For detailed tasks, smaller chunks might be more effective to capture specific details, while broader tasks might benefit from larger chunks to capture more context.\n\n      \n\n      \n\n        \nExperimentation\n: Often, the best way to determine the ideal chunk size is through empirical testing. Run experiments with different chunk sizes and evaluate the performance on a validation set to find the optimal balance between granularity and context.\n\n      \n\n      \n\n        \nOverlap Consideration\n: Sometimes, it’s beneficial to have overlap between chunks to ensure that no important information is missed at the boundaries. Decide on an appropriate overlap size based on the task and data characteristics.\n\n      \n\n    \n\n  \n\n  \nTo summarize, determining the ideal chunk size for a RAG system is a balancing act that involves considering the characteristics of your data, the limitations of your retriever model, the resources at your disposal, the specific requirements of your task, and empirical experimentation. It’s a process that may require iteration and fine-tuning to achieve the best results.\n\n\n\n\n\nRetriever Ensembling and Reranking\n\n\n\n\n  \nIn some scenarios, it may be beneficial to simultaneously utilize multiple chunk sizes and apply a re-ranking mechanism to refine the retrieved results. A detailed discourse on re-ranking is available in the \nRe-ranking\n section.\n\n  \nThis approach serves two primary purposes:\n    \n\n      \nIt potentially improves the quality of retrieved content—albeit at increased computational cost—by aggregating outputs from multiple chunking strategies, provided the re-ranker performs with a reasonable degree of accuracy.\n\n      \nIt enables systematic comparison of different retrieval methods relative to the re-ranker’s effectiveness.\n\n    \n\n  \n\n  \n\n    \nThe methodology proceeds as follows:\n\n\n    \n\n      \nSegment the same source document using various chunk sizes, for example: 128, 256, 512, and 1024 tokens.\n\n      \nDuring the retrieval phase, extract relevant segments from each retrieval method, thereby forming an ensemble of retrievers.\n\n      \nApply a re-ranking model to prioritize and filter the aggregated results.\n\n    \n\n  \n\n  \nThe following diagram \n(source)\n illustrates the process.\n\n\n\n\n\n\n\n\n\n  \nAccording to \nevaluation data provided by LlamaIndex\n, the ensemble retrieval strategy leads to a modest improvement in faithfulness metrics, suggesting slightly enhanced relevance of retrieved content. However, pairwise comparisons show equal preference between the ensembled and baseline approaches, thereby leaving the superiority of ensembling open to debate.\n\n  \nIt is important to note that this ensembling methodology is not limited to variations in chunk size. It can also be extended to other dimensions of a RAG pipeline, including vector-based, keyword-based, and hybrid search strategies.\n\n\n\n\n\nEmbeddings\n\n\n\n\n  \nOnce you have your prompt chunked appropriately, the next step is to embed it. Embedding prompts and documents in RAG involves transforming both the user’s query (prompt) and the documents in the knowledge base into a format that can be effectively compared for relevance. This process is critical for RAG’s ability to retrieve the most relevant information from its knowledge base in response to a user query. Here’s how it typically works:\n\n  \nOne option to help pick which embedding model would be best suited for your task is to look at \nHuggingFace’s Massive Text Embedding Benchmark (MTEB) leaderboard\n. There is a question of whether a dense or sparse embedding can be used so let’s look into benefits of each below:\n\n  \nSparse embedding:\n Sparse embeddings such as TF-IDF are great for lexical matching the prompt with the documents. Best for applications where keyword relevance is crucial. It’s computationally less intensive but may not capture the deeper semantic meanings in the text.\n\n  \nSemantic embedding:\n Semantic embeddings, such as BERT or SentenceBERT lend themselves naturally to the RAG use-case.\n    \n\n      \nBERT:\n Suitable for capturing contextual nuances in both the documents and queries. Requires more computational resources compared to sparse embeddings but offers more semantically rich embeddings.\n\n      \nSentenceBERT:\n Ideal for scenarios where the context and meaning at the sentence level are important. It strikes a balance between the deep contextual understanding of BERT and the need for concise, meaningful sentence representations. This is usually the preferred route for RAG.\n\n    \n\n  \n\n\n\n\n\nNaive Chunking vs. Late Chunking vs. Late Interaction (\nColBERT\n and \nColPali\n)\n\n\n\n\n  \n\n    \nThe choice between naive chunking, late chunking, and late interaction (\nColBERT\n/\nColPali\n) depends on the specific requirements of the retrieval task:\n\n\n    \n\n      \nNaive Chunking\n is suitable for scenarios with strict resource constraints but where retrieval precision is less critical.\n\n      \nLate Chunking\n, introduced by \nJinaAI\n, offers an attractive middle ground, maintaining context and providing improved retrieval accuracy without incurring significant additional costs. Put simply, late chunking balances the trade-offs between cost and precision, making it an excellent option for building scalable and effective RAG systems, particularly in long-context retrieval scenarios.\n\n      \nLate Interaction (\nColBERT\n/\nColPali\n)\n is best suited for applications where retrieval precision is paramount and resource costs are less of a concern.\n\n    \n\n  \n\n  \n\n    \nLet’s explore the differences between three primary strategies: Naive Chunking, Late Chunking, and Late Interaction (\nColBERT\n and \nColPali\n), focusing on their methodologies, advantages, and trade-offs.\n\n  \n\n\n\n\n\nOverview\n\n\n\n\n  \nLong-context retrieval presents a challenge when balancing precision, context retention, and cost efficiency. Solutions range from simple and low-cost, like Naive Chunking, to more sophisticated and resource-intensive approaches, such as \nLate Interaction ([ColBERT](https://arxiv.org/abs/2004.12832))\n. \nLate Chunking\n, a novel approach by \nJinaAI\n, offers a middle ground, preserving context with efficiency comparable to Naive Chunking.\n\n\n\n\n\n\n\n\nNaive/Vanilla Chunking\n\n\n\nWhat is Naive/Vanilla Chunking?\n\n\n\n\n  \nAs discussed in the \nChunking\n section, naive/vanilla chunking divides a document into fixed-size chunks based on metrics like sentence boundaries or token count (e.g., 512 tokens per chunk).\n\n  \nEach chunk is independently embedded into a vector without considering the context of neighboring chunks.\n\n\n\n\n\nExample\n\n\n\n\n  \n\n    \nConsider the following paragraph: \nAlice went for a walk in the woods one day and on her walk, she spotted something. She saw a rabbit hole at the base of a large tree. She fell into the hole and found herself in a strange new world.\n\n  \n\n  \n\n    \nIf chunked by sentences:\n\n    \n\n      \nChunk 1\n: “Alice went for a walk in the woods one day and on her walk, she spotted something.”\n\n      \nChunk 2\n: “She saw a rabbit hole at the base of a large tree.”\n\n      \nChunk 3\n: “She fell into the hole and found herself in a strange new world.”\n\n    \n\n  \n\n\n\n\n\nAdvantages and Limitations\n\n\n\n\n  \nAdvantages\n:\n    \n\n      \nEfficient in terms of storage and computation.\n\n      \nSimple to implement and integrate with most retrieval pipelines.\n\n    \n\n  \n\n  \nLimitations\n:\n    \n\n      \nContext Loss\n: Each chunk is processed independently, leading to a loss of contextual relationships. For example, the connection between “she” and “Alice” would be lost, reducing retrieval accuracy for context-heavy queries like “Where did Alice fall?”.\n\n      \nFragmented Meaning\n: Splitting paragraphs or semantically related sections can dilute the meaning of each chunk, reducing retrieval precision.\n\n    \n\n  \n\n\n\n\n\nLate Chunking\n\n\n\nWhat is Late Chunking?\n\n\n\n\n  \nLate Chunking flips the order of vectorizing (i.e., embedding generation) and chunking compared to naive/vanilla chunking. In other words, it delays the chunking process until after the entire document has been embedded into token-level representations. This allows chunks to retain context from the full document, leading to richer, more contextually aware embeddings.\n\n\n\n\n\nHow Late Chunking Works\n\n\n\n\n  \nEmbedding First\n: The entire document is embedded into token-level representations using a long context model.\n\n  \nChunking After\n: After embedding, the token-level representations are pooled into chunks based on a predefined chunking strategy (e.g., 512-token chunks).\n\n  \nContext Retention\n: Each chunk retains contextual information from the full document, allowing for improved retrieval precision without increasing storage costs.\n\n\n\n\n\nExample\n\n\n\n\n  \nUsing the same paragraph:\n    \n\n      \nThe entire paragraph is first embedded as a whole, preserving the relationships between all sentences.\n\n      \nThe document is then split into chunks after embedding, ensuring that chunks like “She fell into the hole…” are contextually aware of the mention of “Alice” from earlier sentences.\n\n    \n\n  \n\n\n\n\n\nAdvantages and Trade-offs\n\n\n\n\n  \nAdvantages\n:\n    \n\n      \nContext Preservation\n: Late chunking ensures that the relationship between tokens across different chunks is maintained.\n\n      \nEfficiency\n: Late chunking requires the same amount of storage as naive chunking while significantly improving retrieval accuracy.\n\n    \n\n  \n\n  \nTrade-offs\n:\n    \n\n      \nRequires Long Context Models\n: To embed the entire document at once, a model with long-context capabilities (e.g., supporting up to 8192 tokens) is necessary.\n\n      \nSlightly Higher Compute Costs\n: Late chunking introduces an extra pooling step after embedding, although it’s more efficient than late interaction approaches like \nColBERT\n.\n\n    \n\n  \n\n\n\n\n\nLate Interaction\n\n\n\nWhat is Late Interaction?\n\n\n\n\n  \nLate Interaction refers to a retrieval approach where token embeddings for both the document and the query are computed separately and compared at the token level, without any pooling operation. The key advantage is fine-grained, token-level matching, which improves retrieval accuracy.\n\n\n\n\n\nColBERT\n: Late Interaction in Practice\n\n\n\n\n  \nColBERT\n (Contextualized Late Interaction over BERT) by Khattab et al. (2020) uses late interaction to compare individual token embeddings from the query and document using a MaxSim operator. This allows for granular, token-to-token comparisons, which results in highly precise matches but at a significantly higher storage cost.\n\n\n\n\n\nMaxSim: A Key Component of \nColBERT\n\n\n\n\n  \nMaxSim\n (Maximum Similarity) is a core component of the \nColBERT\n retrieval framework. It refers to a specific way of calculating the similarity between token embeddings of a query and document during retrieval.\n\n  \nHere’s a step-by-step breakdown of how MaxSim works:\n    \n\n      \nToken-level Embedding Comparisons\n:\n        \n\n          \nWhen a query is processed, it is tokenized and each token is embedded separately (e.g., “apple” and “sweet”).\n\n          \nThe document is already indexed at the token level, meaning that each token in the document also has its own embedding.\n\n        \n\n      \n\n      \nSimilarity Computation\n:\n        \n\n          \nAt query time, the system compares each query token embedding to every token embedding in the document. The similarity between two token embeddings is often measured using a dot product or cosine similarity.\n\n          \nFor example, given a query token \n\"apple\"\n and a document containing tokens like \n\"apple\"\n, \n\"banana\"\n, and \n\"fruit\"\n, the system computes the similarity of \n\"apple\"\n to each of these tokens.\n\n        \n\n      \n\n      \nSelecting Maximum Similarity (MaxSim)\n:\n        \n\n          \nThe system selects the highest similarity score between the query token and the document tokens. This is known as the MaxSim operation.\n\n          \nIn the above example, the system compares the similarity of \n\"apple\"\n (query token) with all document tokens and selects the highest similarity score, say between \n\"apple\"\n and the corresponding token \n\"apple\"\n in the document.\n\n        \n\n      \n\n      \nMaxSim Aggregation\n:\n        \n\n          \nThe MaxSim scores for each token in the query are aggregated (usually summed) to calculate a final relevance score for the document with respect to the query.\n\n        \n\n        \n\n          \nThis approach allows for token-level precision, capturing subtle nuances in the document-query matching that would be lost with traditional pooling methods.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nExample\n\n\n\n\n  \n\n    \nConsider the query \n\"sweet apple\"\n and two documents:\n\n\n    \n\n      \nDocument 1\n: “The apple is sweet and crisp.”\n\n      \nDocument 2\n: “The banana is ripe and yellow.”\n\n    \n\n  \n\n  \n\n    \nEach query token, \n\"sweet\"\n and \n\"apple\"\n, is compared with every token in both documents:\n\n\n    \n\n      \nFor \nDocument 1\n, \n\"sweet\"\n has a high similarity with \n\"sweet\"\n in the document, and \n\"apple\"\n has a high similarity with \n\"apple\"\n.\n\n      \nFor \nDocument 2\n, \n\"sweet\"\n does not have a strong match with any token, and \n\"apple\"\n does not appear.\n\n    \n\n  \n\n  \n\n    \nUsing MaxSim, Document 1 would have a higher relevance score for the query than Document 2 because the most similar tokens in Document 1 (i.e., \n\"sweet\"\n and \n\"apple\"\n) align more closely with the query tokens.\n\n  \n\n\n\n\n\nAdvantages and Trade-offs\n\n\n\n\n  \nAdvantages\n:\n    \n\n      \nHigh Precision\n: \nColBERT\n’s token-level comparisons, facilitated by MaxSim, lead to highly accurate retrieval, particularly for specific or complex queries.\n\n      \nFlexible Query Matching\n: By calculating similarity at the token level, \nColBERT\n can capture fine-grained relationships that simpler models might overlook.\n\n    \n\n  \n\n  \nTrade-offs\n:\n    \n\n      \nStorage Intensive\n: Storing all token embeddings for each document can be extremely costly. For example, storing token embeddings for a corpus of 100,000 documents could require upwards of 2.46 TB.\n\n      \nComputational Complexity\n: While precise, MaxSim increases computational demands at query time, as each token in the query must be compared to all tokens in the document.\n\n    \n\n  \n\n\n\n\n\nColPali\n: Expanding to Multimodal Retrieval\n\n\n\n\n  \nColPali\n by Faysse et al. (2024) integrates the late interaction mechanism from \nColBERT\n with a Vision Language Model (VLM) called PaliGemma to handle multimodal documents, such as PDFs with text, images, and tables. Instead of relying on OCR and layout parsing, \nColPali\n uses screenshots of PDF pages to directly embed both visual and textual content. This enables powerful multimodal retrieval in complex documents.\n\n\n\n\n\nExample\n\n\n\n\n  \nConsider a complex PDF with both text and images. \nColPali\n treats each page as an image and embeds it using a VLM. When a user queries the system, the query is matched with embedded screenshots via late interaction, improving the ability to retrieve relevant pages based on both visual and textual content.\n\n\n\n\n\n\n\n\nComparative Analysis\n\n\n\n\n\n\n \n\n\n\n\nMetric\n\n\nNaive Chunking\n\n\nLate Chunking\n\n\nLate Interaction ([ColBERT](https://arxiv.org/abs/2004.12832))\n\n\n\n\n\n\n\n\n\n\nStorage Requirements\n\n\nMinimal storage, ~4.9 GB for 100,000 documents\n\n\nSame as naive chunking, ~4.9 GB for 100,000 documents\n\n\nExtremely high storage, ~2.46 TB for 100,000 documents\n\n\n\n\n\n\nRetrieval Precision\n\n\nLower precision due to context fragmentation\n\n\nImproved precision by retaining context across chunks\n\n\nHighest precision with token-level matching\n\n\n\n\n\n\nComplexity and Cost\n\n\nSimple implementation, minimal resources\n\n\nModerately more complex, efficient in compute and storage\n\n\nHighly complex, resource-intensive in both storage and computation\n\n\n\n\n\n\n\n\n\n\n\nSentence Embeddings: The What and Why\n\n\n\nBackground: Differences compared to Token-Level Models like BERT\n\n\n\n\n  \nAs an overview, let’s look into how sentence transformers differ compared to token-level embedding models such as BERT.\n\n  \nSentence Transformers are a modification of the traditional BERT model, tailored specifically for generating embeddings of entire sentences (i.e., sentence embeddings). The key differences in their training approaches are:\n    \n\n      \nObjective\n: BERT is trained to predict masked words in a sentence and next sentence prediction. It’s optimized for understanding words and their context within a sentence. Sentence Transformers, on the other hand, are trained specifically to understand the meaning of entire sentences. They generate embeddings where sentences with similar meanings are close in the embedding space.\n\n      \nLevel of Embedding\n: The primary difference lies in the level of embedding. BERT provides embeddings for each token (word or subword) in a sentence, whereas sentence transformers provide a single embedding for the entire sentence.\n\n      \nTraining Data and Tasks\n: While BERT is primarily trained on large text corpora with tasks focused on understanding words in context, Sentence Transformers are often trained on data sets that include sentence pairs. This training focuses on similarity and relevance, teaching the model how to understand and compare the meanings of entire sentences.\n\n      \nSiamese and Triplet Network Structures\n: Sentence Transformers often use Siamese or Triplet network structures. These networks involve processing pairs or triplets of sentences and adjusting the model so that similar sentences have similar embeddings, and dissimilar sentences have different embeddings. This is different from BERT’s training, which does not inherently involve direct comparison of separate sentences.\n\n      \nFine-Tuning for Specific Tasks\n: Sentence Transformers are often fine-tuned on specific tasks like semantic similarity, paraphrase identification, or information retrieval. This fine-tuning is more focused on sentence-level understanding as opposed to BERT, which might be fine-tuned for a wider range of NLP tasks like question answering, sentiment analysis, etc., focusing on word or phrase-level understanding.\n\n      \nApplicability\n: BERT and similar models are more versatile for tasks that require understanding at the token level (like named entity recognition, question answering), whereas sentence transformers are more suited for tasks that rely on sentence-level understanding (like semantic search, sentence similarity).\n\n      \nEfficiency in Generating Sentence Embeddings or Similarity Tasks\n: In standard BERT, generating sentence embeddings usually involves taking the output of one of the hidden layers (often the first token, \n[CLS]\n) as a representation of the whole sentence. However, this method is not always optimal for sentence-level tasks. Sentence Transformers are specifically optimized to produce more meaningful and useful sentence embeddings and are thus more efficient for tasks involving sentence similarity computations. Since they produce a single vector per sentence, computing similarity scores between sentences is computationally less intensive compared to token-level models.\n\n    \n\n  \n\n  \nIn summary, while BERT is a general-purpose language understanding model with a focus on word-level contexts, Sentence Transformers are adapted specifically for understanding and comparing the meanings of entire sentences, making them more effective for tasks that require sentence-level semantic understanding.\n\n\n\n\n\nRelated: Training Process for Sentence Transformers vs. Token-Level Embedding Models\n\n\n\n\n  \nLet’s look into how sentence transformers trained differently compared to token-level embedding models such as BERT.\n\n  \nSentence transformers are trained to generate embeddings at the sentence level, which is a distinct approach from token-level embedding models like BERT. Here’s an overview of their training and how it differs from token-level models:\n    \n\n      \nModel Architecture\n: Sentence transformers often start with a base model similar to BERT or other transformer architectures. However, the focus is on outputting a single embedding vector for the entire input sentence, rather than individual tokens.\n\n      \nTraining Data\n: They are trained on a variety of datasets, often including pairs or groups of sentences where the relationship (e.g., similarity, paraphrasing) between the sentences is known.\n\n      \nTraining Objectives\n: BERT is pre-trained on objectives like masked language modeling (predicting missing words) and next sentence prediction, which are focused on understanding the context at the token level. Sentence transformers, on the other hand, are trained specifically to understand the context and relationships at the sentence level. Their training objective is typically to minimize the distance between embeddings of semantically similar sentences while maximizing the distance between embeddings of dissimilar sentences. This is achieved through contrastive loss functions like triplet loss, cosine similarity loss, etc.\n\n      \nOutput Representation\n: In BERT, the sentence-level representation is typically derived from the embedding of a special token (like \n[CLS]\n) or by pooling (i.e., averaging) token embeddings. Sentence transformers are designed to directly output a meaningful sentence-level representation.\n\n      \nFine-tuning for Downstream Tasks\n: Sentence transformers can be fine-tuned on specific tasks, such as semantic text similarity, where the model learns to produce embeddings that capture the nuanced meaning of entire sentences.\n\n    \n\n  \n\n  \nIn summary, sentence transformers are specifically optimized for producing representations at the sentence level, focusing on capturing the overall semantics of sentences, which makes them particularly useful for tasks involving sentence similarity and clustering. This contrasts with token-level models like BERT, which are more focused on understanding and representing the meaning of individual tokens within their wider context.\n\n\n\n\n\nApplying Sentence Transformers for RAG\n\n\n\n\n  \nNow, let’s look into why sentence transformers are the numero uno choice of models to generate embeddings for RAG.\n\n  \nRAG leverages Sentence Transformers for their ability to understand and compare the semantic content of sentences. This integration is particularly useful in scenarios where the model needs to retrieve relevant information before generating a response. Here’s how Sentence Transformers are useful in a RAG setting:\n    \n\n      \nImproved Document Retrieval\n: Sentence Transformers are trained to generate embeddings that capture the semantic meaning of sentences. In a RAG setting, these embeddings can be used to match a query (like a user’s question) with the most relevant documents or passages in a database. This is critical because the quality of the generated response often depends on the relevance of the retrieved information.\n\n      \nEfficient Semantic Search\n: Traditional keyword-based search methods might struggle with understanding the context or the semantic nuances of a query. Sentence Transformers, by providing semantically meaningful embeddings, enable more nuanced searches that go beyond keyword matching. This means that the retrieval component of RAG can find documents that are semantically related to the query, even if they don’t contain the exact keywords.\n\n      \nContextual Understanding for Better Responses\n: By using Sentence Transformers, the RAG model can better understand the context and nuances of both the input query and the content of potential source documents. This leads to more accurate and contextually appropriate responses, as the generation component of the model has more relevant and well-understood information to work with.\n\n      \nScalability in Information Retrieval\n: Sentence Transformers can efficiently handle large databases of documents by pre-computing embeddings for all documents. This makes the retrieval process faster and more scalable, as the model only needs to compute the embedding for the query at runtime and then quickly find the closest document embeddings.\n\n      \nEnhancing the Generation Process\n: In a RAG setup, the generation component benefits from the retrieval component’s ability to provide relevant, semantically-rich information. This allows the language model to generate responses that are not only contextually accurate but also informed by a broader range of information than what the model itself was trained on.\n\n    \n\n  \n\n  \nIn summary, Sentence Transformers enhance the retrieval capabilities of RAG models with LLMs by enabling more effective semantic search and retrieval of information. This leads to improved performance in tasks that require understanding and generating responses based on large volumes of text data, such as question answering, chatbots, and information extraction.\n\n\n\n\n\nRetrieval\n\n\n\n\n  \nLet’s look at three different types of retrieval: standard, sentence window, and auto-merging. Each of these approaches has specific strengths and weaknesses, and their suitability depends on the requirements of the RAG task, including the nature of the dataset, the complexity of the queries, and the desired balance between specificity and contextual understanding in the responses.\n\n\n\n\n\nStandard/Naive approach\n\n\n\n\n  \nAs we see in the image below \n(source)\n, the standard pipeline uses the same text chunk for indexing/embedding as well as the output synthesis.\n\n\n\n\n\n\n\n\n\n  \nIn the context of RAG in LLMs, here are the advantages and disadvantages of the three approaches:\n\n\n\n\n\nAdvantages\n\n\n\n  \nSimplicity and Efficiency\n: This method is straightforward and efficient, using the same text chunk for both embedding and synthesis, simplifying the retrieval process.\n\n  \nUniformity in Data Handling\n: It maintains consistency in the data used across both retrieval and synthesis phases.\n\n\n\n\n\nDisadvantages\n\n\n\n  \nLimited Contextual Understanding\n: LLMs may require a larger window for synthesis to generate better responses, which this approach may not adequately provide.\n\n  \nPotential for Suboptimal Responses\n: Due to the limited context, the LLM might not have enough information to generate the most relevant and accurate responses.\n\n\n\n\n\nSentence-Window Retrieval / Small-to-Large Retrieval\n\n\n\n  \nThe sentence-window approach breaks down documents into smaller units, such as sentences or small groups of sentences.\n\n  \nIt decouples the embeddings for retrieval tasks (which are smaller chunks stored in a Vector DB), but for synthesis it adds back in the context around the retrieved chunks, as seen in the image below \n(source)\n.\n\n\n\n\n\n\n\n\n\n  \nDuring retrieval, we retrieve the sentences that are most relevant to the query via similarity search and replace the sentence with the full surrounding context (using a static sentence-window around the context, implemented by retrieving sentences surrounding the one being originally retrieved) as shown in the figure below \n(source)\n.\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\n  \nEnhanced Specificity in Retrieval\n: By breaking documents into smaller units, it enables more precise retrieval of segments directly relevant to a query.\n\n  \nContext-Rich Synthesis\n: It reintroduces context around the retrieved chunks for synthesis, providing the LLM with a broader understanding to formulate responses.\n\n  \nBalanced Approach\n: This method strikes a balance between focused retrieval and contextual richness, potentially improving response quality.\n\n\n\n\n\nDisadvantages\n\n\n\n  \nIncreased Complexity\n: Managing separate processes for retrieval and synthesis adds complexity to the pipeline.\n\n  \nPotential Contextual Gaps\n: There’s a risk of missing broader context if the surrounding information added back is not sufficiently comprehensive.\n\n\n\n\n\nAuto-merging Retriever / Hierarchical Retriever\n\n\n\n\n  \nThe image below \n(source)\n, illustrates how auto-merging retrieval can work where it doesn’t retrieve a bunch of fragmented chunks as would happen with the naive approach.\n\n\n\n\n\n\n\n\n\n  \nThe fragmentation in the naive approach would be worse with smaller chunk sizes as shown below \n(source)\n.\n\n\n\n\n\n\n\n\n\n  \nAuto-merging retrieval aims to combine (or merge) information from multiple sources or segments of text to create a more comprehensive and contextually relevant response to a query. This approach is particularly useful when no single document or segment fully answers the query but rather the answer lies in combining information from multiple sources.\n\n  \nIt allows smaller chunks to be merged into bigger parent chunks. It does this via the following steps:\n    \n\n      \nDefine a hierarchy of smaller chunks linked to parent chunks.\n\n      \nIf the set of smaller chunks linking to a parent chunk exceeds some threshold (say, cosine similarity), then “merge” smaller chunks into the bigger parent chunk.\n\n    \n\n  \n\n  \nThe method will finally retrieve the parent chunk for better context.\n\n\n\n\n\nAdvantages\n\n\n\n  \nComprehensive Contextual Responses\n: By merging information from multiple sources, it creates responses that are more comprehensive and contextually relevant.\n\n  \nReduced Fragmentation\n: This approach addresses the issue of fragmented information retrieval, common in the naive approach, especially with smaller chunk sizes.\n\n  \nDynamic Content Integration\n: It dynamically combines smaller chunks into larger, more informative ones, enhancing the richness of the information provided to the LLM.\n\n\n\n\n\nDisadvantages\n\n\n\n  \nComplexity in Hierarchy and Threshold Management\n: The process of defining hierarchies and setting appropriate thresholds for merging is complex and critical for effective functioning.\n\n  \nRisk of Over-generalization\n: There’s a possibility of merging too much or irrelevant information, leading to responses that are too broad or off-topic.\n\n  \nComputational Intensity\n: This method might be more computationally intensive due to the additional steps in merging and managing the hierarchical structure of text chunks.\n\n\n\n\n\nContextual Retrieval\n\n\n\n\n  \nFor LLMs to deliver relevant and accurate responses, they must retrieve the right information from a knowledge base. Traditional RAG improves model accuracy by fetching relevant text chunks and appending them to the prompt. However, such methods often remove crucial context when encoding information, leading to failed retrievals and suboptimal outputs.\n\n  \nContextual Retrieval, introduced by \nAnthropic\n, is an advanced technique designed to improve this process by ensuring that retrieved chunks maintain their original context. It employs contextual embeddings – embeddings that incorporate chunk-specific background information and contextual BM25 – an enhanced BM25 ranking that considers the broader document context.\n\n  \n\n    \nBy prepending contextual metadata to each document chunk before embedding, Contextual Retrieval significantly enhances search accuracy. This approach reduces failed retrievals by 49% and, when combined with reranking, by 67%.\n\n  \n\n  \nWhy Context Matters in Retrieval\n:\n    \n\n      \nTraditional RAG solutions divide documents into small chunks for efficient retrieval. However, these fragments often lose critical context. For example, the statement “The company’s revenue grew by 3% over the previous quarter” lacks information about which company or quarter it refers to. Contextual Retrieval solves this by embedding relevant metadata into each chunk.\n\n    \n\n  \n\n  \nImplementation of Contextual Retrieval\n:\n    \n\n      \nTo implement Contextual Retrieval, a model like Claude 3 Haiku can generate concise context for each chunk. This context is then prepended before embedding and indexing, ensuring more precise retrieval. Developers can automate this process at scale using specialized retrieval pipelines.\n\n    \n\n  \n\n  \nPrompt Used for Contextual Retrieval\n:\n    \n\n      \nAnthropic’s method involves using Claude to generate a short, document-specific context for each chunk using the following prompt:\n        \n  <document>  \n          \n  </document>  \n\n  Here is the chunk we want to situate within the whole document:  \n\n  <chunk>  \n          \n  </chunk>  \n\n  Please give a short, succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n\n        \n\n      \n\n      \nThis process automatically generates a concise contextualized description that is prepended to the chunk before embedding and indexing.\n\n    \n\n  \n\n  \nCombining Contextual Retrieval with Reranking\n:\n    \n\n      \nFor maximum performance, Contextual Retrieval can be paired with reranking models, which filter and reorder retrieved chunks based on their relevance. This additional step enhances retrieval precision, ensuring only the most relevant chunks are passed to the LLM.\n\n    \n\n  \n\n  \nThe following flowchart from Anthropic’s \nblog\n shows the combined contextual retrieval and reranking stages which seek to maximize retrieval accuracy.\n\n\n\n\n\n\n\n\n\n  \nKey Takeaways\n:\n    \n\n      \nContextual Embeddings improve retrieval accuracy by preserving document meaning.\n\n      \nBM25 + Contextualization enhances exact-match retrieval.\n\n      \nCombining Contextual Retrieval with reranking further boosts retrieval effectiveness.\n\n      \nDevelopers can implement Contextual Retrieval using prompt-based preprocessing and automated pipelines.\n\n    \n\n  \n\n  \nWith Contextual Retrieval, LLM-powered knowledge systems can achieve greater accuracy, scalability, and relevance, unlocking new levels of performance in real-world applications.\n\n\n\n\n\nUsing Approximate Nearest Neighbors (ANN) for Retrieval\n\n\n\n\n  \nThe next step is to consider which approximate nearest neighbors (ANN) library to choose from indexing. One option to pick the best option is to look at \nANN-Benchmarks\n.\n\n  \nA detailed discourse on the concept of ANN can be found in our \nANN primer\n.\n\n\n\n\n\nRe-ranking\n\n\n\n\n  \nRe-ranking is an optional yet critical component in RAG pipelines, functioning as the refinement stage where an initially retrieved candidate set—usually limited to dozens of documents or passages—is reordered based on their relevance to the input query. This step ensures that the most pertinent content is prioritized for inclusion in the final prompt presented to the language model.\n\n  \nAs the candidate set is small, computationally intensive but accurate re-ranking techniques are feasible, with neural Learning-to-Rank (LTR) models being the most commonly used.\n\n\n\n\n\nNeural Re-rankers: Types and Architectures\n\n\n\n\n  \n\n    \nNeural re-rankers are broadly classified into three methodological paradigms based on how they evaluate relevance: pointwise, pairwise, and listwise. Each of these paradigms corresponds to specific models: monoBERT exemplifies the pointwise approach, duoBERT represents the pairwise method, and ListBERT along with ListT5 embody the listwise strategy, as detailed below:\n\n\n    \n\n      \n\n        \nmonoBERT\n, proposed by Nogueira et al. (2019) in \nMulti-Stage Document Ranking with BERT\n, scores each document-query pair independently using BERT as a cross-encoder. Each document is concatenated with the query, and a relevance score is predicted for that pair alone. This makes monoBERT a pointwise model, offering high-quality relevance estimation but at a high inference cost when applied to many pairs.\n\n      \n\n      \n\n        \nduoBERT\n, also proposed by Nogueira et al. (2019) in \nMulti-Stage Document Ranking with BERT\n, extends monoBERT by comparing pairs of documents relative to a given query. It predicts which of two documents is more relevant, allowing for more nuanced and direct ranking decisions. This pairwise approach helps resolve fine-grained distinctions in relevance that monoBERT might overlook.\n\n      \n\n      \n\n        \nListBERT\n, proposed by Kumar et al. (2022) in \nListBERT: Learning to Rank E-commerce products with Listwise BERT\n, brings a listwise learning paradigm to transformer-based ranking. Instead of scoring documents independently or in pairs, ListBERT considers a full list of documents simultaneously. It uses listwise loss functions tailored for ranking tasks (e.g., ListMLE, Softmax Cross Entropy) and was originally applied in the context of e-commerce to rank products effectively.\n\n      \n\n      \n\n        \nListT5\n, proposed by Yoon et al. (2024) in \nListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval\n, advances the listwise approach further by employing a Fusion-in-Decoder (FiD) architecture adapted from T5. This model jointly attends to multiple candidate documents during both training and inference, making it particularly suitable for zero-shot retrieval scenarios. It has shown state-of-the-art performance in contexts requiring the ranking of passages with minimal labeled data.\n\n      \n\n    \n\n  \n\n  \n\n    \nWhile \nmonoBERT\n and \nduoBERT\n utilize cross-encoder architectures, \nListBERT\n and \nListT5\n employ different mechanisms. ListBERT uses a listwise approach with BERT, and ListT5 is based on a Fusion-in-Decoder (FiD) architecture adapted from T5, which is not a traditional cross-encoder.\n\n  \n\n\n\n\n\nDomain-Specific Adaptations\n\n\n\n\n  \nReranking models can also be fine-tuned for specific domains to improve performance in specialized applications. For instance, \nLegal-BERT\n has been adapted for tasks like legal document classification and contract clause retrieval, demonstrating that domain-specific pretraining significantly boosts accuracy in re-ranking. Similar adaptations exist in finance, healthcare, and technical fields where vocabulary and relevance judgments differ markedly from general-purpose datasets.\n\n\n\n\n\nInstruction-Following Re-ranking: Precision and Control in RAG\n\n\n\n\n  \n\n    \nA growing frontier in reranking is instruction-following re-ranking, which introduces the ability to control ranking behavior using natural language instructions. This approach addresses the limitations of static relevance criteria by allowing dynamic, context-specific customization, which is particularly beneficial in enterprise RAG systems where documents may conflict or vary in trustworthiness and recency.\n\n  \n\n  \n\n    \nExamples of Natural Language Instructions\n:\n\n\n    \n\n      \n“Prioritize internal documentation over third-party sources. Favor the most recent information.”\n\n      \n“Disregard news summaries. Emphasize detailed technical reports from trusted analysts.”\n\n    \n\n  \n\n  \n\n    \nAdvantages\n:\n\n\n    \n\n      \nDynamic Relevance Modeling\n: Instructions enable runtime tuning of what “relevance” means, depending on the user’s intent or business context.\n\n      \nConflict Resolution\n: They allow systems to resolve contradictory or overlapping sources by enforcing prioritization rules.\n\n      \nPrompt Optimization\n: Ensuring higher-quality content appears earlier in the prompt helps maximize the utility of the limited context window, where the LLM’s attention is most focused.\n\n    \n\n  \n\n  \n\n    \nImplementation and Deployment\n:\n\n\n    \n\n      \nThese rerankers are often deployed as standalone APIs or integrated modules that rescore a shortlist of candidate documents after the initial retrieval phase. They can be combined with other techniques such as late chunking or retriever ensembling, effectively acting as the final curation layer before the documents are fed into the language model.\n\n      \nA notable example of this is Contextual AI’s system, presented in their post on \nthe world’s first instruction-following reranker\n, which demonstrates real-world integration of this technology.\n\n    \n\n  \n\n\n\n\n\nResponse Generation / Synthesis\n\n\n\n\n  \nThe last step of the RAG pipeline is to generate responses back to the user. In this step, the model synthesizes the retrieved information with its pre-trained knowledge to generate coherent and contextually relevant responses. This process involves integrating the insights gleaned from various sources, ensuring accuracy and relevance, and crafting a response that is not only informative but also aligns with the user’s original query, maintaining a natural and conversational tone.\n\n  \nNote that while creating the expanded prompt (with the retrieved top-\\(k\\) chunks) for an LLM to make an informed response generation, a strategic placement of vital information at the beginning or end of input sequences could enhance the RAG system’s effectiveness and thus make the system more performant. This is summarized in the paper below.\n\n\n\n\n\nLost in the Middle: How Language Models Use Long Contexts\n\n\n\n  \nWhile recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context.\n\n  \nThis paper by Liu et al. from Percy Liang’s lab at Stanford, UC Berkeley, and Samaya AI analyzes language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. Put simply, they analyze and evaluate how LLMs use the context by identifying relevant information within it.\n\n  \nThey tested open-source (MPT-30B-Instruct, LongChat-13B) and closed-source (OpenAI’s GPT-3.5-Turbo and Anthropic’s Claude 1.3) models. They used multi-document question-answering where the context included multiple retrieved documents and one correct answer, whose position was shuffled around. Key-value pair retrieval was carried out to analyze if longer contexts impact performance.\n\n  \nThey find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. In other words, their findings basically suggest that Retrieval-Augmentation (RAG) performance suffers when the relevant information to answer a query is presented in the middle of the context window with strong biases towards the beginning and the end of it.\n\n  \nA summary of their learnings is as follows:\n    \n\n      \nBest performance when the relevant information is at the beginning.\n\n      \nPerformance decreases with an increase in context length.\n\n      \nToo many retrieved documents harm performance.\n\n      \nImproving the retrieval and prompt creation step with a ranking stage could potentially boost performance by up to 20%.\n\n      \nExtended-context models (GPT-3.5-Turbo vs. GPT-3.5-Turbo (16K)) are not better if the prompt fits the original context.\n\n    \n\n  \n\n  \nConsidering that RAG retrieves information from an external database – which most commonly contains longer texts that are split into chunks. Even with split chunks, context windows get pretty large very quickly, at least much larger than a “normal” question or instruction. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Their analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.\n\n  \n“There is no specific inductive bias in transformer-based LLM architectures that explains why the retrieval performance should be worse for text in the middle of the document. I suspect it is all because of the training data and how humans write: the most important information is usually in the beginning or the end (think paper Abstracts and Conclusion sections), and it’s then how LLMs parameterize the attention weights during training.” \n(source)\n\n  \nIn other words, human text artifacts are often constructed in a way where the beginning and the end of a long text matter the most which could be a potential explanation to the characteristics observed in this work.\n\n  \nYou can also model this with the lens of two popular cognitive biases that humans face (primacy and recency bias), as in the following figure \n(source)\n.\n\n\n\n\n\n\n\n\n\n  \nThe final conclusion is that combining retrieval with ranking (as in recommender systems) should yield the best performance in RAG for question answering.\n\n  \nThe following figure \n(source)\n shows an overview of the idea proposed in the paper: “LLMs are better at using info at beginning or end of input context”.\n\n\n\n\n\n\n\n\n\n  \nThe following figure from the paper illustrates the effect of changing the position of relevant information (document containing the answer) on multidocument question answering performance. Lower positions are closer to the start of the input context. Performance is generally highest when relevant information is positioned at the very start or very end of the context, and rapidly degrades when models must reason over information in the middle of their input context.\n\n\n\n\n\n\n\n\nThe “Needle in a Haystack” Test\n\n\n\n\n  \nTo understand the in-context retrieval ability of long-context LLMs over various parts of their prompt, a simple ‘needle in a haystack’ analysis could be conducted. This method involves embedding specific, targeted information (the ‘needle’) within a larger, more complex body of text (the ‘haystack’). The purpose is to test the LLM’s ability to identify and utilize this specific piece of information amidst a deluge of other data.\n\n  \nIn practical terms, the analysis could involve inserting a unique fact or data point into a lengthy, seemingly unrelated text. The LLM would then be tasked with tasks or queries that require it to recall or apply this embedded information. This setup mimics real-world situations where essential details are often buried within extensive content, and the ability to retrieve such details is crucial.\n\n  \nThe experiment could be structured to assess various aspects of the LLM’s performance. For instance, the placement of the ‘needle’ could be varied—early, middle, or late in the text—to see if the model’s retrieval ability changes based on information location. Additionally, the complexity of the surrounding ‘haystack’ can be modified to test the LLM’s performance under varying degrees of contextual difficulty. By analyzing how well the LLM performs in these scenarios, insights can be gained into its in-context retrieval capabilities and potential areas for improvement.\n\n  \nThis can be accomplished using the \nNeedle In A Haystack\n library. The following plot shows OpenAI’s GPT-4-128K’s (top) and (bottom) performance with varying context length.\n\n\n\n\n\n\n\n\n\n\n\n\n  \nThe following figure \n(source)\n shows Claude 2.1’s long context question answering errors based on the areas of the prompt context length. On an average, Claude 2.1 demonstrated a 30% reduction in incorrect answers compared to Claude 2.\n\n\n\n\n\n\n\n\n\n  \nHowever, in Anthropic’s \nLong context prompting for Claude 2.1\n blog, Anthropic noted that adding “Here is the most relevant sentence in the context:” to the start of Claude’s response raised the score from 27% to 98% on the original evaluation! The figure below from the blog shows that Claude 2.1’s performance when retrieving an individual sentence across its full 200K token context window. This experiment uses the aforementioned prompt technique to guide Claude in recalling the most relevant sentence.\n\n\n\n\n\n\n\n\nRAG in Multi-Turn Chatbots: Embedding Queries for Retrieval\n\n\n\n\n  \n\n    \nIn multi-turn chatbot environments, RAG must extend beyond addressing isolated, single-turn queries. Conversations are inherently dynamic—context accumulates, user objectives evolve, and intent may shift subtly across multiple interactions. This dynamic nature renders one design decision particularly critical: determining which input text should be embedded during the retrieval phase. This decision has a direct impact on both the relevance of the retrieved content and the overall quality of the generated response.\n\n  \n\n  \n\n    \nIn contrast to single-turn systems, where embedding the current user input may suffice, multi-turn RAG systems face a more fluid and complex challenge. Limiting retrieval inputs to only the most recent user message is computationally efficient but often insufficient for capturing the nuances of ongoing discourse. Incorporating recent conversational turns offers improved contextual grounding, while advanced techniques such as summarization and query rewriting can significantly enhance retrieval precision.\n\n  \n\n  \n\n    \nThere is no universally optimal approach—the most suitable strategy depends on factors such as the application’s specific requirements, available computational resources, and tolerance for system complexity. Nevertheless, the most robust implementations often adopt a layered methodology: integrating recent dialogue context, monitoring evolving user intent, and utilizing reformulated or enriched queries. This composite approach typically results in more accurate, contextually appropriate retrieval and, consequently, more coherent and effective responses.\n\n  \n\n  \n\n    \nThe following sections outlines the key strategies and considerations for query embedding in multi-turn RAG chatbot systems.\n\n  \n\n\n\n\n\nEmbedding the Latest User Turn Only\n\n\n\n\n  \nThe simplest approach is to embed just the latest user message. For example, if a user says, “What are the symptoms of Lyme disease?”, that exact sentence is passed to the retriever for embedding.\n\n  \nPros\n:\n    \n\n      \nFast and computationally cheap.\n\n      \nReduces the risk of embedding irrelevant or stale context.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nIgnores conversational context and prior turns, which may contain critical disambiguating details (e.g., “Is it common in dogs?” following a discussion about pets).\n\n    \n\n  \n\n\n\n\n\nEmbedding Concatenated Recent Turns (Truncated Dialogue History)\n\n\n\n\n  \nA more nuanced approach involves embedding the current user message along with a sliding window of recent turns (usually alternating user and assistant messages).\n\n  \nFor example:\n    \nUser: My dog has been acting strange lately.\nAssistant: Can you describe the symptoms?\nUser: He’s tired, limping, and has a fever. Could it be Lyme disease?\n\n    \n\n    \n\n      \nThe retriever input would include all or part of the above.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nPreserves immediate context that can significantly improve retrieval relevance.\n\n      \nEspecially useful for resolving pronouns and follow-up queries.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nCan dilute the focus of the query if too many irrelevant prior turns are included.\n\n      \nRisk of exceeding input length limits for embedding models.\n\n    \n\n  \n\n\n\n\n\nEmbedding a Condensed or Summarized History\n\n\n\n\n  \nIn this strategy, prior turns are summarized into a condensed form before concatenation with the current turn. This reduces token count while preserving key context.\n\n  \nCan be achieved using simple heuristics, hand-written rules, or another lightweight LLM summarization pass.\n\n  \nFor example:\n    \nCondensed history: The user is concerned about their dog's health, showing signs of fatigue and limping.\nCurrent query: Could it be Lyme disease?\n\n    \n\n    \n\n      \nEmbed the concatenated string: “The user is concerned… Could it be Lyme disease?”\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nRetains relevant prior context while minimizing noise.\n\n      \nHelps improve retrieval accuracy for ambiguous follow-up questions.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nRequires additional processing and potential summarization latency.\n\n      \nSummarization quality can affect retrieval quality.\n\n    \n\n  \n\n\n\n\n\nEmbedding Structured Dialogue State\n\n\n\n\n  \nThis approach formalizes the conversation history into a structured format (like intent, entities, or user goals), which is then appended to the latest query before embedding.\n\n  \nFor instance:\n    \n[Intent: Diagnose pet illness] [Entity: Dog] [Symptoms: fatigue, limping, fever]\nQuery: Could it be Lyme disease?\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nAllows precision targeting of relevant documents, especially in domain-specific applications.\n\n      \nSupports advanced reasoning by aligning with KBs or ontology-driven retrieval.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nRequires reliable NLU and state-tracking pipelines.\n\n      \nAdds system complexity.\n\n    \n\n  \n\n\n\n\n\nTask-Optimized Embedding via Query Reformulation\n\n\n\n\n  \nSome systems apply a query rewriting model that reformulates the latest turn into a fully self-contained question, suitable for retrieval.\n\n  \nFor example, turning “What about dogs?” into “What are the symptoms of Lyme disease in dogs?”\n\n  \nThese reformulated queries are then embedded for retrieval.\n\n  \nPros\n:\n    \n\n      \nEnsures clarity and focus in queries passed to the retriever.\n\n      \nSignificantly boosts retrieval performance in ambiguous or shorthand follow-ups.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nIntroduces dependency on a high-quality rewrite model.\n\n      \nRisk of introducing hallucination or incorrect reformulations.\n\n    \n\n  \n\n\n\n\n\nBest Practices and Considerations\n\n\n\n\n  \nWindow Size\n: Most systems use a sliding window of 1-3 previous turns depending on token limits and task specificity.\n\n  \nQuery Length vs. Clarity Tradeoff\n: Longer queries with more context may capture nuance but risk introducing noise. Condensed or reformulated queries can help mitigate this.\n\n  \nPersonalization\n: In some advanced setups, user profiles or long-term memory can be injected into the retrieval query, but this must be carefully curated to avoid privacy or relevance pitfalls.\n\n  \nSystem Goals\n: If the chatbot is task-oriented (e.g., booking travel), structured state may be best. If it is open-domain (e.g., a virtual assistant), concatenated dialogue or rewrite strategies tend to perform better.\n\n\n\n\n\nComponent-Wise Evaluation\n\n\n\n\n  \nComponent-wise evaluation in RAG systems for LLMs involves assessing individual components of the system separately. This approach typically examines the performance of the retrieval component, which fetches relevant information from a database or corpus, and the generation component, which synthesizes responses based on the retrieved data. By evaluating these components individually, researchers can identify specific areas for improvement in the overall RAG system, leading to more efficient and accurate information retrieval and response generation in LLMs.\n\n  \nWhile metrics such as Context Precision, Context Recall, and Context Relevance provide insights into the performance of the retrieval component of the RAG system, Groundedness, and Answer Relevance offer a view into the quality of the generation.\n\n  \nSpecifically,\n    \n\n      \nMetrics to evaluate retrieval:\n Context Relevance, Context Recall, and Context Precision, which collectively assess the relevance, completeness, and accuracy of the information retrieved in response to a user’s query. Context Precision focuses on the system’s ability to rank relevant items higher, Context Recall evaluates how well the system retrieves all relevant parts of the context, and Context Relevance measures the alignment of retrieved information with the user’s query. These metrics ensure the effectiveness of the retrieval system in providing the most relevant and complete context for generating accurate responses.\n\n      \nMetrics to evaluate generation:\n Faithfulness and Answer Relevance, which measure the factual consistency of the generated answer with the given context and its relevance to the original question, respectively. Faithfulness focuses on the factual accuracy of the answer, ensuring all claims made can be inferred from the given context. Answer Relevance assesses how well the answer addresses the original question, penalizing incomplete or redundant responses. These metrics ensure the generation component produces contextually appropriate and semantically relevant answers.\n\n    \n\n  \n\n  \nThe harmonic mean of these four aspects gives you the overall score (also called ragas score) which is a single measure of the performance of your RAG system across all the important aspects.\n\n  \nMost of the measurements do not require any labeled data, making it easier for users to run it without worrying about building a human-annotated test dataset first. In order to run ragas all you need is a few questions and if your using context_recall, a reference answer.\n\n  \nOverall, these metrics offer a comprehensive view of the RAG system’s retrieval performance, which can be implemented using libraries for evaluating RAG pipelines such as \nRagas\n or \nTruLens\n and offer detailed insights about your RAG pipeline’s performance, focusing on the contextual and factual alignment of retrieved and generated content in response to user queries. Specifically, \nRagas\n, offers metrics tailored for evaluating each component of your RAG pipeline in isolation. This approach complements the broader, system-level end-to-end evaluation of your system (which is detailed in \nEnd-to-End Evaluation\n), allowing for a deeper understanding of how well a RAG system performs in real-world scenarios where the intricacies of context and factual accuracy are paramount. The figure below \n(source)\n shows the metrics that Ragas offers which are tailored for evaluating each component (retrieval, generation) of your RAG pipeline in isolation.\n\n\n\n\n\n\n\n\n\n  \nThe image below \n(source)\n, shows the “triad” of metrics that can be used to evaluate RAG: Groundedness (also known as Faithfulness), Answer Relevance, and Context Relevance. Note that Context Precision and Context Recall are also important and were more recently introduced in a newer version of \nRagas\n.\n\n\n\n\n\n\n\n\nRetrieval Metrics\n\n\n\n\n  \nEvaluating the retrieval component of RAG in the context of LLMs involves assessing how effectively the system retrieves relevant information to support the generation of accurate and contextually appropriate responses.\n\n\n\n\n\nContext Precision\n\n\n\n\n  \n\n    \nDefinition\n: Context Precision is a metric used to assess the accuracy of ranking ground-truth relevant items from the context higher in the results. It measures whether all the relevant chunks of information appear at the top ranks when responding to a query. Ideally all the relevant chunks must appear at the top ranks. The metric is scored between 0 and 1 using the question, ground truth, and the contexts, with higher scores indicating better precision.\n\n  \n\n  \nEvaluation Approach\n: Context Precision is calculated using the following steps:\n    \n\n      \nFor each chunk in the retrieved context, determine if it is relevant or not relevant based on the ground truth for the given question.\n\n      \n\n        \nCompute Precision@k for each chunk in the context using the formula:\n\n\n\\[\\text{Precision@k} = \\frac{\\text{true positives@k}}{\\text{true positives@k} + \\text{false positives@k}}\\]\n      \n\n      \n\n        \nCalculate the Context Precision@k by averaging the Precision@k values for all relevant items in the top \\(K\\) results:\n\n\n\\[\\text{Context Precision@k} = \\frac{\\sum_{k=1}^K (\\text{Precision@k} \\times v_k)}{\\text{Total number of relevant items in the top } K \\text{ results}}\\]\n\n        \n\n          \nwhere \\(K\\) is the total number of chunks in contexts and \\(v_k \\in \\{0,1\\}\\) is the relevance indicator at rank \\(k\\).\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nExample\n \n(source)\n: Let’s consider an example of calculating context precision using a question and its corresponding ground truth.\n\n\n    \n\n      \nQuestion\n: Where is France and what is its capital?\n\n      \nGround Truth\n: France is in Western Europe, and its capital is Paris.\n\n      \nHigh Context Precision Example\n:\n        \n\n          \nContexts: \n[\"France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower\", \"The country is also renowned for its wines and sophisticated cuisine. Lascaux's ancient cave drawings, Lyon's Roman theater and the vast Palace of Versailles attest to its rich history.\"]\n\n        \n\n      \n\n      \nLow Context Precision Example\n:\n        \n\n          \nContexts: \n[\"The country is also renowned for its wines and sophisticated cuisine. Lascaux's ancient cave drawings, Lyon's Roman theater and\", \"France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower\"]\n\n        \n\n      \n\n      \nIn this example, the calculation of context precision involves identifying relevant chunks related to the question and their ranking in the contexts. For the low context precision example:\n        \n\n          \nPrecision@1\n = \\(\\frac{0}{1}\\) = 0\n\n          \nPrecision@2\n = \\(\\frac{1}{2}\\) = 0.5\n\n          \nContext Precision\n = \\(\\frac{(0 + 0.5)}{1}\\) = 0.5\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nContext Recall\n\n\n\n\n  \n\n    \nDefinition\n: Context Recall measures how well the retrieved context aligns with the annotated answer, treated as the ground truth. This metric is essential for assessing the accuracy of the retrieval system in identifying and ranking relevant information. It evaluate the performance of the retrieval system in identifying relevant information based on a sample query and its corresponding ground truth answer. The context recall score helps in understanding how much of the ground truth information is accurately retrieved from the context. The context recall score ranges from 0 to 1, with higher values indicating better performance.\n\n  \n\n  \n\n    \nEvaluation Approach\n: To estimate context recall, each sentence in the ground truth answer is analyzed to determine whether it can be attributed to the retrieved context. The ideal scenario is when all sentences in the ground truth answer are attributable to the retrieved context. The formula used for calculating context recall is:\n\n\n\\[\\text{Context Recall} = \\frac{\\mid \\text{GT sentences attributable to context} \\mid}{\\mid \\text{Total sentences in GT} \\mid}\\]\n  \n\n  \n\n    \nExample\n \n(source)\n:\n\n    \n\n      \nGround Truth Question: “Where is France and what is its capital?”\n\n      \n\n        \nGround Truth Answer: “France is in Western Europe and its capital is Paris.”\n\n      \n\n      \nHigh Context Recall Example\n:\n        \n\n          \nRetrieved Context: “France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre, and monuments like the Eiffel Tower.”\n\n        \n\n      \n\n      \nLow Context Recall Example\n:\n        \n\n          \nRetrieved Context: “France, in Western Europe, encompasses medieval cities, alpine villages, and Mediterranean beaches. The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater, and the vast Palace of Versailles attest to its rich history.”\n\n        \n\n      \n\n      \nCalculation\n:\n        \n\n          \nStep 1\n: Break the ground truth answer into individual statements:\n            \n\n              \nStatement 1: “France is in Western Europe.”\n\n              \nStatement 2: “Its capital is Paris.”\n\n            \n\n          \n\n          \nStep 2\n: Verify if each ground truth statement can be attributed to the retrieved context:\n            \n\n              \nStatement 1: Yes (in both high and low context recall examples)\n\n              \nStatement 2: No (in the low context recall example)\n\n            \n\n          \n\n          \n\n            \nStep 3\n: Calculate context recall using the formula:\n\n\n\\[\\text{Context Recall} = \\frac{1}{2} = 0.5 \\quad \\text{(for the low context recall example)}\\]\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nContext Relevance\n\n\n\n\n  \nDefinition\n:\n    \n\n      \n“Is the passage returned relevant for answering the given query?”\n\n      \nMeasures how well the context retrieved by the RAG system aligns with the user’s query. It specifically evaluates whether the retrieved information is relevant and appropriate for the given query, ensuring that only essential information is included to address the query effectively.\n\n    \n\n  \n\n  \nEvaluation Approach\n: Involves a two-step procedure: first, the identification of relevant sentences using semantic similarity measures to produce a relevance score for each sentence. Can be measured with smaller BERT-style models, embedding distances, or with LLMs. The approach involves estimating the value of context relevance by identifying sentences within the retrieved context that are directly relevant for answering the given question. This is followed by the quantification of overall context relevance, where the final score is calculated using the formula:\n\n\n\n\n\\[\\text {Context Relevance} = \\frac{\\text { Number of sentences that are relevant to the query within the retrieved context}}{\\text { Total number of sentences in retrieved context}}\\]\n\n\n\n  \nExamples\n:\n    \n\n      \nHigh context relevance example\n: For a question like “What is the capital of France?”, a highly relevant context would be “France, in Western Europe, encompasses medieval cities, alpine villages and Mediterranean beaches. Paris, its capital, is famed for its fashion houses, classical art museums including the Louvre and monuments like the Eiffel Tower.”\n\n      \nLow context relevance example\n: For the same question, a less relevant context would include additional, unrelated information such as “The country is also renowned for its wines and sophisticated cuisine. Lascaux’s ancient cave drawings, Lyon’s Roman theater and the vast Palace of Versailles attest to its rich history.”\n\n    \n\n  \n\n  \nThis metric ensures that the RAG system provides concise and directly related information, enhancing the efficiency and accuracy of the response given to a specific query.\n\n\n\n\n\nGeneration Metrics\n\n\n\n\n  \nEvaluating the generation component of RAG in the context of LLMs involves assessing the ability of the system to seamlessly integrate retrieved information into coherent, contextually relevant, and linguistically accurate responses, ensuring a harmonious blend of retrieved data and generative language skills. Put simply, these metrics collectively provide a nuanced and multidimensional approach to evaluating RAG systems, emphasizing not just the retrieval of information but its contextual relevance, factual accuracy, and semantic alignment with user queries.\n\n\n\n\n\nGroundedness (a.k.a. Faithfulness)\n\n\n\n\n  \nDefinition\n: Groundedness (also known as Faithfulness) evaluates the factual consistency of a generated answer against a given context. It is measured based on the alignment between the answer and the retrieved context, with scores ranging from 0 to 1. A higher score indicates better factual consistency.\n\n  \nEvaluation Approach\n:\n    \n\n      \nThe faithfulness of a generated answer is determined by checking if all the atomic (stand-alone) claims made in the answer can be inferred from the provided context. The process involves identifying a set of atomic claims from the answer and cross-referencing each claim with the context to confirm if it can be inferred. The faithfulness score is calculated using the formula:\n\n    \n\n\n\\[\\text{Faithfulness score} = \\frac{\\text{Number of claims in the generated answer that can be inferred from the given context}}{\\text{Total number of claims in the generated answer}}\\]\n  \n\n  \n\n    \nExample\n \n(source)\n: \n\nQuestion\n: Where and when was Einstein born?\n\nContext\n: Albert Einstein (born 14 March 1879) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.\n\n\n    \n\n      \nHigh faithfulness answer\n: Einstein was born in Germany on 14th March 1879.\n\n      \n\n        \nLow faithfulness answer\n: Einstein was born in Germany on 20th March 1879.\n\n      \n\n      \nFor the low faithfulness answer:\n        \n\n          \nStep 1\n: Break the generated answer into individual statements.\n            \n\n              \nStatement 1: “Einstein was born in Germany.”\n\n              \nStatement 2: “Einstein was born on 20th March 1879.”\n\n            \n\n          \n\n          \nStep 2\n: Verify if each statement can be inferred from the given context.\n            \n\n              \nStatement 1: Yes\n\n              \nStatement 2: No\n\n            \n\n          \n\n          \n\n            \nStep 3\n: Calculate the faithfulness score using the formula.\n\n\n\\[\\text{Faithfulness} = \\frac{1}{2} = 0.5\\]\n          \n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nAnswer Relevance\n\n\n\n\n  \nDefinition\n:\n    \n\n      \nThe Answer Relevance metric evaluates how closely the generated answer aligns with the given query/prompt. This assessment focuses on the pertinence of the response, penalizing answers that are incomplete or contain redundant information. Higher scores indicate better relevance. The overarching concept behind answer relevance is that if the answer correctly addresses the question, it is likely that the original question can be accurately reconstructed from the answer alone.\n\n      \nAnswer relevance is reference free metric. If you’re looking to compare ground truth answer with generated answer refer to \nAnswer Correctness\n.\n\n      \nThe image below \n(source)\n shows the output format of Answer Relevance.\n\n    \n\n\n    \n\n  \n\n  \nEvaluation Approach\n:\n    \n\n      \nAnswer Relevance is quantified by calculating the mean cosine similarity between the original question and a set of generated questions based on the provided answer. Specifically, the metric is defined as follows:\n\n    \n\n\n\\[\\begin{aligned}\n& \\text{Answer Relevance} = \\frac{1}{N} \\sum_{i=1}^N \\cos (E_{g_i}, E_o) \\\\\n& \\text{Answer Relevance} = \\frac{1}{N} \\sum_{i=1}^N \\frac{E_{g_i} \\cdot E_o}{\\left\\|E_{g_i}\\right\\|\\left\\|E_o\\right\\|}\n\\end{aligned}\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(E_{g_i}\\) is the embedding of the generated question \\(i\\).\n\n          \n\\(E_o\\) is the embedding of the original question.\n\n          \n\\(N\\) is the number of generated questions, typically set to 3 by default.\n\n        \n\n      \n\n      \nIt is important to note that although the score generally ranges from 0 to 1, it is not strictly limited to this range due to the cosine similarity measure, which can range from -1 to 1. This metric does not rely on a reference answer and is purely focused on the relevance of the generated answer to the original question. If comparing the ground truth answer with the generated answer is required, one should refer to the “answer correctness” metric.\n\n      \nAn answer is considered relevant if it directly and appropriately responds to the original question. This metric does not consider the factual accuracy of the answer but rather penalizes cases where the answer is incomplete or contains unnecessary details. The process involves prompting a Large Language Model (LLM) to generate appropriate questions based on the provided answer and then measuring the mean cosine similarity between these questions and the original question. The idea is that a highly relevant answer should allow the LLM to generate questions that closely align with the original question.\n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nQuestion\n: Where is France and what is its capital?\n\n      \nLow relevance answer\n: France is in Western Europe.\n\n      \nHigh relevance answer\n: France is in Western Europe and Paris is its capital.\n\n    \n\n  \n\n  \nCalculation Steps\n:\n    \n\n      \nStep 1\n: Generate \\(n\\) variants of the question from the provided answer using an LLM. For example:\n        \n\n          \nQuestion 1: “In which part of Europe is France located?”\n\n          \nQuestion 2: “What is the geographical location of France within Europe?”\n\n          \nQuestion 3: “Can you identify the region of Europe where France is situated?”\n\n        \n\n      \n\n      \nStep 2\n: Calculate the mean cosine similarity between these generated questions and the original question.\n\n    \n\n  \n\n\n\n\n\nAnswer Semantic Similarity\n\n\n\n\n  \nCategory\n: Answer Quality and Semantic Alignment\n\n  \nRequirement\n: Access to ground truth answers is necessary to evaluate the semantic similarity of generated responses accurately.\n\n  \nDefinition\n: Evaluates the degree of semantic similarity between the generated answer by the RAG system and the ground truth. This metric specifically assesses how closely the meaning of the generated answer mirrors that of the ground truth.\n\n  \nMeasurement Methods\n: This metric is measured using cross-encoder models designed to calculate the semantic similarity score. These models analyze the semantic content of both the generated answer and the ground truth.\n\n  \n\n    \nEvaluation Approach\n: The approach involves comparing the generated answer with the ground truth to determine the extent of semantic overlap. The semantic similarity is quantified on a scale from 0 to 1, where higher scores indicate a greater alignment between the generated answer and the ground truth. The formula for Answer Semantic Similarity is implicitly based on the evaluation of semantic overlap rather than a direct formula.\n\n  \n\n  \nBERTScore\n:\n    \n\n      \nUses contextual embeddings from pre-trained BERT models to match tokens in the candidate and reference text.\n\n      \nComputes precision, recall, and F1 scores by aligning embeddings based on cosine similarity, capturing nuanced semantic overlap.\n\n    \n\n  \n\n  \nMoverScore\n:\n    \n\n      \nExtends BERTScore by incorporating Earth Mover’s Distance (EMD) to assess the minimal semantic “effort” needed to transform one text into another.\n\n      \nLeverages both contextual embeddings and IDF weighting to emphasize important content over common filler words.\n\n    \n\n  \n\n  \nAdvantages of MoverScore over BERTScore\n:\n    \n\n      \nBetter captures the global semantic flow between texts by considering word importance and distribution, not just local alignment.\n\n      \nMore robust in handling paraphrased or reordered sentences, where BERTScore may undervalue semantic similarity due to token-level matching.\n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nGround truth\n: Albert Einstein’s theory of relativity revolutionized our understanding of the universe.\n\n      \nHigh similarity answer\n: Einstein’s groundbreaking theory of relativity transformed our comprehension of the cosmos.\n\n      \nLow similarity answer\n: Isaac Newton’s laws of motion greatly influenced classical physics.\n\n    \n\n  \n\n  \nIn this metric, a higher score reflects a better quality of the generated response in terms of its semantic closeness to the ground truth, indicating a more accurate and contextually relevant answer.\n\n\n\n\n\nBLEU Score\n\n\n\n\n  \nCategory\n: N-gram Precision-Based Evaluation\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate the BLEU score.\n\n  \nDefinition\n: BLEU (Bilingual Evaluation Understudy) is a metric that evaluates the quality of text by comparing a candidate translation to one or more reference translations. It measures the precision of n-grams in the candidate text that appear in the reference texts, with a brevity penalty to penalize overly short translations.\n\n  \nMeasurement Methods\n: BLEU calculates modified n-gram precision for n-grams up to a specified length (commonly 4). It also applies a brevity penalty to account for short candidate translations that might otherwise score artificially high.\n\n  \n\n    \nEvaluation Approach\n: The BLEU score is computed using the formula:\n\n\n\\[\\text{BLEU} = \\text{BP} \\times \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(\\text{BP}\\) is the brevity penalty.\n\n          \n\\(p_n\\) is the modified n-gram precision.\n\n          \n\\(w_n\\) is the weight for each n-gram (typically uniform).\n\n        \n\n      \n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nReference\n: The cat is on the mat.\n\n      \nCandidate\n: The cat is on mat.\n\n      \nUnigram Precision\n: 5 matches out of 5 words = 1.0\n\n      \nBigram Precision\n: 4 matches out of 4 bigrams = 1.0\n\n      \nTrigram Precision\n: 3 matches out of 3 trigrams = 1.0\n\n      \n4-gram Precision\n: 2 matches out of 2 four-grams = 1.0\n\n      \nBrevity Penalty\n: Applied due to shorter length.\n\n      \nBLEU Score\n: Calculated by combining precisions and brevity penalty.\n\n      \nIn this example, despite high n-gram precision, the brevity penalty reduces the BLEU score to account for the missing word “the” before “mat.”\n\n    \n\n  \n\n\n\n\n\nROUGE Score\n\n\n\n\n  \nCategory\n: Recall-Oriented N-gram Evaluation\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate the ROUGE score.\n\n  \nDefinition\n: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic summarization and machine translation by comparing the overlap of n-grams between the candidate and reference texts.\n\n  \nMeasurement Methods\n: Common variants include:\n    \n\n      \nROUGE-N\n: Measures overlap of n-grams.\n\n      \nROUGE-L\n: Measures the longest common subsequence.\n\n      \nROUGE-S\n: Measures skip-bigram overlap.\n\n    \n\n  \n\n  \n\n    \nEvaluation Approach\n: For ROUGE-N, the recall is calculated as:\n\n\n\\[\\text{ROUGE-N} = \\frac{\\text{Number of matching n-grams}}{\\text{Total number of n-grams in reference}}\\]\n  \n\n  \nExample\n:\n    \n\n      \nReference\n: “The cat is on the mat.”\n\n      \nCandidate\n: “The cat is on mat.”\n\n      \nROUGE-1 (Unigram) Recall\n: 5 matches out of 6 unigrams = 0.833\n\n      \nROUGE-2 (Bigram) Recall\n: 4 matches out of 5 bigrams = 0.8\n\n      \nIn this example, the candidate misses the unigram “the” before “mat,” affecting the recall scores.\n\n    \n\n  \n\n\n\n\n\nString Presence\n\n\n\n\n  \nCategory\n: Keyword or Phrase Matching\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate string presence.\n\n  \nDefinition\n: The String Presence metric checks if the generated response contains specific reference text, such as certain keywords or phrases. It is useful in scenarios where ensuring the inclusion of particular content is essential.\n\n  \nMeasurement Methods\n: This is a binary metric that returns 1 if the reference string is present in the response and 0 otherwise.\n\n  \nEvaluation Approach\n: The presence of the reference string is verified within the candidate response.\n\n  \nExample\n:\n    \n\n      \nReference\n: “climate change”\n\n      \nCandidate\n: The recent study highlights the impacts of climate change on polar bears.\n\n      \nString Presence Score\n: 1 (since “climate change” is present in the candidate).\n\n    \n\n  \n\n\n\n\n\nExact Match\n\n\n\n\n  \nCategory\n: Strict Matching Evaluation\n\n  \nRequirement\n: Access to ground truth references is necessary to evaluate exact matches.\n\n  \nDefinition\n: The Exact Match metric assesses whether the generated response is identical to the reference text. It is particularly useful in scenarios requiring precise outputs, such as predefined answers or specific commands.\n\n  \nMeasurement Methods\n: This binary metric returns 1 if the candidate text matches the reference text exactly and 0 otherwise.\n\n  \nEvaluation Approach\n: A direct comparison is made between the candidate and reference texts.\n\n  \nExample\n:\n    \n\n      \nReference\n: \\(E=mc^2\\)\n\n      \nCandidate\n: \\(E=mc^2\\)\n\n      \nExact Match Score\n: 1 (since the candidate matches the reference exactly).\n\n    \n\n  \n\n\n\n\n\nContext Entities Recall\n\n\n\n\n  \n\n    \nDefinition\n: Context Entities Recall is a metric that measures the recall of entities from the retrieved context compared to the ground truth. It calculates the fraction of entities in the ground truth that are also present in the context. This metric is crucial for scenarios where accurate entity retrieval is essential, such as tourism help desks or historical question answering.\n\n  \n\n  \nEvaluation Approach\n:\n    \n\n      \nTo compute this metric, two sets are used:\n        \n\n          \n\\(GE\\) (Ground Truth Entities): The set of entities present in the ground truth.\n\n          \n\\(CE\\) (Context Entities): The set of entities present in the retrieved context.\n\n        \n\n      \n\n      \n\n        \nThe Context Entities Recall is calculated using the formula:\n\n\n\\[\\text{Context Entity Recall} = \\frac{|CE \\cap GE|}{|GE|}\\]\n\n        \n\n          \nwhere, \\(\\mid CE \\cap GE\\mid\\) represents the number of entities common to both the context and the ground truth, while \\(\\mid GE\\mid\\) is the total number of entities in the ground truth.\n\n        \n\n      \n\n    \n\n  \n\n  \nExample\n \n(source)\n:\n    \n\n      \n\n        \nGround Truth\n: The Taj Mahal is an ivory-white marble mausoleum on the right bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1631 by the Mughal emperor Shah Jahan to house the tomb of his favorite wife, Mumtaz Mahal.\n\n      \n\n      \n\n        \nHigh Entity Recall Context\n: The Taj Mahal is a symbol of love and architectural marvel located in Agra, India. It was built by the Mughal emperor Shah Jahan in memory of his beloved wife, Mumtaz Mahal. The structure is renowned for its intricate marble work and beautiful gardens surrounding it.\n\n      \n\n      \n\n        \nLow Entity Recall Context\n: The Taj Mahal is an iconic monument in India. It is a UNESCO World Heritage Site and attracts millions of visitors annually. The intricate carvings and stunning architecture make it a must-visit destination.\n\n      \n\n      \nCalculation\n:\n        \n\n          \nEntities in Ground Truth (GE)\n: \n['Taj Mahal', 'Yamuna', 'Agra', '1631', 'Shah Jahan', 'Mumtaz Mahal']\n\n          \nEntities in High Entity Recall Context (CE1)\n: \n['Taj Mahal', 'Agra', 'Shah Jahan', 'Mumtaz Mahal', 'India']\n\n          \nEntities in Low Entity Recall Context (CE2)\n: \n['Taj Mahal', 'UNESCO', 'India']\n\n        \n\n      \n\n      \n\n        \nContext Entity Recall - 1\n:\n\n\n\\[\\text{Context Entity Recall - 1} = \\frac{|CE1 \\cap GE|}{|GE|} = \\frac{4}{6} = 0.666\\]\n      \n\n      \n\n        \nContext Entity Recall - 2\n:\n\n\n\\[\\text{Context Entity Recall - 2} = \\frac{|CE2 \\cap GE|}{|GE|} = \\frac{1}{6} = 0.166\\]\n      \n\n      \nThe first context demonstrates a higher entity recall, indicating better entity coverage in comparison to the ground truth. If these contexts were generated by different retrieval mechanisms, the first mechanism would be deemed superior for applications where entity accuracy is crucial.\n\n    \n\n  \n\n\n\n\n\n\n\n\nMultimodal Input Handling\n\n\n\n\n  \nRAG traditionally focuses on textual inputs. However, real-world scenarios frequently involve multimodal inputs, particularly text combined with images. Consider queries such as “What brand are the shoes in this image?”, “Describe the issue shown in the screenshot and suggest how to fix it”, and “Provide nutritional details for the meal shown here.” Addressing these queries requires handling both text and visual elements simultaneously.\n\n  \nIntegrating multimodal embeddings in RAG systems enables robust and precise handling of queries containing both visual and textual elements, significantly enhancing retrieval accuracy and the overall quality of generated responses.\n\n\n\n\n\nFlow of Multimodal Input\n\n\n\n\n  \nQuery Input\n:\n    \n\n      \nThe user submits a query comprising text and an image. For example, a user might upload a picture of a jacket alongside the text query, “Is this jacket available in waterproof material?”\n\n    \n\n  \n\n  \nEmbedding Multimodal Input\n:\n    \n\n      \nBoth text and image inputs need to be converted into embeddings to capture their semantic essence. This typically involves:\n        \n\n          \nText Embedding\n: Utilizing models like Sentence-BERT or GPT embeddings to create dense vectors representing the semantic meaning of the textual query.\n\n          \nImage Embedding\n: Using visual embedding models such as CLIP (Contrastive Language-Image Pre-training), ViT (Vision Transformer), or ResNet variants. These models process images to create dense vector representations capturing visual features.\n\n        \n\n      \n\n      \nThe resulting embeddings are then concatenated or fused into a single multimodal embedding vector. This fusion captures both the textual semantics and the visual features coherently.\n\n    \n\n  \n\n  \nStorage and Retrieval from Vector Database\n:\n    \n\n      \nThe multimodal embeddings are stored in a vector database, similar to text-only scenarios.\n\n      \nDuring retrieval, multimodal embeddings derived from user queries are compared against the stored embeddings in the database.\n\n    \n\n  \n\n  \nSimilarity Matching via Cosine Similarity\n:\n    \n\n      \nRetrieval involves computing cosine similarity between the multimodal query embedding and the embeddings stored in the vector database.\n\n      \nCosine similarity effectively measures semantic and visual similarity, ensuring retrieved items closely align with both textual context and visual content of the query.\n\n    \n\n  \n\n  \nRanked Results and Response Generation\n:\n    \n\n      \nItems with the highest similarity scores – provide specific product details (e.g., material information, waterproof ratings) – are retrieved and ranked according to relevance.\n\n      \nThese ranked results are then fed into an LLM to synthesize contextually accurate and visually informed responses. The final response leverages the multimodal context to precisely answer queries such as material specifications or availability in different sizes or colors, with a coherent respons such ase: “Yes, this particular jacket model is made from Gore-Tex, which is fully waterproof.”\n\n    \n\n  \n\n\n\n\n\nBenefits of Multimodal Embeddings in RAG\n\n\n\n\n  \nEnhanced User Experience\n: Allows users to naturally query using images, which often conveys more information than text alone.\n\n  \nPrecision and Relevance\n: Combining textual semantics and visual features significantly enhances retrieval accuracy.\n\n  \nScalable Solution\n: Multimodal embeddings can seamlessly integrate with existing vector databases, offering scalability and performance optimization.\n\n\n\n\n\nMultimodal RAG\n\n\n\n\n  \nMany documents contain a mixture of content types, including text and images. Yet, information captured in images is lost in most RAG applications. With the emergence of multimodal LLMs, like GPT-4V, it is worth considering how to utilize images in RAG.\n\n  \nHere are three ways to use images in RAG:\n    \n\n      \nOption 1:\n\n        \n\n          \nUse multimodal embeddings (such as CLIP) to embed images and text.\n\n          \nRetrieve both using similarity search.\n\n          \nPass raw images and text chunks to a multimodal LLM for answer synthesis.\n\n        \n\n      \n\n      \nOption 2:\n\n        \n\n          \nUse a multimodal LLM (such as GPT-4V, LLaVA, or Fuyu-8b) to produce text summaries from images.\n\n          \nEmbed and retrieve text.\n\n          \nPass text chunks to an LLM for answer synthesis.\n\n        \n\n      \n\n      \nOption 3:\n\n        \n\n          \nUse a multimodal LLM (such as GPT-4V, LLaVA, or Fuyu-8b) to produce text summaries from images.\n\n          \nEmbed and retrieve image summaries with a reference to the raw image. You can use a \nmulti-vector retriever\n with a Vector DB such as \nChroma\n to store raw text and images along with their summaries for retrieval.\n\n          \nPass raw images and text chunks to a multimodal LLM for answer synthesis.\n\n        \n\n      \n\n    \n\n  \n\n  \nOption 2 is appropriate for cases when a multi-modal LLM cannot be used for answer synthesis (e.g., cost, etc).\n\n  \nThe following figure \n(source)\n offers an overview of all three aforementioned options.\n\n\n\n\n\n\n\n\n\n  \nLangChain offers cookbooks for \nOption 1\n and \nOption 3\n.\n\n  \nThe following infographic \n(source)\n also offers a top-level overview of Multimodal RAG:\n\n\n\n\n\n\n\n\nAgentic Retrieval-Augmented Generation\n\n\n\n\n  \n\n    \nAgent-based Retrieval-Augmented Generation (RAG), or Agentic RAG, represents an advanced approach in AI that enhances the traditional RAG pipeline with intelligent agents. In conventional RAG systems, an AI model queries a knowledge base to retrieve relevant information and generate responses. However, Agentic RAG extends beyond this by employing AI agents capable of orchestrating multi-step retrieval processes, utilizing external tools, and dynamically adapting to the query. This added layer of autonomy enables advanced reasoning, decision-making, and adaptability, allowing the system to handle complex queries and diverse data sources with greater precision and responsiveness.\n\n  \n\n  \n\n    \nBy integrating AI agents, Agentic RAG transforms traditional RAG, providing a flexible, intelligent solution for nuanced, real-world inquiries. This shift enables organizations to deploy AI systems with a higher degree of accuracy, flexibility, and intelligence, allowing them to tackle intricate tasks and deliver more precise results across a wide range of applications.\n\n  \n\n\n\n\n\nHow Agentic RAG Works\n\n\n\n\n  \n\n    \nIn an agentic RAG system, AI agents play key roles in the retrieval process, using specialized tools to retrieve context-sensitive information. Unlike traditional RAG, where retrieval functions are static, agentic RAG allows dynamic selection and operation of tools based on query requirements. Retrieval agents may utilize tools such as:\n\n\n    \n\n      \nVector Search Engines\n: Retrieve information from vectorized data in databases.\n\n      \nWeb Search Tools\n: Access live web data for up-to-date, contextually relevant information.\n\n      \nCalculators\n: Perform computations for queries that require accurate calculation.\n\n      \nAPIs for Software Programs\n: Programmatically retrieve information from applications like email or chat programs to access user-specific data.\n\n    \n\n  \n\n  \n\n    \nIn the context of Agentic RAG, the retrieval process is “agentic,” meaning agents are capable of reasoning and decision-making regarding which sources and tools to use, based on the specific requirements of the query. This flexibility elevates their tool usage beyond simple retrieval, allowing for a more dynamic and adaptive response.\n\n  \n\n\n\n\n\nAgentic Decision-Making in Retrieval\n\n\n\n\n  \n\n    \nThe decision-making process of retrieval agents encompasses several key actions, including:\n\n\n    \n\n      \nDeciding Whether to Retrieve\n: Assessing if additional information is necessary for the query.\n\n      \nChoosing the Appropriate Tool\n: Selecting the most suitable tool (e.g., a vector search engine or web search) based on the query.\n\n      \nQuery Formulation\n: Refining or rephrasing the query to enhance retrieval accuracy.\n\n      \nEvaluating Retrieved Results\n: Reviewing the retrieved information to determine sufficiency, and whether further retrieval is needed.\n\n    \n\n  \n\n\n\n\n\nAgentic RAG Architectures: Single-Agent vs. Multi-Agent Systems\n\n\n\n\n  \nAgentic RAG can be implemented with a single agent or multiple agents, each offering unique strengths.\n\n\n\n\n\nSingle-Agent RAG (Router)\n\n\n\n\n  \nThe simplest implementation of agentic RAG involves a single agent functioning as a “router.” This agent determines the appropriate source or tool for retrieving information based on the query. The single agent toggles between different options, such as a vector database, web search, or an API. This setup provides a versatile retrieval process, enabling access to multiple data sources beyond a single vector search tool.\n\n  \nAs shown in the figure below (\nsource\n), the single-agent RAG system (router) architecture involves a single agent serving as a “router,” dynamically selecting the best tool or source based on the query, enabling efficient information retrieval across multiple data channels.\n\n\n\n\n\n\n\n\nMulti-Agent RAG Systems\n\n\n\n\n  \n\n    \nFor more complex queries, multi-agent RAG systems provide additional flexibility. These systems feature a “master agent” that coordinates several specialized retrieval agents, such as:\n\n\n    \n\n      \nInternal Data Retrieval Agent\n: Retrieves information from proprietary, internal databases.\n\n      \nPersonal Data Retrieval Agent\n: Accesses user-specific information, such as emails or chat history.\n\n      \nPublic Data Retrieval Agent\n: Conducts web searches for up-to-date public information.\n\n    \n\n  \n\n  \n\n    \nBy utilizing multiple agents tailored to specific sources or tasks, multi-agent RAG systems can deliver comprehensive, accurate responses across diverse channels.\n\n  \n\n  \n\n    \nAs shown in the figure below (\nsource\n), the multi-agent RAG system architecture utilizes multiple specialized retrieval agents to access different sources and tools, offering a flexible and comprehensive approach to complex queries.\n\n  \n\n\n\n\n\n\n\n\nBeyond Retrieval: Expanding Agentic RAG’s Capabilities\n\n\n\n\n  \n\n    \nAgentic RAG systems can incorporate agents for tasks beyond retrieval, including:\n\n\n    \n\n      \nValidating Information\n: Cross-referencing data across sources to ensure accuracy.\n\n      \nPerforming Multi-step Reasoning\n: Following logical steps to address complex queries before generating responses.\n\n      \nUpdating System Memory\n: Tracking and retaining user-specific preferences or past queries, enabling personalized and context-aware responses.\n\n    \n\n  \n\n  \n\n    \nBy expanding its capabilities beyond simple retrieval, Agentic RAG delivers a powerful, context-sensitive AI solution capable of handling intricate, real-world applications.\n\n  \n\n\n\n\n\nAgentic RAG vs. Vanilla RAG: Key Differences\n\n\n\n\n  \nWhile both vanilla and agentic RAG systems aim to retrieve information and generate responses, agentic RAG introduces several significant enhancements:\n\n\n\n\n\n\n\n\n \n\n\n\n\nFeature\n\n\nVanilla RAG\n\n\nAgentic RAG\n\n\n\n\n\n\n\n\n\n\nAccess to External Tools\n\n\nNo\n\n\nYes – Utilizes external tools like vector search engines, web search tools, calculators, and APIs.\n\n\n\n\n\n\nQuery Pre-processing\n\n\nNo\n\n\nYes – Agents dynamically refine, rephrase, and adapt queries for optimized retrieval.\n\n\n\n\n\n\nDecision-making in Retrieval\n\n\nLimited to direct retrieval from knowledge base\n\n\nAgents autonomously decide if retrieval is needed, select tools, and adapt based on query complexity and source type.\n\n\n\n\n\n\nMulti-step Retrieval Process\n\n\nNo\n\n\nYes – Agents perform multi-step, adaptive retrieval processes involving various sources or tool combinations.\n\n\n\n\n\n\nData Validation\n\n\nNo\n\n\nYes – Information is cross-referenced across sources to validate accuracy, supporting complex, real-world responses.\n\n\n\n\n\n\nDynamic Tool Selection\n\n\nStatic retrieval tools only\n\n\nDynamic – Agents choose specific tools (e.g., vector search, APIs) based on query needs.\n\n\n\n\n\n\nAdaptability to Query\n\n\nLimited\n\n\nHighly adaptive – Agents select and operate tools based on real-time assessment of query requirements.\n\n\n\n\n\n\nTypes of Agents\n\n\nNot applicable\n\n\nMultiple specialized agents, such as internal data retrieval, personal data retrieval, public data retrieval.\n\n\n\n\n\n\nSingle-Agent vs. Multi-Agent System\n\n\nNot applicable\n\n\nSingle-agent router or multi-agent systems, with “master” and specialized agents for complex queries.\n\n\n\n\n\n\nReasoning and Logic Capability\n\n\nNo\n\n\nYes – Supports multi-step reasoning, allowing logical sequence handling before generating responses.\n\n\n\n\n\n\nMemory and Personalization\n\n\nLimited to immediate query\n\n\nYes – Capable of updating memory to retain user preferences or history, allowing personalized responses.\n\n\n\n\n\n\nReal-world Applications\n\n\nPrimarily static responses from a fixed database\n\n\nSupports a wide range of real-world applications by responding to complex, nuanced inquiries with context sensitivity.\n\n\n\n\n\n\n\n\n\n\n\n\n  \nDrawing a parallel with problem-solving, agentic RAG offers capabilities akin to having a smartphone in hand—equipped with multiple apps and tools to help answer a question—whereas vanilla RAG is akin to being in a library with limited resources.\n\n\n\n\n\nImplementing Agentic RAG: Key Approaches\n\n\n\n\n  \nTo implement agentic RAG, developers can use either language models with function calling or agent frameworks, each providing specific advantages in terms of flexibility and control.\n\n  \nBoth methods—function calling in language models and agent frameworks—enable agentic RAG, though each has unique benefits:\n    \n\n      \nFunction Calling\n provides control over each tool interaction, suitable for cases with specific tool chains or simple agent setups.\n\n      \nAgent Frameworks\n offer pre-built integrations and routing logic, ideal for larger, multi-agent architectures.\n\n    \n\n  \n\n  \nUsing these implementations, developers can build flexible and adaptive agentic RAG pipelines, enhancing retrieval, reasoning, and response generation capabilities for AI-driven applications.\n\n\n\n\n\nLanguage Models with Function Calling\n\n\n\n\n  \nFunction calling allows language models to interact directly with external tools. For example, OpenAI’s function calling for GPT-4 or Cohere’s connectors API lets developers connect language models to databases, calculators, and other services. This interaction involves defining a function (such as querying a database), passing it to the model via a schema, and routing the model’s queries through the defined functions. This approach enables the model to leverage specific tools as needed, based on the query.\n\n\n\n\n\nAgent Frameworks\n\n\n\n\n  \nSeveral agent frameworks—such as LangChain, LlamaIndex, CrewAI—simplify agentic RAG implementation by providing pre-built templates and tool integrations. Key features include:\n    \n\n      \nLangChain\n: Offers support for language model tools, and its LCEL and LangGraph frameworks integrate these tools seamlessly.\n\n      \nLlamaIndex\n: Provides a QueryEngineTool to streamline retrieval tasks.\n\n      \nCrewAI\n: A leading framework for multi-agent setups, which supports shared tool access among agents.\n\n    \n\n  \n\n\n\n\n\nEnterprise-driven Adoption\n\n\n\n\n  \nOrganizations are increasingly transitioning to agentic RAG to gain more autonomous and accurate AI-driven systems. Enterprises such as Microsoft and Replit have introduced agents to enhance task completion and software development assistance. With agentic RAG, companies can build AI applications capable of handling diverse, real-time data sources, providing robust and adaptable responses for complex queries and tasks.\n\n\n\n\n\nBenefits\n\n\n\n\n  \nThe primary benefits of agentic RAG include:\n    \n\n      \nEnhanced Retrieval Accuracy\n: By routing queries through specialized agents, agentic RAG can provide more accurate responses.\n\n      \nAutonomous Task Performance\n: Agents can perform multi-step reasoning, independently solving complex problems.\n\n      \nImproved Collaboration\n: These systems can better assist users by handling more varied and personalized queries.\n\n    \n\n  \n\n\n\n\n\nLimitations\n\n\n\n\n  \nAgentic RAG does present challenges, such as:\n    \n\n      \nIncreased Latency\n: Running multiple agents and interacting with tools can add delays to the response.\n\n      \nReliability of Agents\n: Depending on the LLM’s reasoning capabilities, agents may fail to complete certain tasks accurately.\n\n      \nComplexity in Error Handling\n: Systems need robust fallback mechanisms to recover if an agent fails to retrieve or process data.\n\n    \n\n  \n\n\n\n\n\nCode\n\n\n\n\n  \nImplementing agentic RAG requires setting up an agent framework capable of handling tool integrations and coordinating retrieval processes. This section walks through an example code setup, demonstrating both language models with function calling and agent frameworks for building an agentic RAG pipeline.\n\n\n\n\n\nImplementing Agentic RAG with Function Calling\n\n\n\n\n  \n\n    \nFunction calling in language models allows them to interact with tools by defining functions that retrieve data from external sources. This method leverages API calls, database queries, and computation tools to enrich the response with dynamic data.\n\n  \n\n  \n\n    \nHere’s an example implementation using a function for retrieval from a database via the Weaviate vector search API.\n\n  \n\n\n\n\n\nDefine the Function for Retrieval\n\n\n\n\n  \nTo start, we define a function that uses Weaviate’s hybrid search to query a database and retrieve relevant results.\n\n\n\n\n\ndef\n \nget_search_results\n(\nquery\n:\n \nstr\n)\n \n->\n \nstr\n:\n\n    \n\"\"\"Sends a query to Weaviate's Hybrid Search. Parses the response into a formatted string.\"\"\"\n\n\n    \nresponse\n \n=\n \nblogs\n.\nquery\n.\nhybrid\n(\nquery\n,\n \nlimit\n=\n5\n)\n  \n# Retrieve top 5 results based on the query\n\n    \nstringified_response\n \n=\n \n\"\"\n\n    \nfor\n \nidx\n,\n \no\n \nin\n \nenumerate\n(\nresponse\n.\nobjects\n):\n\n        \nstringified_response\n \n+=\n \nf\n\"Search Result \n{\nidx\n+\n1\n}\n:\n\\n\n\"\n\n        \nfor\n \nprop\n \nin\n \no\n.\nproperties\n:\n\n            \nstringified_response\n \n+=\n \nf\n\"\n{\nprop\n}\n: \n{\no\n.\nproperties\n[\nprop\n]\n}\n\\n\n\"\n\n        \nstringified_response\n \n+=\n \n\"\n\\n\n\"\n\n\n    \nreturn\n \nstringified_response\n\n\n\n\n\nDefine the Tools Schema\n\n\n\n\n  \nNext, we define a tools schema that connects the function to the language model. This schema tells the model how to use the function for retrieving data.\n\n\n\n\n\ntools_schema\n \n=\n \n[{\n\n    \n'type'\n:\n \n'function'\n,\n\n    \n'function'\n:\n \n{\n\n        \n'name'\n:\n \n'get_search_results'\n,\n\n        \n'description'\n:\n \n'Get search results for a provided query.'\n,\n\n        \n'parameters'\n:\n \n{\n\n          \n'type'\n:\n \n'object'\n,\n\n          \n'properties'\n:\n \n{\n\n            \n'query'\n:\n \n{\n\n              \n'type'\n:\n \n'string'\n,\n\n              \n'description'\n:\n \n'The search query.'\n,\n\n            \n},\n\n          \n},\n\n          \n'required'\n:\n \n[\n'query'\n],\n\n        \n},\n\n    \n},\n\n\n}]\n\n\n\n\n\nSetting Up the Interaction Loop\n\n\n\n\n  \nTo ensure the model can call the tool multiple times (if needed), we set up a loop that enables the model to interact with tools and retrieve data iteratively until it has all necessary information.\n\n\n\n\n\ndef\n \nollama_generation_with_tools\n(\nuser_message\n:\n \nstr\n,\n \ntools_schema\n:\n \nlist\n,\n \ntool_mapping\n:\n \ndict\n,\n \nmodel_name\n:\n \nstr\n \n=\n \n\"llama3.1\"\n)\n \n->\n \nstr\n:\n\n    \nmessages\n \n=\n \n[{\n\"role\"\n:\n \n\"user\"\n,\n \n\"content\"\n:\n \nuser_message\n}]\n\n    \nresponse\n \n=\n \nollama\n.\nchat\n(\nmodel\n=\nmodel_name\n,\n \nmessages\n=\nmessages\n,\n \ntools\n=\ntools_schema\n)\n\n    \n    \n# Check if the model needs to use a tool\n\n    \nif\n \nnot\n \nresponse\n[\n\"message\"\n].\nget\n(\n\"tool_calls\"\n):\n\n        \nreturn\n \nresponse\n[\n\"message\"\n][\n\"content\"\n]\n\n    \n    \n# Handle tool calls and retrieve information\n\n    \nfor\n \ntool\n \nin\n \nresponse\n[\n\"message\"\n][\n\"tool_calls\"\n]:\n\n        \nfunction_to_call\n \n=\n \ntool_mapping\n[\ntool\n[\n\"function\"\n][\n\"name\"\n]]\n\n        \nfunction_response\n \n=\n \nfunction_to_call\n(\ntool\n[\n\"function\"\n][\n\"arguments\"\n][\n\"query\"\n])\n\n        \nmessages\n.\nappend\n({\n\"role\"\n:\n \n\"tool\"\n,\n \n\"content\"\n:\n \nfunction_response\n})\n\n    \n    \n# Generate final response after tool calls\n\n    \nfinal_response\n \n=\n \nollama\n.\nchat\n(\nmodel\n=\nmodel_name\n,\n \nmessages\n=\nmessages\n)\n\n    \nreturn\n \nfinal_response\n[\n\"message\"\n][\n\"content\"\n]\n\n\n\n\n\nExecuting the Agentic RAG Query\n\n\n\n\n  \nFinally, we run the function, allowing the language model to interact with the \nget_search_results\n tool.\n\n\n\n\n\ntool_mapping\n \n=\n \n{\n\"get_search_results\"\n:\n \nget_search_results\n}\n  \n# Maps tool name to function\n\nresponse\n \n=\n \nollama_generation_with_tools\n(\n\n    \n\"How is HNSW different from DiskANN?\"\n,\n\n    \ntools_schema\n=\ntools_schema\n,\n\n    \ntool_mapping\n=\ntool_mapping\n\n\n)\n\n\nprint\n(\nresponse\n)\n\n\n\n\n\n\n  \nThis setup enables the language model to retrieve dynamic information and perform tool-based retrievals as needed.\n\n\n\n\n\nImplementing Agentic RAG with Agent Frameworks\n\n\n\n\n  \nUsing agent frameworks streamlines the implementation process by providing templates and pre-built modules for multi-agent orchestration. Here’s how to set up an agentic RAG pipeline using LangChain as an example.\n\n\n\n\n\nStep 1: Define Agents and Tools\n\n\n\n\n  \nLangChain simplifies agentic RAG by managing tools and routing tasks. First, define the agents and register the tools they will use.\n\n\n\n\n\nfrom\n \nlangchain.tools\n \nimport\n \nWebSearchTool\n,\n \nDatabaseTool\n,\n \nCalculatorTool\n\n\nfrom\n \nlangchain.agents\n \nimport\n \nAgent\n\n\n\n# Define tools for retrieval\n\nweb_search_tool\n \n=\n \nWebSearchTool\n(\napi_key\n=\n\"YOUR_WEB_SEARCH_API_KEY\"\n)\n\n\ndatabase_tool\n \n=\n \nDatabaseTool\n(\ndb_client\n=\n\"your_database_client\"\n)\n\n\ncalculator_tool\n \n=\n \nCalculatorTool\n()\n\n\n\n# Set up an agent with a routing function\n\nretrieval_agent\n \n=\n \nAgent\n(\n\n    \ntools\n=\n[\nweb_search_tool\n,\n \ndatabase_tool\n,\n \ncalculator_tool\n],\n\n    \nrouting_function\n=\n\"retrieve_and_select_tool\"\n\n\n)\n\n\n\n\n\nStep 2: Configure Agent Routing\n\n\n\n\n  \nSet up the routing function to let the agent decide which tool to use based on the input query.\n\n\n\n\n\ndef\n \nretrieve_and_select_tool\n(\nquery\n):\n\n    \nif\n \n\"calculate\"\n \nin\n \nquery\n:\n\n        \nreturn\n \ncalculator_tool\n\n    \nelif\n \n\"web\"\n \nin\n \nquery\n:\n\n        \nreturn\n \nweb_search_tool\n\n    \nelse\n:\n\n        \nreturn\n \ndatabase_tool\n\n\n\n\n\nStep 3: Chain Agents for Multi-Agent RAG\n\n\n\n\n  \nIn multi-agent RAG, you might have a “master agent” that routes queries to specialized agents based on query type. Here’s how to set up a master agent to coordinate multiple agents.\n\n\n\n\n\nfrom\n \nlangchain.agents\n \nimport\n \nMultiAgent\n\n\n\n# Define specialized agents\n\ninternal_agent\n \n=\n \nAgent\n(\ntools\n=\n[\ndatabase_tool\n],\n \nrouting_function\n=\n\"database_retrieval\"\n)\n\n\npublic_agent\n \n=\n \nAgent\n(\ntools\n=\n[\nweb_search_tool\n],\n \nrouting_function\n=\n\"web_retrieval\"\n)\n\n\n\n# Create a master agent to coordinate retrieval\n\nmaster_agent\n \n=\n \nMultiAgent\n(\nagents\n=\n[\ninternal_agent\n,\n \npublic_agent\n])\n\n\n\n# Function to handle a query using master agent\n\ndef\n \nhandle_query_with_master_agent\n(\nquery\n):\n\n    \nreturn\n \nmaster_agent\n.\nhandle_query\n(\nquery\n)\n\n\n\n\n\nRunning the Multi-Agent Query\n\n\n\n\n  \nFinally, to test the system, input a query and let the master agent route it appropriately:\n\n\n\n\n\nresponse\n \n=\n \nhandle_query_with_master_agent\n(\n\"Find recent studies on neural networks\"\n)\n\n\nprint\n(\nresponse\n)\n\n\n\n\n\nDisadvantages of Agentic RAG\n\n\n\n\n  \n\n    \nDespite its advantages, agentic RAG comes with several limitations that should be carefully considered, particularly for time-sensitive applications:\n\n\n    \n\n      \n\n        \nIncreased Latency\n: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.\n\n      \n\n      \n\n        \nHigher Computational Cost\n: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.\n\n      \n\n      \n\n        \nProduction Feasibility\n: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.\n\n      \n\n    \n\n  \n\n  \n\n    \nWhile these drawbacks limit agentic RAG’s use in certain scenarios, its capability to generate high-quality, well-researched responses can make it worthwhile in contexts where response time is less critical and information accuracy is paramount.\n\n  \n\n\n\n\n\nSummary\n\n\n\n\n  \nAgentic RAG refers to an agent-based implementation of RAG. AI agents are entities tasked with accomplishing specific objectives. These agents are often equipped with memory and tools, which they can utilize to carry out their tasks effectively. Among these tools, one significant capability is the ability to retrieve information from various sources, such as web searches or internal documents.\n\n  \nIn the context of agentic RAG, the “retrieval becomes agentic.” This implies that the AI agent is capable of reasoning and making decisions regarding which sources are most appropriate for retrieving the required information. The agent’s tool usage evolves beyond simple information retrieval, becoming more flexible and dynamic.\n\n  \nThe distinction between standard and agentic RAG can be summarized as follows:\n    \n\n      \nCommon RAG\n: The user input prompts a single call to a database, retrieving additional information in response to the query.\n\n      \nAgentic RAG\n: The agent is able to deliberate on which source is the most suitable for retrieving information based on the query, providing a more sophisticated and adaptable approach.\n\n    \n\n  \n\n  \nThe following figure (\nsource\n) offers a visual summary of Agentic RAG:\n\n\n\n\n\n\n\n\nRAG vs. Long Context Windows\n\n\n\nComputational Cost\n\n\n\n\n  \nProcessing extremely long contexts incurs substantial computational overhead. For instance, utilizing a 10 million token context window with state-of-the-art models like Llama 4 demands considerable hardware resources—approximately 32 H100 GPUs—which translates to over $100 per hour in inference costs. The key-value (KV) cache alone can exceed 1 terabyte of VRAM. These requirements pose a significant barrier to the practical deployment of long-context inference systems at scale, especially for organizations with limited infrastructure budgets.\n\n\n\n\n\nInference Latency and Throughput\n\n\n\n\n  \nAs the number of tokens increases, the latency of inference rises proportionally, often leading to a considerable decline in throughput. Even when hardware resources are available, this degradation in response time can negatively impact user experience in latency-sensitive applications such as virtual assistants, search engines, or real-time analytics systems.\n\n\n\n\n\nContextual Comprehension and Model Training Limitations\n\n\n\n\n  \nAlthough large context windows are theoretically capable of accommodating vast amounts of input data, current LLMs are typically trained on much smaller maximum context lengths—commonly up to 128,000 tokens. Consequently, performance across the full extent of a 10 million token context window is unproven and likely suboptimal. Empirical studies suggest that retrieval accuracy tends to diminish for information placed in the middle of a long context due to a phenomenon informally referred to as the “Lost in the Middle” effect. Therefore, while long-context architectures offer the promise of expanded capacity, their practical utility is constrained by training regimes and architectural bottlenecks.\n\n\n\n\n\nRAG as a Targeted, Cost-Efficient Solution\n\n\n\n\n  \nIn contrast, RAG provides a principled and efficient mechanism for narrowing down relevant content from a large corpus before conditioning the model’s generative process. By introducing a retrieval stage that identifies and ranks the most pertinent information, RAG minimizes unnecessary context, optimizes for response accuracy, and reduces memory and compute demands. This retrieval-first approach allows RAG systems to operate effectively within the token limitations of current LLMs, while maintaining scalability and affordability.\n\n\n\n\n\nImproving RAG Systems\n\n\n\n\n  \nTo enhance and refine RAG systems, consider the following three structured methods, each accompanied by comprehensive guides and practical implementations:\n    \n\n      \nRe-ranking Retrieved Results\n: A fundamental and effective method involves employing a Re-ranking Model to refine the results obtained through initial retrieval. This approach prioritizes more relevant results, thereby improving the overall quality of the generated content. MonoT5, MonoBERT, DuoBERT, etc. are examples of deep models that can be used as re-rankers. For a detailed exploration of this technique, refer to the \nguide and code example\n provided by Mahesh Deshwal. A detailed discourse on re-ranking is available in the \nRe-ranking\n section.\n\n      \nFLARE Technique\n: Subsequent to re-ranking, one should explore the FLARE methodology. This technique dynamically queries the internet (could also be a local knowledge base) whenever the confidence level of a segment of the generated content falls below a specified threshold. This overcomes a significant limitation of conventional RAG systems, which typically query the knowledge base only at the outset and subsequently produce the final output. Akash Desai’s \nguide and code walkthrough\n offer an insightful understanding and practical application of this technique. More on the FLARE technique in the \nActive Retrieval Augmented Generation\n section.\n\n      \nHyDE Approach\n: Finally, the HyDE technique introduces an innovative concept of generating a hypothetical document in response to a query. This document is then converted into an embedding vector. The uniqueness of this method lies in using the vector to identify a similar neighborhood within the corpus embedding space, thereby retrieving analogous real documents based on vector similarity. To delve into this method, refer to Akash Desai’s \nguide and code implementation\n. More on the HyDE technique in the \nPrecise Zero-Shot Dense Retrieval Without Relevance Labels\n section.\n\n    \n\n  \n\n  \nEach of these methods offers a unique approach to refining RAG systems, contributing to more accurate and contextually relevant results.\n\n\n\n\n\nRAG 2.0\n\n\n\n\n  \nRAG 2.0\n, unveiled by \nContextual AI\n, represents a significant advancement in robust AI systems for enterprise use, optimizing the entire system end-to-end unlike its predecessor. This new generation introduces Contextual Language Models (CLMs) which not only surpass the original RAG benchmarks but also outperform the strongest available models based on GPT-4, across various industry benchmarks, demonstrating superior performance in open domain question-answering and specialized tasks like truth verification.\n\n  \nThe introduction of RAG 2.0 marks a departure from the use of off-the-shelf models and disjointed components, which characterized previous systems as brittle and suboptimal for production environments. Instead, RAG 2.0 end-to-end optimizes the language model and retriever as a single system.\n\n  \nKey improvements are evident in real-world applications where RAG 2.0 CLMs have been deployed. Using Google Cloud’s latest ML infrastructure, these models have shown significant accuracy enhancements, particularly in sectors like finance and law, highlighting their potential in specialized domains.\n\n  \nFurther comparisons reveal that RAG 2.0 significantly outperforms traditional long-context models, providing higher accuracy with less computational demand. This makes RAG 2.0 particularly appealing for scaling in production environments.\n\n  \nOverall, RAG 2.0’s innovative approach not only pushes the boundaries of generative AI in production settings but also demonstrates its superiority through extensive benchmarks and real-world deployments, inviting enterprises to join in its ongoing development and application.\n\n\n\n\n\nSelected Papers\n\n\n\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n\n\n\n\n\n\n  \nThe paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.\n\n  \nAddressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.\n\n  \nThe RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.\n\n  \nThe retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.\n\n  \nRAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.\n\n  \nA summary of the methods and models used for query/document embedding and retrieval, as well as the end-to-end structure of the RAG framework is as below:\n    \n\n      \nQuery/Document Embedding:\n\n        \n\n          \nThe retrieval component, Dense Passage Retriever (DPR), follows a bi-encoder architecture.\n\n          \nDPR uses BERTBASE as the foundation for both document and query encoders.\n\n          \nFor a document \\(z\\), a dense representation \\(d(z)\\) is produced by a document encoder, \\(BERT_d\\).\n\n          \nFor a query \\(x\\), a query representation \\(q(x)\\) is produced by a query encoder, \\(BERT_q\\).\n\n          \nThe embeddings are created such that relevant documents for a given query are close in the embedding space, allowing effective retrieval.\n\n        \n\n      \n\n      \nRetrieval Process:\n\n        \n\n          \nThe retrieval process involves calculating the top-\\(k\\) documents with the highest prior probability, which is essentially a Maximum Inner Product Search (MIPS) problem.\n\n          \nThe MIPS problem is solved approximately in sub-linear time to efficiently retrieve relevant documents.\n\n        \n\n      \n\n      \nEnd-to-End Structure:\n\n        \n\n          \nThe RAG model uses the input sequence \\(x\\) to retrieve text documents \\(z\\), which are then used as additional context for generating the target sequence \\(y\\).\n\n          \nThe generator component is modeled using BART-large, a pre-trained seq2seq transformer with 400M parameters. BART-large combines the input \\(x\\)with the retrieved content \\(z\\) for generation.\n\n          \nThe RAG-Sequence model uses the same retrieved document for generating the complete sequence, while the RAG-Token model can use different passages per token.\n\n          \nThe training process involves jointly training the retriever and generator components without direct supervision on what document should be retrieved. The training minimizes the negative marginal log-likelihood of each target using stochastic gradient descent with Adam.\n\n          \nNotably, the document encoder BERTd is kept fixed during training, avoiding the need for periodic updates of the document index.\n\n        \n\n      \n\n    \n\n  \n\n  \nThe following figure from the paper illustrates an overview of the proposed approach. They combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query \\(x\\), they use Maximum Inner Product Search (MIPS) to find the top-\\(K\\) documents \\(z_i\\). For final prediction \\(y\\), they treat \\(z\\) as a latent variable and marginalize over seq2seq predictions given different documents.\n\n\n\n\n\n\n\n\n\n  \nIn open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.\n\n  \nRAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.\n\n  \nOn the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.\n\n  \nThis study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.\n\n  \nCode\n; \ninteractive demo\n.\n\n\n\n\n\nActive Retrieval Augmented Generation\n\n\n\n\n  \nDespite the remarkable ability of large language models (LLMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output.\n\n  \nAugmenting LLMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LLMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries.\n\n  \nThis paper from Jiang et al. at CMU, Sea AI Lab, and Meta AI in EMNLP 2023 presents Forward-Looking Active REtrieval augmented generation (FLARE), a method addressing the tendency of large language models (LLMs) to produce factually inaccurate content.\n\n  \nFLARE iteratively uses predictions of upcoming sentences to actively decide when and what to retrieve across the generation process, enhancing LLMs with dynamic, multi-stage external information retrieval.\n\n  \nUnlike traditional retrieve-and-generate models that use fixed intervals or input-based retrieval, FLARE targets continual information gathering for long text generation, reducing hallucinations and factual inaccuracies.\n\n  \nThe system triggers retrieval when generating low-confidence tokens, determined by a probability threshold. This anticipates future content, forming queries to retrieve relevant documents for regeneration.\n\n  \nThe following figure from the paper illustrates FLARE. Starting with the user input \\(x\\) and initial retrieval results \\(D_x\\), FLARE iteratively generates a temporary next sentence (shown in gray italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the system retrieves relevant documents and regenerates the sentence.\n\n\n\n\n\n\n\n\n\n  \nFLARE was tested on four long-form, knowledge-intensive generation tasks/datasets, exhibiting superior or competitive performance, demonstrating its effectiveness in addressing the limitations of existing retrieval-augmented LLMs.\n\n  \nThe model is adaptable to existing LLMs, as shown with its implementation on GPT-3.5, and employs off-the-shelf retrievers and the Bing search engine.\n\n  \nCode\n.\n\n\n\n\n\nMuRAG: Multimodal Retrieval-Augmented Generator\n\n\n\n\n  \nThis paper by Chen et al. from Google Research proposes Multimodal Retrieval-Augmented Transformer (MuRAG), which looks to extend the retrieval process beyond text to include other modalities like images or structured data, which can then be used alongside textual information to inform the generation process.\n\n  \nMuRAG’s magic lies in its two-phase training approach: pre-training and fine-tuning, each carefully crafted to build the model’s ability to tap into a vast expanse of multimodal knowledge.\n\n  \nThe key goal of MuRAG is to incorporate both visual and textual knowledge into language models to improve their capability for multimodal question answering.\n\n  \nMuRAG is distinct in its ability to access an external non-parametric multimodal memory (images and texts) to enhance language generation, addressing the limitations of text-only retrieval in previous models.\n\n  \nMuRAG has a dual-encoder architecture combines pre-trained visual transformer (ViT) and a text encoder (T5) models to create a backbone encoder, enabling the encoding of image-text pairs, image-only, and text-only inputs into a unified/joint multimodal representation.\n\n  \nMuRAG is pre-trained on a mixture of image-text data (LAION, Conceptual Captions) and text-only data (PAQ, VQA). It uses a contrastive loss for retrieving relevant knowledge and a generation loss for answer prediction. It employs a two-stage training pipeline: initial training with small in-batch memory followed by training with a large global memory.\n\n  \nDuring the retriever stage, MuRAG takes a query \\(q\\) of any modality as input and retrieves from a memory \\(\\mathcal{M}\\) of image-text pairs. Specifically, we apply the backbone encoder \\(f_\\theta\\) to encode a query \\(q\\), and use maximum inner product search (MIPS) over all of the memory candidates \\(m \\in \\mathcal{M}\\) to find the top-\\(k\\) nearest neighbors \\(\\operatorname{Top}_K(\\mathcal{M} \\mid q)=\\left[m_1, \\cdots, m_k\\right]\\). Formally, we define \\(\\operatorname{Top}_K(\\mathcal{M} \\mid q)\\) as follows:\n\n\n\n\n\\[\\operatorname{Top}_K(\\mathcal{M} \\mid q)=\\underset{m \\in \\mathcal{M}}{\\operatorname{Top}} \\quad f_\\theta(q)_{[\\mathrm{CLS}]} \\cdot f_\\theta(m)_{[\\mathrm{CLS}]}\\]\n\n\n\n  \n\n    \nDuring the reader stage, the retrievals (the raw image patches) are combined with the query \\(q\\) as an augmented input \\(\\left[m_1, \\cdots, m_k, q\\right]\\), which is fed to the backbone encoder \\(f_\\theta\\) to produce retrievalaugmented encoding. The decoder model \\(g_\\theta\\) uses attention over this representation to generate textual outputs \\(\\mathbf{y}=y_1, \\cdots, y_n\\) token by token.\n\n\n\\[p\\left(y_i \\mid y_{i-1}\\right)=g_\\theta\\left(y_i \\mid f_\\theta\\left(\\operatorname{Top}_K(\\mathcal{M} \\mid q) ; q\\right) ; y_{1: i-1}\\right)\\]\n\n    \n\n      \nwhere \\(y\\) is decoded from a given vocabulary \\(\\mathcal{V}\\).\n\n    \n\n  \n\n  \n\n    \nThe figure below from the original paper \n(source)\n shows how the model taps into an external repository to retrieve a diverse range of knowledge encapsulated within both images and textual fragments. This multimodal information is then employed to enhance the generative process. The upper section outlines the setup for the pre-training phase, whereas the lower section specifies the framework for the fine-tuning phase.\n\n  \n\n\n\n\n\n\n\n\n\n  \nThe process can be summarized as follows:\n    \n\n      \nFor retrieval, MuRAG uses maximum inner product search to find the top-\\(k\\) most relevant image-text pairs from the memory given a question. The “memory” here refers to the external knowledge base that the model can retrieve information from. Specifically, the memory contains a large collection of image-text pairs that are encoded offline by the backbone encoder prior to training.\n\n      \nDuring training and inference, given a question, MuRAG’s retriever module will search through this memory to find the most relevant image-text pairs using maximum inner product search.\n\n      \nThe memory serves as the knowledge source and can contain various types of multimodal data like images with captions, passages of text, tables, etc. that are related to the downstream task.\n\n      \nFor example, when fine-tuning on the WebQA dataset, the memory contains 1.1 million image-text pairs extracted from Wikipedia that the model can retrieve from to answer questions.\n\n      \nSo in summary, the memory is the large non-parametric external knowledge base encoded in a multimodal space that MuRAG learns to retrieve relevant knowledge from given a question, in order to augment its language generation capabilities. The memory provides the world knowledge to complement what is stored implicitly in the model’s parameters.\n\n      \nFor reading, the retrieved multimodal context is combined with the question embedding and fed into the decoder to generate an answer.\n\n    \n\n  \n\n  \nMuRAG achieves state-of-the-art results on two multimodal QA datasets - WebQA and MultimodalQA, outperforming text-only methods by 10-20% accuracy. It demonstrates the value of incorporating both visual and textual knowledge.\n\n  \nKey limitations are the reliance on large-scale pre-training data, computational costs, and issues in visual reasoning like counting objects. But overall, MuRAG represents an important advance in building visually-grounded language models.\n\n\n\n\n\nHypothetical Document Embeddings (HyDE)\n\n\n\n\n  \nPublished in \nPrecise Zero-Shot Dense Retrieval without Relevance Labels\n by Gao et al. from CMU and University of Waterloo, proposes an innovative approach called Hypothetical Document Embeddings (HyDE) for effective zero-shot dense retrieval in the absence of relevance labels. HyDE leverages an instruction-following language model, such as InstructGPT, to generate a hypothetical document that captures relevance patterns, although it may contain factual inaccuracies. An unsupervised contrastive encoder, like Contriever, then encodes this document into an embedding vector to identify similar real documents in the corpus embedding space, effectively filtering out incorrect details.\n\n  \nThe implementation of HyDE combines InstructGPT (a GPT-3 model) and Contriever models, utilizing OpenAI playground’s default temperature setting for generation. For English retrieval tasks, the English-only Contriever model was used, while for non-English tasks, the multilingual mContriever was employed.\n\n  \nThe following image from the paper illustrates the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever/mContriever models.\n\n\n\n\n\n\n\n\n\n  \nExperiments were conducted using the Pyserini toolkit. The results demonstrate HyDE’s significant improvement over the state-of-the-art unsupervised dense retriever Contriever, with strong performance comparable to fine-tuned retrievers across various tasks and languages. Specifically, in web search and low-resource tasks, HyDE showed sizable improvements in precision and recall-oriented metrics. It remained competitive even compared to fine-tuned models, particularly in terms of recall. In multilingual retrieval, HyDE improved the mContriever model and outperformed non-Contriever models fine-tuned on MS-MARCO. However, there were some performance gaps with fine-tuned mContrieverFT, likely due to under-training in non-English languages.\n\n  \nFurther analysis explored the effects of using different generative models and fine-tuned encoders with HyDE. Larger language models brought greater improvements, and the use of fine-tuned encoders with HyDE showed that less powerful instruction language models could impact the performance of the fine-tuned retriever.\n\n  \nOne possible pitfall of HyDE is that it can potentially “hallucinate” in the sense that it generates hypothetical documents that may contain invented or inaccurate details. This phenomenon occurs because HyDE uses an instruction-following language model, like InstructGPT, to generate a document based on a query. The generated document is intended to capture the relevance patterns of the query, but since it’s created without direct reference to real-world data, it can include false or fictional information. This aspect of HyDE is a trade-off for its ability to operate in zero-shot retrieval scenarios, where it creates a contextually relevant but not necessarily factually accurate document to guide the retrieval process.\n\n  \nIn conclusion, the paper introduces a new paradigm of interaction between language models and dense encoders/retrievers, showing that relevance modeling and instruction understanding can be effectively handled by a powerful and flexible language model. This approach eliminates the need for relevance labels, offering practical utility in the initial stages of a search system’s life, and paving the way for further advancements in tasks like multi-hop retrieval/QA and conversational search.\n\n\n\n\n\nRAGAS: Automated Evaluation of Retrieval Augmented Generation\n\n\n\n\n  \nThis paper by Es et al. from Exploding Gradients, Cardiff University, and AMPLYFI introduces RAGAS, a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) systems.\n\n  \nRAGAS focuses on evaluating the performance of RAG systems in dimensions such as the effectiveness of the retrieval system in providing relevant context, the LLM’s ability to utilize this context, and the overall quality of generation.\n\n  \nThe framework proposes a suite of metrics to evaluate these dimensions without relying on ground truth human annotations.\n\n  \nRAGAS focuses on three quality aspects: Faithfulness, Answer Relevance, and Context Relevance.\n    \n\n      \nFaithfulness\n: Defined as the extent to which the generated answer is grounded in the provided context. It’s measured using the formula:\n\\(F = \\frac{|V|}{|S|}\\)\nwhere, \\(|V|\\) is the number of statements supported by the context and \\(|S|\\) is the total number of statements extracted from the answer.\n\n      \nAnswer Relevance\n: This metric assesses how well the answer addresses the given question. It’s calculated by generating potential questions from the answer and measuring their similarity to the original question using the formula:\n\\(AR = \\frac{1}{n} \\sum_{i=1}^{n} \\text{sim}(q, q_i)\\)\nwhere \\(q\\) is the original question, \\(q_i\\) are the generated questions, and sim denotes the cosine similarity between their embeddings.\n\n      \nContext Relevance\n: Measures the extent to which the retrieved context contains only the information necessary to answer the question. It is quantified using the proportion of extracted relevant sentences to the total sentences in the context:\n\\(CR = \\frac{\\text{number of extracted sentences}}{\\text{total number of sentences in } c(q)}\\)\n\n    \n\n  \n\n  \nThe paper validates RAGAS using the WikiEval dataset, demonstrating its alignment with human judgments in evaluating these aspects.\n\n  \nThe authors argue that RAGAS contributes to faster and more efficient evaluation cycles for RAG systems, which is vital due to the rapid adoption of LLMs.\n\n  \nRAGAS is validated using the WikiEval dataset, which includes question-context-answer triples annotated with human judgments for faithfulness, answer relevance, and context relevance.\n\n  \nThe evaluation shows that RAGAS aligns closely with human judgments, particularly in assessing faithfulness and answer relevance.\n\n  \nCode\n.\n\n\n\n\n\nFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\n\n\n\n\n  \nThis paper by Ovadia et al. from Microsoft presents an insightful comparison of knowledge injection methods in large language models (LLMs). The core question addressed is whether unsupervised fine-tuning (USFT) is more effective than retrieval-augmented generation (RAG) for improving LLM performance on knowledge-intensive tasks.\n\n  \nThe researchers focus on LLMs’ ability to memorize, understand, and retrieve factual data, using a knowledge base scraped from Wikipedia and a dataset of current events questions created with GPT-4. The study employs models like Llama2-7B, Mistral-7B, and Orca2-7B, evaluating them on tasks from the Massively Multitask Language Understanding Evaluation (MMLU) benchmark and a current events dataset.\n\n  \nTwo methods of knowledge injection are explored: fine-tuning, which continues the model’s pre-training process using task-specific data, and retrieval-augmented generation (RAG), which uses external knowledge sources to enhance LLMs’ responses. The paper also delves into supervised, unsupervised, and reinforcement learning-based fine-tuning methods.\n\n  \nThe key finding is that RAG outperforms unsupervised fine-tuning in knowledge injection. RAG, which uses external knowledge sources, is notably more effective in terms of knowledge injection than USFT alone and even more so than a combination of RAG and fine-tuning, particularly in scenarios where questions directly corresponded to the auxiliary dataset. This suggests that USFT may not be as efficient in embedding new knowledge into the model’s parameters.\n\n  \nThe figure below from the paper shows a visualization of the knowledge injection framework.\n\n\n\n\n\n\n\n\n\n  \nNote that USFT in this context is a direct continuation of pre-training (hence also called continued pre-training in literature), predicting the next token on the dataset. Interestingly, fine-tuning with multiple paraphrases of the same fact significantly improves the baseline performance, indicating the importance of repetition and varied presentation of information for effective knowledge assimilation.\n\n  \nThe authors created a knowledge base by scraping Wikipedia articles relevant to various topics, which was used for both fine-tuning and RAG. Additionally, a dataset of multiple-choice questions about current events was generated using GPT-4, with paraphrases created to augment this dataset.\n\n  \nLimitations of the study include the exclusive focus on unsupervised fine-tuning, without exploring supervised fine-tuning or reinforcement learning from human feedback (RLHF). The study also notes a high variance in accuracy performance across experiments, making it challenging to ascertain the statistical significance of the results.\n\n  \nThe paper also questions why baseline models don’t achieve a 25% accuracy rate for multiple-choice questions with four options, suggesting that the tasks may not represent truly “unseen” knowledge. Moreover, the research primarily assesses straightforward knowledge or fact tasks, without delving into reasoning capabilities.\n\n  \nIn summary, while fine-tuning can be beneficial, RAG is identified as a superior method for knowledge injection in LLMs, especially for tasks involving new information. The results highlight the potential of using diverse fine-tuning techniques and auxiliary knowledge bases for further research in this domain.\n\n\n\n\n\nDense X Retrieval: What Retrieval Granularity Should We Use?\n\n\n\n\n  \nOne crucial choice in RAG pipeline design is chunking: should it be sentence level, passage level, or chapter level? This choice significantly impacts your retrieval and response generation performance.\n\n  \nThis paper by Chen et al. from the University of Washington, Tencent AI Lab, University of Pennsylvania, Carnegie Mellon University introduces a novel approach to dense retrieval in open-domain NLP tasks by using “propositions” as retrieval units, instead of the traditional document passages or sentences. A proposition is defined as an atomic expression within text, encapsulating a distinct factoid in a concise, self-contained natural language format. This change in retrieval granularity has a significant impact on both retrieval and downstream task performances.\n\n  \nPropositions follow three key principles:\n    \n\n      \nEach proposition encapsulates a distinct meaning, collectively representing the semantics of the entire text.\n\n      \nThey are minimal and indivisible, ensuring precision and clarity.\n\n      \nEach proposition is contextualized and self-contained, including all necessary text context (like coreferences) for full understanding.\n\n    \n\n  \n\n  \nThe authors developed a text generation model, named “Propositionizer,” to segment Wikipedia pages into propositions. This model was fine-tuned in two steps, starting with prompting GPT-4 for paragraph-to-propositions pairs generation, followed by fine-tuning a Flan-T5-large model.\n\n  \nThe effectiveness of propositions as retrieval units was evaluated using the FACTOIDWIKI dataset, a processed English Wikipedia dump segmented into passages, sentences, and propositions. Experiments were conducted on five open-domain QA datasets: Natural Questions (NQ), TriviaQA (TQA), Web Questions (WebQ), SQuAD, and Entity Questions (EQ). Six different dense retriever models were compared: SimCSE, Contriever, DPR, ANCE, TAS-B, and GTR.\n\n  \nThe figure below from the paper illustrates the fact that that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet effective strategy to increase dense retrievers’ generalization performance at inference time \\((A, B)\\). We empirically compare the retrieval and downstream open-domain QA tasks performance when dense retrievers work with Wikipedia indexed at the level of 100-word passage, sentence or proposition \\((C, D)\\).\n\n\n\n\n\n\n\n\n\n  \nResults:\n    \n\n      \nPassage Retrieval Performance:\n Proposition-based retrieval consistently outperformed sentence and passage-level retrieval across all datasets and models. This was particularly evident with unsupervised retrievers like SimCSE and Contriever, which showed an average Recall@5 improvement of 12.0% and 9.3%, respectively.\n\n      \nCross-Task Generalization:\n The advantage of proposition retrieval was most pronounced in cross-task generalization settings, especially for queries about less common entities. It showed significant improvement over other granularities in datasets not seen during the training of the retriever models.\n\n      \nDownstream QA Performance:\n In the retrieve-then-read setting, proposition-based retrieval led to stronger downstream QA performance. This was true for both unsupervised and supervised retrievers, with notable improvements in exact match (EM) scores.\n\n      \nDensity of Question-Related Information:\n Propositions proved to offer a higher density of relevant information, resulting in the correct answers appearing more frequently within the top-l retrieved words. This was a significant advantage over sentence and passage retrieval, particularly in the range of 100-200 words.\n\n      \nError Analysis:\n The study also highlighted the types of errors typical to each retrieval granularity. For example, passage-level retrieval often struggled with entity ambiguity, while proposition retrieval faced challenges in multi-hop reasoning tasks.\n\n    \n\n  \n\n  \nThe figure plot from the paper shows that retrieving by propositions yields the best retrieval performance in both passage retrieval task and downstream open-domain QA task, e.g. with Contriever or GTR as the backbone retriever.\n\n\n\n\n\n\n\n\n\n  \nThe research demonstrates that using propositions as retrieval units significantly improves dense retrieval performance and downstream QA task accuracy, outperforming traditional passage and sentence-based methods. The introduction of FACTOIDWIKI, with its 250 million propositions, is expected to facilitate future research in information retrieval.\n\n\n\n\n\nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n\n\n\n\n  \nThis paper by Saad-Falcon et al. from Stanford University and UC Berkeley, the paper introduces ARES (Automated RAG Evaluation System) for evaluating Retrieval-Augmented Generation (RAG) systems in terms of context relevance, answer faithfulness, and answer relevance.\n\n  \nARES generates synthetic training data using a language model and fine-tunes lightweight LM judges to assess individual RAG components. It utilizes a small set of human-annotated data points for prediction-powered inference (PPI), enabling statistical guarantees for its predictions.\n\n  \nThe framework has three stages:\n    \n\n      \nLLM Generation of Synthetic Dataset\n: ARES uses generative LLMs (like FLAN-T5 XXL) to create synthetic datasets of question-answer pairs derived from target corpus passages. This stage includes both positive and negative examples for training.\n\n      \nPreparing LLM Judges\n: Separate lightweight LM models are fine-tuned for three classification tasks - context relevance, answer faithfulness, and answer relevance - using the synthetic dataset. These models are tuned using a contrastive learning objective.\n\n      \nRanking RAG Systems with Confidence Intervals\n:\n        \n\n          \nAfter preparing the LLM judges, the next step involves using them to score and rank various RAG systems. This process begins with ARES sampling in-domain query-document-answer triples from each RAG approach. The judges then label each triple, assessing context relevance, answer faithfulness, and answer relevance. These labels are averaged for each in-domain triple to evaluate the performance of the RAG systems across the three metrics.\n\n          \nWhile average scores could be reported as quality metrics for each RAG system, these scores are based on unlabeled data and predictions from synthetically-trained LLM judges, which may introduce noise. An alternative is to rely solely on a small human preference validation set for evaluation, examining the extent to which each RAG system aligns with human annotations. However, this method requires labeling outputs from each RAG system separately, which can be time-consuming and expensive.\n\n          \nTo enhance the precision of the evaluation, ARES employs prediction-powered inference (PPI). PPI is a statistical method that narrows the confidence interval of predictions on a small annotated dataset by utilizing predictions on a larger, non-annotated dataset. It combines labeled datapoints and ARES judge predictions on non-annotated datapoints to construct tighter confidence intervals for RAG system performance.\n\n          \nPPI involves using LLM judges on the human preference validation set to learn a rectifier function. This function constructs a confidence set of the ML model’s performance, taking into account each ML prediction in the larger non-annotated dataset. The confidence set helps create a more precise confidence interval for the average performance of the ML model (e.g., its context relevance, answer faithfulness, or answer relevance accuracy). By integrating the human preference validation set with a larger set of datapoints with ML predictions, PPI develops reliable confidence intervals for ML model performance, outperforming traditional inference methods.\n\n          \nThe PPI rectifier function addresses errors made by the LLM judge and generates confidence bounds for the success and failure rates of the RAG system. It estimates performances in context relevance, answer faithfulness, and answer relevance. PPI also allows for estimating confidence intervals with a specified probability level; in these experiments, a standard 95% alpha is used.\n\n          \nFinally, the accuracy confidence interval for each component of the RAG is determined, and the midpoints of these intervals are used to rank the RAG systems. This ranking enables a comparison of different RAG systems and configurations within the same system, aiding in identifying the optimal approach for a specific domain.\n            \n\n              \nIn summary, ARES employs PPI to score and rank RAG systems, using human preference validation sets to calculate confidence intervals. PPI operates by first generating predictions for a large sample of data points, followed by human annotation of a small subset. These annotations are used to calculate confidence intervals for the entire dataset, ensuring accuracy in the system’s evaluation capabilities.\n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \nTo implement ARES for scoring a RAG system and comparing to other RAG configurations, three components are needed:\n    \n\n      \nA human preference validation set of annotated query, document, and answer triples for the evaluation criteria (e.g. context relevance, answer faithfulness, and/or answer relevance). There should be at least 50 examples but several hundred examples is ideal.\n\n      \nA set of few-shot examples for scoring context relevance, answer faithfulness, and/or answer relevance in your system.\n\n      \nA much larger set of unlabeled query-document-answer triples outputted by your RAG system for scoring.\n\n    \n\n  \n\n  \nThe figure below from the paper shows an overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference validation set of 150 annotated datapoints or more, and five few-shot examples of in-domain queries and answers, which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation, we first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a constrastive learning framework, we fine-tune an LLM to classify query–passage–answer triples across three criteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judge to evaluate RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n\n\n\n\n\n\n\n\n\n  \nExperiments conducted on datasets from KILT and SuperGLUE demonstrate ARES’s accuracy in evaluating RAG systems, outperforming existing automated evaluation approaches like RAGAS. ARES is effective across various domains, maintaining accuracy even with domain shifts in queries and documents.\n\n  \nThe paper highlights the strengths of ARES in cross-domain applications and its limitations, such as its inability to generalize across drastic domain shifts (e.g., language changes, text-to-code). It also explores the potential of using GPT-4 for generating labels as a replacement for human annotations in the PPI process.\n\n  \nARES code and datasets are available for replication and deployment at \nGitHub\n.\n\n  \nCode\n\n\n\n\n\nSeven Failure Points When Engineering a Retrieval Augmented Generation System\n\n\n\n\n  \nThis technical report by Barnett et al. from the Applied Artificial Intelligence Institute, Deakin University, Australia, explores failure points in the implementation of Retrieval Augmented Generation (RAG) systems. based on three case studies in diverse domains: research, education, and biomedical.\n\n  \nRAG systems, which integrate retrieval mechanisms with Large Language Models (LLMs) to generate contextually relevant responses, are scrutinized for their operational challenges. The paper identifies seven key failure points in RAG systems:\n    \n\n      \nFP1 Missing Relevant Content:\n The first failure case is when asking a question that cannot be answered from the available documents. In the happy case the RAG system will respond with something like “Sorry, I don’t know”. However, for questions that are related to the content but don’t have answers the system could be fooled into giving a response.\n\n      \nFP2 Missed the Top Ranked Documents:\n The answer to the question is in the document but did not rank highly enough to be returned to the user. In theory, all documents are ranked and used in the next steps. However, in practice only the top \\(K\\) documents are returned where \\(K\\) is a value selected based on performance.\n\n      \nFP3 Not in Context - Consolidation Strategy Limitations:\n Documents with the answer were retrieved from the database but did not make it into the context for generating an answer. This occurs when many documents are returned from the database and a consolidation process takes place to retrieve the answer.\n\n      \nFP4 Not Extracted Here:\n the answer is present in the context, but the large language model failed to extract out the correct answer. Typically, this occurs when there is too much noise or contradicting information in the context.\n\n      \nFP5 Wrong Format:\n The question involved extracting information in a certain format such as a table or list and the large language model ignored the instruction.\n\n      \nFP6 Incorrect Specificity:\n The answer is returned in the response but is not specific enough or is too specific to address the user’s need. This occurs when the RAG system designers have a desired outcome for a given question such as teachers for students. In this case, specific educational content should be provided with answers not just the answer. Incorrect specificity also occurs when users are not sure how to ask a question and are too general.\n\n      \nFP7 Incomplete Responses:\n Incomplete answers are not incorrect but miss some of the information even though that information was in the context and available for extraction. An example question such as “What are the key points covered in documents A, B and C?” A better approach is to ask these questions separately.\n\n    \n\n  \n\n  \nThe study also emphasizes the importance of real-time validation and the evolving robustness of RAG systems. It concludes with suggestions for future research directions, highlighting the significance of chunking, embeddings, and the trade-offs between RAG systems and fine-tuning LLMs.\n\n  \nThe following image from the paper shows the Indexing and Query processes required for creating a Retrieval Augmented Generation (RAG) system. The indexing\nprocess is typically done at development time and queries at runtime. Failure points identified in this study are shown in red boxes. All required stages are underlined.\n\n\n\n\n\n\n\n\n\n  \nMoreover, the paper provides insights into the challenges faced in implementing RAG systems, such as handling diverse document types, query preprocessing, and the need for continuous calibration and monitoring of these systems. These findings are derived from practical experiences and offer valuable guidance for practitioners in the field.\n\n\n\n\n\nRAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n\n\n\n\n  \nThis paper by Sarthi et al. from Manning’s Lab at Stanford, published in ICLR 2024, introduces RAPTOR, a novel approach for retrieval-augmented language models. RAPTOR addresses the limitation of existing retrieval methods that primarily fetch short text chunks, hindering comprehensive document understanding. It constructs a tree by recursively embedding, clustering, and summarizing text chunks, offering multi-level summarization and facilitating efficient information retrieval from extensive documents.\n\n  \nAt its core, RAPTOR employs a tree structure starting from leaf nodes (text chunks) and builds up to the root through successive clustering and summarization. This method allows the model to access information at various abstraction levels, significantly enhancing performance on complex, multi-step reasoning tasks. When combined with GPT-4, RAPTOR achieved a 20% absolute accuracy improvement on the QuALITY benchmark over previous state-of-the-art models.\n\n  \nSome key insights into why using a tree-structure lets your RAG pipeline handle more complex questions:\n    \n\n      \nCluster semantically related chunks to dynamically identify distinct topics within your documents.\n\n      \nCreate new chunks by summarizing clusters.\n\n      \nMix high-level and low-level chunks during retrieval, to dynamically surface relevant information depending on the query.\n\n    \n\n  \n\n  \nThe model utilizes SBERT for embedding text chunks and Gaussian Mixture Models (GMMs) for clustering, allowing flexible groupings of related content. Summarization is performed by a language model (GPT-3.5-turbo), producing summaries that guide the construction of higher tree levels. This recursive process creates a scalable and computationally efficient system that linearly scales in both token expenditure and build time, as detailed in the scalability analysis.\n\n  \nQuerying within RAPTOR’s tree employs two strategies: tree traversal and collapsed tree, with the latter showing superior flexibility and effectiveness in preliminary tests on the QASPER dataset. The model’s innovative clustering mechanism, highlighted in an ablation study, proves essential for capturing thematic content and outperforms standard retrieval methods.\n\n  \nThe figure below from the paper shows the tree construction process: RAPTOR recursively clusters chunks of text based on their vector embeddings and generates text summaries of those clusters, constructing a tree from the bottom up. Nodes clustered together are siblings; a parent node contains the text summary of that cluster.\n\n\n\n\n\n\n\n\n\n  \nExperimental results across various datasets (NarrativeQA, QASPER, QuALITY) demonstrate RAPTOR’s effectiveness, setting new benchmarks and outperforming existing retrieval-augmented models. The paper’s qualitative analysis illustrates RAPTOR’s ability to retrieve relevant information for thematic questions, showcasing its superiority over Dense Passage Retrieval (DPR) methods in handling complex queries.\n\n  \nThe paper includes a comprehensive reproducibility statement, detailing the use of publicly available language models and datasets, ensuring that the community can replicate and extend upon RAPTOR’s findings.\n\n\n\n\n\nThe Power of Noise: Redefining Retrieval for RAG Systems\n\n\n\n\n  \nThis paper by Cuconasu et al. from Sapienza University of Rome, Technology Innovation Institute, and University of Pisa introduces a comprehensive study on Retrieval-Augmented Generation (RAG) systems, highlighting the significant influence of Information Retrieval (IR) components on RAG’s performance, beyond the generative abilities of Large Language Models (LLMs).\n\n  \nTheir research investigates the characteristics required in a retriever for optimal RAG prompt formulation, emphasizing the balance between relevant, related, and irrelevant documents.\n\n  \nThe study reveals that including irrelevant documents surprisingly enhances RAG system performance by over 30% in accuracy, challenging the assumption that only relevant and related documents should be retrieved. This finding underscores the potential of integrating seemingly noise-adding strategies to improve RAG system outputs, thereby laying the groundwork for future research in IR and language model integration.\n\n  \nThe experimental methodology employed involves a detailed examination of the Natural Questions dataset, testing various configurations of document relevance and placement within the RAG prompt. This methodological rigor allows the researchers to dissect the impact of document type (gold, relevant, related, irrelevant) and position on the accuracy of RAG system responses, with attention to how these factors influence LLM’s generative performance.\n\n  \nInsights from the experiments led to the formulation of strategies for optimizing RAG systems, proposing a nuanced approach to document retrieval that includes a mix of relevant and intentionally irrelevant documents. This approach aims to maximize system performance within the context size constraints of LLMs, offering a novel perspective on the integration of retrieval processes with generative language models for enhanced factual accuracy and context awareness.\n\n  \nThe study’s findings challenge traditional IR strategies and suggest a paradigm shift towards the inclusion of controlled noise in the retrieval process for language generation tasks. The researchers advocate for further exploration into the mechanisms by which irrelevant documents improve RAG system performance, highlighting the need for new IR techniques tailored to the unique demands of language generation models.\n\n\n\n\n\nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n\n\n\n\n  \nThis paper by Tang et al. from the Hong Kong University of Science and Technology introduces MultiHop-RAG, a novel dataset and benchmark for evaluating Retrieval-Augmented Generation (RAG) systems on multi-hop queries. These queries necessitate retrieving and reasoning over multiple pieces of evidence, a challenge not adequately addressed by existing RAG systems.\n\n  \nMultiHop-RAG consists of a knowledge base derived from English news articles, multi-hop queries, their answers, and the supporting evidence required for those answers. This dataset aims to mimic real-world applications where complex queries involving multiple pieces of information are common.\n\n  \nThe figure below from the paper shows the RAG flow with a multi-hop query.\n\n\n\n\n\n\n\n\n\n  \nThe authors categorize multi-hop queries into four types: Inference, Comparison, Temporal, and Null queries. The first three types — Inference, Comparison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encompassing tasks like inferring relationships, comparing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance. Each type requires a distinct retrieval and reasoning strategy over the evidence, with Null queries designed to test the model’s ability to refrain from generating an answer when the query cannot be resolved with the available knowledge.\n\n  \nThey define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query \\(q\\), the chunks in the retrieval set \\(\\mathcal{R}_q\\) collectively provide an answer to \\(q\\). For example, the query “Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?” requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a singlehop query such as “What is Google’s profit margin in the third-quarter reports for 2023 ,” where the answer can be directly derived from a single piece of evidence.\n\n  \nBased on the queries commonly used in realworld RAG systems, they identify four types of multi-hop queries. For each type, they present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\n    \n\n      \nInference query:\n For such a query \\(q\\), the answer is deduced through reasoning from the retrieval set \\(\\mathcal{R}_q\\). An example of an inference query might be: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\n\n      \nComparison query:\n For such a query \\(q\\), the answer requires a comparison of evidence within the retrieval set \\(\\mathcal{R}_q\\). For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?”\n\n      \nTemporal query:\n For such a query \\(q\\), the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\n\n      \nNull query:\n For such as query \\(q\\), the answer cannot be derived from the retrieved set \\(\\mathcal{R}_q\\). They include the null query to assess the generation quality, especially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assuming ABCS is a non-existent company, a null query might ask: What are the sales of company ABCS as reported in its 2022 and 2023 annual reports?\n\n    \n\n  \n\n  \nThe dataset was created using GPT-4 to generate multi-hop queries from a pool of factual sentences extracted from news articles. The queries were then validated for quality and relevance. This process ensures the dataset’s utility in benchmarking the capability of RAG systems to handle complex queries beyond the capacity of current systems.\n\n  \nExperimental results demonstrate that existing RAG methods struggle with multi-hop query retrieval and answering, underscoring the necessity for advancements in this area. The benchmarking also explores the effectiveness of different embedding models for evidence retrieval and the reasoning capabilities of various state-of-the-art Large Language Models (LLMs) including GPT-4, PaLM, and Llama2-70B, revealing significant room for improvement.\n\n  \nThe authors hope that MultiHop-RAG will encourage further research and development in RAG systems, particularly those capable of sophisticated multi-hop reasoning, thereby enhancing the practical utility of LLMs in complex information-seeking tasks.\n\n  \nCode\n\n\n\n\n\nRAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture\n\n\n\n\n  \nThis paper by Balaguer et al. from Microsoft, delves into two prevalent approaches for incorporating proprietary and domain-specific data into Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments prompts with external data, whereas Fine-Tuning embeds additional knowledge directly into the model. The paper outlines a comprehensive pipeline for both approaches, evaluating their effectiveness on multiple popular LLMs including Llama2-13B, GPT-3.5, and GPT-4.\n\n  \nThe research particularly focuses on agriculture, an industry with relatively limited AI penetration, proposing a disruptive application: providing location-specific insights to farmers. The pipeline stages include data acquisition, PDF information extraction, question and answer generation using this data, and leveraging GPT-4 for result evaluation. Metrics are introduced to assess the performance of the RAG and Fine-Tuning pipeline stages.\n\n  \nThe figure below from the paper shows the methodology pipeline. Domain-specific datasets are collected, and the content and structure of the documents are extracted. This information is then fed to the Q&A generation step. Synthesized question-answer pairs are used to fine-tune the LLMs. Models are evaluated with and without RAG under different GPT-4-based metrics.\n\n\n\n\n\n\n\n\n\n  \nExperimental results from an agricultural dataset highlight the pipeline’s capability in capturing geography-specific knowledge. Fine-Tuning demonstrated a significant accuracy increase of over 6 percentage points, a benefit that accumulates with RAG, further enhancing accuracy by 5 percentage points. One experiment showcased the fine-tuned model’s ability to leverage information across geographies to answer specific questions, boosting answer similarity from 47% to 72%.\n\n  \nThe paper presents an in-depth comparison of answers from GPT-4, Bing Chat, and agronomist experts to the same query across different U.S. states, revealing the models’ generic responses versus the experts’ nuanced, location-specific answers. This comparative analysis underscores the potential of fine-tuning and RAG in producing more contextually appropriate responses for industry-specific applications.\n\n  \nThe proposed methodology aims at generating domain-specific questions and answers to create a valuable knowledge resource for industries requiring specific contextual and adaptive responses. Through an extensive evaluation involving benchmarks from major agriculture-producing countries, the study establishes a baseline understanding of model performance in the agricultural context and explores the impact of spatial shift on knowledge encoding and the benefits of spatially-scoped fine-tuning.\n\n  \nAdditionally, the research investigates the implications of retrieval techniques and fine-tuning on LLM performance. It identifies RAG as particularly effective in contexts requiring domain-specific knowledge and fine-tuning as beneficial for imparting new skills to models, albeit at a higher initial cost. This work serves as a foundation for applying RAG and fine-tuning techniques across industries, demonstrating their utility in enhancing model efficiency from the Q&A generation process onwards.\n\n\n\n\n\nRAFT: Adapting Language Model to Domain Specific RAG\n\n\n\n\n  \nThis paper by Zhang et al. from UC Berkeley introduces Retrieval Augmented Fine Tuning (RAFT) as a method to adapt pre-trained Large Language Models (LLMs) for domain-specific Retrieval Augmented Generation (RAG), focusing on “open-book” in-domain settings. By training the model to identify and ignore distractor documents while citing relevant information from pertinent documents, RAFT enhances the model’s reasoning capability and its ability to answer questions based on a specific set of documents.\n\n  \nThe concept draws an analogy to preparing for an open-book exam, where RAFT simulates the conditions of such an exam by incorporating both relevant and irrelevant (distractor) documents during training. This contrasts with existing methods that either do not leverage the opportunity to learn from domain-specific documents or fail to prepare the model for the dynamics of RAG in an open-book test setting.\n\n  \nThe figure below from the paper draws an analogy to how best to prepare for an exam? (a) Fine-tuning based approaches implement “studying” by either directly “memorizing” the input documents or answering practice QA without referencing the documents. (b) Alternatively, incontext retrieval methods fail to leverage the learning opportunity afforded by the fixed domain and are equivalent to taking an open-book exam without studying. While these approaches leverage in-domain learning, they fail to prepare for open-book tests. In contrast, (c) RAFT leverages fine-tuning with question-answer pairs while referencing the documents in a simulated imperfect retrieval setting — thereby effectively preparing for the open-book exam setting.\n\n\n\n\n\n\n\n\n\n  \nThe methodology involves creating training data that includes a question, a set of documents (with one or more being relevant to the question), and a CoT-style answer derived from the relevant document(s). The paper explores the impact of including distractor documents in the training set and the proportion of training data that should contain the oracle document.\n\n  \nThe figure below from the paper shows an overview of RAFT. The top-left figure depicts our approach of adapting LLMs to reading solution from a set of positive and negative documents in contrast to standard RAG setup where models are trained based on the retriever outputs, which is a mixture of both memorization and reading. At test time, all methods follow the standard RAG setting, provided with a top-k retrieved documents in the context.\n\n\n\n\n\n\n\n\n\n  \nExperiments conducted across PubMed, HotpotQA, and Gorilla datasets demonstrate RAFT’s effectiveness. It consistently outperforms both supervised fine-tuning and RAG across these datasets, particularly highlighting the importance of the chain-of-thought (CoT) style responses in improving model performance.\n\n  \nResults from various experiments indicate that mixing a fraction of the training data without the oracle document in its context is beneficial for in-domain RAG tasks. Moreover, training with a balance of relevant and irrelevant documents at test time shows that RAFT can generalize well to different numbers of retrieved documents, enhancing robustness against inaccuracies in retrieval.\n\n  \nRAFT’s approach is compared against several baselines, including LLaMA-7B with and without RAG, domain-specific fine-tuning with 0-shot prompting (DSF), and DSF with RAG. Across different datasets, RAFT demonstrates significant improvements, underscoring its potential in domain-specific applications.\n\n  \nThe paper also discusses related works, highlighting advancements in retrieval-augmented language models, memorization versus generalization in LLMs, and fine-tuning strategies for adapting LLMs to specific tasks. RAFT’s contribution lies in its focus on preparing LLMs for domain-specific RAG by effectively leveraging both relevant and distractor documents during training.\n\n  \nThe study posits RAFT as a valuable strategy for adapting pre-trained LLMs to domain-specific tasks, especially where leveraging external documents is crucial. By training models to discern relevant information from distractors and generating CoT-style answers, RAFT significantly enhances the model’s ability to perform in open-book exam settings, paving the way for more nuanced and effective domain-specific applications of LLMs.\n\n  \nProject page\n; \nCode\n\n\n\n\n\nCorrective Retrieval Augmented Generation\n\n\n\n\n  \nThe paper by Yan et al. from the University of Science and Technology of China, UCLA, and Google Research, proposed Corrective Retrieval Augmented Generation (CRAG) which addresses the challenge of hallucinations and inaccuracies in large language models (LLMs) by proposing a novel framework that enhances the robustness of retrieval-augmented generation (RAG) methods.\n\n  \nCRAG introduces a lightweight retrieval evaluator that assesses the quality of documents retrieved for a query and triggers actions based on a confidence degree, aiming to correct or enhance the retrieval process. The framework also incorporates large-scale web searches to augment the pool of retrieved documents, ensuring a broader spectrum of relevant and accurate information.\n\n  \nA key feature of CRAG is its decompose-then-recompose algorithm, which processes the retrieved documents to highlight crucial information while discarding irrelevant content. This method significantly improves the model’s ability to utilize the retrieved documents effectively, enhancing the quality and accuracy of the generated text.\n\n  \nThe figure below from the paper shows an overview of CRAG at inference. A retrieval evaluator is constructed to evaluate the relevance of the\nretrieved documents to the input, and estimate a confidence degree based on which different knowledge retrieval actions of \n{Correct, Incorrect, Ambiguous}\n can be triggered.\n\n\n\n\n\n\n\n\n\n  \nCRAG is designed to be plug-and-play, allowing seamless integration with various RAG-based approaches. Extensive experiments across four datasets demonstrate CRAG’s ability to significantly enhance the performance of RAG-based methods in both short- and long-form generation tasks, showcasing its adaptability and generalizability.\n\n  \nThe study identifies scenarios where conventional RAG approaches may falter due to inaccurate retrievals. CRAG addresses this by enabling self-correction and efficient utilization of retrieved documents, marking a significant step towards improving the reliability and effectiveness of RAG methods.\n\n  \nLimitations acknowledged include the ongoing challenge of accurately detecting and correcting erroneous knowledge. The necessity of fine-tuning a retrieval evaluator and the potential biases introduced by web searches are highlighted as areas for future improvement.\n\n  \nCode\n\n\n\n\n\nFine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\n\n\n\n\n  \nThis paper by Soudani et al. from Radboud University and the University of Amsterdam investigates the efficacy of Retrieval Augmented Generation (RAG) and fine-tuning (FT) on enhancing the performance of large language models (LLMs) for question answering (QA) tasks involving low-frequency factual knowledge. The authors conducted a comprehensive comparison to determine which approach is more beneficial for customizing LLMs to handle less popular entities, using a dataset characterized by a wide range of entity popularity levels. They found that fine-tuning significantly improves performance across entities of varying popularity, with notable gains in the most and least popular groups. Conversely, RAG was observed to surpass other methods, particularly when combined with FT in smaller models, although its advantage diminishes in base models and is non-existent in larger models.\n\n  \nThe evaluation setup included a diverse range of factors such as model size, retrieval models, quality of synthetic data generation, and fine-tuning method (PEFT vs. full fine-tuning). The findings underscored the importance of advancements in retrieval and data augmentation techniques for the success of both RAG and FT strategies. For FT, two data augmentation methods were used to generate synthetic training data: an End-to-End approach utilizing a model trained for paragraph-level QA generation and a Prompt method using LLMs for QA generation.\n\n  \nFor RAG, various retrieval models were employed to enhance the LLM’s response generation by providing additional context from a document corpus. The performance of the retrieval models played a significant role in the effectiveness of the RAG approach. The study also highlighted the role of synthetic data quality over quantity, with models trained on prompt-generated data outperforming those trained on E2E-generated data.\n\n  \nThe figure below from the paper shows a correlation between subject entity popularity in a question and the effects of RAG and FT on FlanT5-\nsmall performance in open-domain question answering. FT markedly improves accuracy in the initial and final buckets relative to others (indicated by the pink line).\n\n\n\n\n\n\n\n\nHGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation\n\n\n\n\n  \nThis paper by Fang et al. from Queen’s University introduce a novel structured, multi-layered graph approach named Hierarchical Graph of Thoughts (HGOT). This framework aims to mitigate hallucinations in large language models (LLMs) by enhancing the retrieval of relevant information for in-context learning. HGOT uses emergent planning capabilities of LLMs to decompose complex queries into manageable sub-queries. The divide-and-conquer strategy simplifies problem-solving and improves the relevance and accuracy of retrieved information.\n\n  \nHGOT incorporates a unique self-consistency majority voting mechanism for answer selection. This mechanism uses citation recall and precision metrics to evaluate the quality of thoughts, thus directly linking the credibility of an answer to the thought’s quality. The approach employs a scoring mechanism for evaluating retrieved passages, considering citation frequency and quality, self-consistency confidence, and the retrieval module’s ranking.\n\n  \nThe figure below from the paper shows an illustrative example of HGOT in answering a factual question. (The abbreviations employed are as\nfollows: Instr.: Instructions, Q: Question, Ctx.: Context or References, Resp.: ChatGPT’s Response, PL: Plan, D: Dependencies, CI: Confidence, Ans.: Answer, Thot.: Thought)\n\n\n\n\n\n\n\n\n\n  \nThe effectiveness of HGOT is validated against several other retrieval-augmented methods like Demonstrate-Search-Predict (DSP) and ReAct, showing an improvement of up to 7% on datasets such as FEVER, Open-SQuAD, and HotPotQA. This demonstrates HGOT’s enhanced capability for factuality in LLM responses.\n\n  \nIn terms of implementation, HGOT utilizes emergent planning abilities of LLMs to create hierarchical graphs, which organizes the thought process more efficiently and reduces the likelihood of error propagation across multiple reasoning layers. The framework adjusts majority voting by weighting responses based on the quality of their associated citations, and employs a scoring system that factors in multiple qualities of retrieved passages to ensure high-quality, relevant informational support for LLM responses.\n\n\n\n\n\nHow faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs’ internal prior\n\n\n\n\n  \nThis paper by Wu et al. from from Stanford investigates the effectiveness of Retrieval Augmented Generation (RAG) frameworks in moderating the behavior of Large Language Models (LLMs) when confronted with conflicting information. It centers on the dynamic between an LLM’s pre-existing knowledge and the information retrieved via RAG, particularly when discrepancies arise.\n\n  \nThe authors conducted a systematic study using models like GPT-4 and GPT-3.5, simulating scenarios where the models were provided with both accurate and deliberately perturbed information across six distinct datasets. The paper confirms that while correct information typically corrects LLM outputs (with a 94% accuracy rate), incorrect data leads to errors if the model’s internal prior is weak.\n\n  \nThe study introduces a novel experimental setup where documents are systematically modified to test LLM reliance on prior knowledge versus retrieved content. Changes ranged from numerical modifications (e.g., altering drug dosages or dates by specific multipliers or intervals) to categorical shifts in names and locations, assessing model response variations.\n\n  \nThe figure below from the paper shows a schematic of generating modified documents for each dataset. A question is posed to the LLM with and without a reference document containing information relevant to the query. This document is then perturbed to contain modified information and given as context to the LLM. They then observe whether the LLM prefers the modified information or its own prior answer.\n\n\n\n\n\n\n\n\n\n  \nKey findings include an inverse correlation between the likelihood of an LLM adhering to retrieved information and its internal confidence, quantified through token probabilities. Models with stronger priors demonstrated greater resistance to misleading RAG content, reverting to their initial responses.\n\n  \nAdditionally, the paper discusses the influence of different prompting strategies on RAG adherence. The ‘strict’ prompting led to higher reliance on retrieved content, whereas ‘loose’ prompting allowed more independent reasoning from the models, highlighting the importance of prompt design in RAG systems.\n\n  \nResults across the datasets illustrated varying degrees of RAG effectiveness, influenced by the model’s confidence level. This nuanced exploration of RAG dynamics provides insights into improving the reliability of LLMs in practical applications, emphasizing the delicate balance needed in integrating RAG to mitigate errors and hallucinations in model outputs.\n\n\n\n\n\nAdaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\n\n\n\n\n  \nThis paper by Jeong et al. from KAIST presents a novel framework named Adaptive-RAG for dynamic adjustment of retrieval strategies in Large Language Models (LLMs) based on the complexity of incoming queries. This allows for efficient and accurate responses across different query complexities.\n\n  \nThe system categorizes queries into simple, moderate, and complex, each requiring different retrieval strategies: non-retrieval, single-step retrieval, and multi-step retrieval, respectively. The determination of query complexity is facilitated by a classifier trained on automatically labeled data.\n\n  \nThe figure below from the paper shows a conceptual comparison of different retrieval-augmented LLM approaches to question answering. (A) In response to a query, this single-step approach retrieves relevant documents and then generates an answer. However, it may not be sufficient for complex queries that require multi-step reasoning. (B) This multi-step approach iteratively retrieves documents and generates intermediate answers, which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both LLMs and retrievers. (C) Their adaptive approach can select the most suitable strategy for retrieval-augmented LLMs, ranging from iterative, to single, to even no retrieval approaches, based on the complexity of given queries determined by our classifier.\n\n\n\n\n\n\n\n\n\n  \nAdaptive-RAG was validated across multiple open-domain QA datasets, showing significant improvements in both efficiency and accuracy over existing models. It employs a blend of iterative and single-step retrieval processes tailored to the specific needs of a query, which optimizes resource use and response time.\n\n  \nThe implementation utilizes a secondary smaller language model as a classifier to predict query complexity. The classifier is trained on datasets synthesized without human labeling, using model predictions and inherent dataset biases to automatically generate training labels.\n\n  \nExperimental results demonstrate that Adaptive-RAG efficiently allocates resources, handling complex queries with detailed retrieval while effectively answering simpler queries directly through the LLM, thus avoiding unnecessary computation.\n\n  \nAdditionally, Adaptive-RAG’s flexibility is highlighted in its ability to interchange between different retrieval strategies without altering the underlying model architecture or parameters, providing a scalable solution adaptable to varied query complexities.\n\n\n\n\n\nRichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation\n\n\n\n\n  \nThis paper by Wang et al. from the Gaoling School of Artificial Intelligence, Renmin University of China, and Baichuan Intelligent Technology, addresses the need for rich and comprehensive responses to broad, open-ended queries in retrieval-augmented generation (RAG).\n\n  \nThe authors propose a novel framework, RichRAG, to handle complex user queries that have multiple sub-intents. RichRAG consists of three main components: a sub-aspect explorer, a multi-faceted retriever, and a generative list-wise ranker.\n\n  \nThe sub-aspect explorer identifies potential sub-aspects of the input queries. This module leverages large language models (LLMs) for their extensive world knowledge and language understanding capabilities. It generates sub-aspects by fine-tuning on training queries using a next token prediction (NTP) loss function.\n\n  \nThe multi-faceted retriever builds a candidate pool of external documents related to the identified sub-aspects. It retrieves top-N documents for each sub-aspect and combines these into a diverse candidate pool, ensuring broad coverage of the query’s various aspects.\n\n  \nThe generative list-wise ranker sorts the top-k most valuable documents from the candidate pool. Built on a seq-to-seq model structure (T5), it models global interactions among candidates and sub-aspects, using a parallel encoding process and a pooling operation to extract relevance representations. The ranker generates a list of document IDs optimized through supervised fine-tuning and reinforcement learning stages.\n\n  \nThe supervised fine-tuning stage uses a greedy algorithm to build silver target ranking lists based on a coverage utility function, ensuring the ranker can generate comprehensive lists.\n\n  \nThe reinforcement learning stage aligns the ranker’s output with LLM preferences by using a reward function based on the quality and coverage of the generated responses. The Direct Preference Optimization (DPO) algorithm is employed, with training pairs created through a unilateral significance sampling strategy (US3) to ensure valuable and reliable training data.\n\n  \nThe figure below from the paper illustrates the overall framework of RichRAG. We describe the training stages of our ranker at the bottom.\n\n\n\n\n\n\n\n\n\n  \nExperimental results on WikiPassageQA and WikiAsp datasets demonstrate RichRAG’s effectiveness in generating comprehensive responses. The framework shows superior performance in terms of Rouge and Com-Rouge scores compared to existing methods.\n\n  \nRichRAG significantly improves the quality of responses to multi-faceted queries by explicitly modeling sub-aspects and aligning ranking lists with LLM preferences. The efficiency and robustness of the ranker are validated through various experiments, confirming its advantage in handling complex search scenarios.\n\n\n\n\n\nHiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA\n\n\n\n\n  \nThis paper by Chen et al. from introduces HiQA, an advanced multi-document question-answering (MDQA) framework to tackle the challenge of retrieving accurate information from extensive, indistinguishable documents. It incorporates cascading metadata and a multi-route retrieval mechanism to enhance the precision and relevance of knowledge retrieval.\n\n  \nThe paper outlines the methodology comprising three main components: Markdown Formatter (MF), Hierarchical Contextual Augmentor (HCA), and Multi-Route Retriever (MRR). MF converts documents into markdown format, enriching them with structured metadata. HCA further augments document segments with hierarchical metadata, and MRR utilizes a combination of vector similarity, Elasticsearch, and keyword matching for improved retrieval accuracy.\n\n  \nThe following figure from the paper illustrates of the proposed contextual text enhancement. The contextual structure can improve text alignment with the query for better matching in multi-documents scenarios.\n\n\n\n\n\n\n\n\n\n  \nA novel dataset, MasQA, is introduced to evaluate the performance of MDQA systems, highlighting the framework’s superiority in handling massive documents through extensive experiments.\n\n  \nAblation studies demonstrate the individual contribution of each component to the system’s overall effectiveness, with a focus on the HCA’s role in improving retrieval precision.\n\n  \nTheoretical exploration into the impact of HCA on the distribution of document segments within the embedding space supports the framework’s approach, indicating enhanced retrieval accuracy and the avoidance of information loss associated with hard partitioning methods.\n\n\n\n\n\nReferences\n\n\n\n\n  \nDocument Similarity Search with [ColPali](https://arxiv.org/abs/2407.01449)\n\n  \nLate Chunking: Balancing Precision and Cost in Long Context Retrieval\n\n  \nLate Chunking in Long-Context Embedding Models\n\n  \nWeaviate Blog: What is Agentic RAG?\n\n  \nAnthropic: Introducing Contextual Retrieval\n\n\n\n\n\nCitation\n\n\n\n@article{Chadha2020DistilledRAG,\n  title   = {Retrieval Augmented Generation},\n  author  = {Chadha, Aman and Jain, Vinija},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/RAG/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • AI\n\n  \n\n\n  \n\n  \nOverview\n\n\n\n\n  \nHere’s a hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.\n\n\n\n\n\nAlgorithms/Architecture\n\n\n\n\n  \nLinear and Logistic Regression\n\n  \nk-Nearest Neighbors\n\n  \nClustering\n\n  \nSupport Vector Machines (SVM)\n\n  \nNaive Bayes\n\n  \nDecision Trees and Ensemble Methods\n\n  \nML Algorithms Comparative Analysis\n\n  \nDL Architectures Comparative Analysis\n\n  \nPrompt Engineering\n\n  \nGenerative Adversarial Networks (GANs)\n\n  \nDiffusion Models\n\n  \nGraph Neural Networks\n\n  \nAttention\n\n  \nSeparable Convolutions\n\n  \nInductive Bias\n\n  \nConvolutional Neural Networks\n\n  \nReinforcement Learning\n\n  \nMixture-of-Experts\n\n  \nState Space Models\n\n  \nAgents\n\n  \nQuantization\n\n  \nModel Acceleration\n\n  \nCross Validation\n\n\n\n\n\nData/Training\n\n\n\n\n  \nData Sampling\n\n  \nData Imbalance\n\n  \nStandardization vs. Normalization\n\n  \nLearning Paradigms\n\n  \nXavier Initialization\n\n  \nPadding and Packing\n\n  \nRegularization\n\n  \nGradient Descent and Backprop\n\n  \nActivation Functions\n\n  \nLoss Functions\n\n  \nActivation Functions\n\n  \nFine-tuning Models\n\n  \nSplitting Datasets\n\n  \nBatchnorm\n\n  \nDropout\n\n  \nDouble Descent\n\n  \nFine-Tuning and Evaluating BERT\n\n\n\n  \nTraining Loss > Validation Loss?\n\n  \nSVM Kernel/Polynomial Trick\n\n  \nBias Variance Tradeoff\n\n  \nGradient Accumulation and Checkpointing\n\n  \nParameter Efficient Fine-Tuning\n\n  \nHypernetworks\n\n  \nDistributed Training Parallelism\n\n\n\n\n\nSpeech\n\n\n\n\n  \nSpeech Processing\n\n\n\n\n\nVision\n\n\n\n\n  \nVision Transformer (ViT)\n\n  \nReceptive Field\n\n  \nResidual Networks/Skip Connections\n\n  \nGPT-4o Native Image Generation\n\n\n\n\n\nNLP\n\n\n\n\n  \nWord Vectors/Embeddings\n\n  \nNLP Tasks\n\n  \nPreprocessing\n\n  \nTokenization\n\n  \nData Sampling\n\n  \nNeural Architectures\n\n  \nAttention\n\n  \nTransformers\n\n  \nToken Sampling Methods\n\n  \nEncoder vs. Decoder vs. Encoder-Decoder Models\n\n\n\n  \nOverview of Large Language Models (LLMs)\n\n  \nReinforcement Fine-Tuning\n\n  \nPreference Optimization\n\n  \nMachine Translation\n\n  \nKnowledge Graphs\n\n  \nHallucination Detection and Mitigation\n\n  \nAI Text Detection Techniques\n\n  \nNamed Entity Recognition\n\n  \nTextual Entailment\n\n  \nRetrieval Augmented Generation (RAG)\n\n  \nLLM Context Length Extension\n\n  \nDocument Intelligence\n\n\n\n  \nCode Mixing and Switching\n\n  \nLarge Language Model Ops (LLMOps)\n\n  \nLLM/VLM Benchmarks\n\n\n\n\n\nMultimodal\n\n\n\n\n  \nOverview of Vision-Language Models (VLMs)\n\n  \nVLM Architectures\n\n  \nComputer Control\n\n\n\n\n\nModels\n\n\n\n\n  \nBERT\n\n  \nGPT\n\n  \nCLIP\n\n  \nMeena\n\n  \nChatGPT\n\n  \nGPT-4\n\n  \nLLaMA\n\n  \nAlpaca\n\n  \nGemini\n\n  \nToolformer\n\n  \nVisual ChatGPT\n\n  \nTaskMatrix.AI\n\n  \nBigBird\n\n  \nOpenAI o1\n\n  \nDeepSeek R1\n\n  \nDeepSeek Janus-Pro\n\n  \nGemma 3n\n\n\n\n\n\nOffline/Online Evaluation\n\n\n\n\n  \nEvaluation Metrics\n\n  \nF-Beta Score\n\n  \nA/B Testing\n\n\n\n\n\nMLOps\n\n\n\n\n  \nData Drift\n\n  \nMLOps Tooling\n\n  \nMLOps Testing\n\n\n\n\n\nOn-Device AI\n\n\n\n\n  \nModel Compression\n\n  \nPersonally Identifiable Information (PII)\n\n  \nFederated Learning\n\n  \nDifferential Privacy\n\n\n\n\n\nProject Planning, Scheduling, Execution\n\n\n\n\n  \nObjectives and Key Results (OKRs)\n\n  \nRICE Framework\n\n  \nGantt Charts\n\n  \nProject Management\n\n\n\n\n\nMiscellaneous\n\n\n\n\n  \nIlya Sutskever’s Top 30\n\n  \nDebugging Model Training\n\n  \nChain Rule\n\n  \nBayes’ Theorem\n\n  \nProbability Calibration\n\n  \nMulticlass vs. Multilabel Classification\n\n  \nN-Dimensional Tensor Product\n\n  \nPyTorch vs. TensorFlow\n\n  \nApproximate Nearest Neighbors – Similarity Search\n\n  \nTransferability Estimation\n\n  \nTensorBoard\n\n  \nConvolutional Neural Networks for Text Classification\n\n  \nRelationship between Hidden Markov Models and Naive Bayes\n\n  \nMaximum Entropy Markov Models\n\n  \nConditional Random Fields\n\n\n\n\n\n\n\nHyperparameters\n\n\n\n\n  \nHyperparameter Tuning\n\n  \nHyperparameter Logging\n\n\n\n\n\nPractice\n\n\n\n\n  \nInterview Questions\n\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/"},
{"page_content": "Toggle navigation\n\n                  \n\n                  \n\n                  \n\n                  \n\n                  \n\n                  \naman.ai\n\n               \n\n               \n\n               \n\n                  \n\n                     \n\n                     \n\n                     \n\n                     \n\n                     \n\n                     \n\n                  \n\n               \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \n\n            \naman.ai\n\n            \nthe art of artificial intelligence\n\n               one concept at a time\n            \n\n         \n\n      \n\n      \n\n      \n\n      \n\n         \n            \n         \n\n            \nDistilled AI\n\n            \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nAlgorithms/Architecture\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nLinear and Logistic Regression\n\n                           \n\n                           \n\n                              \n• \nk-Nearest Neighbors\n\n                           \n\n                           \n\n                              \n• \nClustering\n\n                           \n\n                           \n\n                              \n• \nSupport Vector Machines (SVM)\n\n                           \n\n                           \n\n                              \n• \nNaive Bayes\n\n                           \n\n                           \n\n                              \n• \nDecision Trees and Ensemble Methods\n\n                           \n\n                           \n\n                              \n• \nML Algorithms Comparative Analysis\n\n                           \n\n                           \n\n                              \n• \nDL Architectures Comparative Analysis\n\n                           \n\n                           \n\n                              \n• \nPrompt Engineering\n\n                           \n\n                           \n\n                              \n• \nGenerative Adversarial Networks (GANs)\n\n                           \n\n                           \n\n                              \n• \nDiffusion Models\n\n                           \n\n                           \n\n                              \n• \nGraph Neural Networks\n\n                           \n\n                           \n\n                              \n• \nAttention\n\n                           \n\n                           \n\n                              \n• \nSeparable Convolutions\n\n                           \n\n                           \n\n                              \n• \nInductive Bias\n\n                           \n\n                           \n\n                              \n• \nConvolutional Neural Networks (CNNs)\n\n                           \n\n                           \n\n                              \n• \nReinforcement Learning\n\n                           \n\n                           \n\n                              \n• \nMixture-of-Experts\n\n                           \n\n                           \n\n                              \n• \nState Space Models\n\n                           \n\n                           \n\n                              \n• \nAgents\n\n                           \n\n                           \n\n                              \n• \nQuantization\n\n                           \n\n                           \n\n                              \n• \nModel Acceleration\n\n                           \n\n                           \n\n                              \n• \nCross Validation\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nData/Training\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nData Sampling\n\n                           \n\n                           \n\n                              \n• \nData Imbalance\n\n                           \n\n                           \n\n                              \n• \nStandardization vs. Normalization\n\n                           \n\n                           \n\n                              \n• \nLearning Paradigms\n\n                           \n\n                           \n\n                              \n• \nXavier Initialization\n\n                           \n\n                           \n\n                              \n• \nPadding and Packing\n\n                           \n\n                           \n\n                              \n• \nRegularization\n\n                           \n\n                           \n\n                              \n• \nGradient Descent & Backprop\n\n                           \n\n                           \n\n                              \n• \nActivation Functions\n\n                           \n\n                           \n\n                              \n• \nLoss Functions\n\n                           \n\n                           \n\n                              \n• \nFine-tuning Models\n\n                           \n\n                           \n\n                              \n• \nSplitting Datasets\n\n                           \n\n                           \n\n                              \n• \nBatchnorm\n\n                           \n\n                           \n\n                              \n• \nDropout\n\n                           \n\n                           \n\n                              \n• \nDouble Descent\n\n                           \n\n                           \n\n                              \n• \nFine-tuning and Evaluating BERT\n\n                           \n\n                           \n\n                              \n• \nTraining Loss > Validation Loss?\n\n                           \n\n                           \n\n                              \n• \nSVM Kernel/Polynomial Trick\n\n                           \n\n                           \n\n                              \n• \nBias Variance Tradeoff\n\n                           \n\n                           \n\n                              \n• \nGradient Accumulation & Checkpointing\n\n                           \n\n                           \n\n                              \n• \nParameter Efficient Fine-Tuning\n\n                           \n\n                           \n\n                              \n• \nHypernetworks\n\n                           \n\n                           \n\n                              \n• \nDistributed Training Parallelism\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nNLP\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nWord Vectors/Embeddings\n\n                           \n\n                           \n\n                              \n• \nNLP Tasks\n\n                           \n\n                           \n\n                              \n• \nPreprocessing\n\n                           \n\n                           \n\n                              \n• \nTokenization\n\n                           \n\n                           \n\n                              \n• \nData Sampling\n\n                           \n\n                           \n\n                              \n• \nNeural Architectures\n\n                           \n\n                           \n\n                              \n• \nAttention\n\n                           \n\n                           \n\n                              \n• \nTransformers\n\n                           \n\n                           \n\n                              \n• \nToken Sampling Methods\n\n                           \n\n                           \n\n                              \n• \nEncoder vs. Decoder vs. Encoder-Decoder\n\n                           \n\n                           \n\n                           \n\n                              \n• \nLarge Language Models (LLMs)\n\n                           \n\n                           \n\n                              \n• \nPreference Optimization\n\n                           \n\n                           \n\n                              \n• \nReinforcement Fine-Tuning\n\n                           \n                           \n                           \n\n                              \n• \nMachine Translation\n\n                           \n\n                           \n\n                              \n• \nKnowledge Graphs\n\n                           \n\n                           \n\n                              \n• \nHallucination Detection and Mitigation\n\n                           \n\n                           \n\n                              \n• \nAI Text Detection Techniques\n\n                           \n\n                           \n\n                              \n• \nNamed Entity Recognition\n\n                           \n\n                           \n\n                              \n• \nTextual Entailment\n\n                           \n\n                           \n\n                              \n• \nRetrieval Augmented Generation (RAG)\n\n                           \n\n                           \n\n                              \n• \nLLM Context Length Extension\n\n                           \n\n                           \n\n                              \n• \nDocument Intelligence\n\n                           \n\n                           \n\n                           \n\n                              \n• \nCode Mixing and Switching\n\n                           \n\n                           \n\n                              \n• \nLarge Language Model Ops (LLMOps)\n\n                           \n\n                           \n\n                              \n• \nLLM/VLM Benchmarks\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nSpeech\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nSpeech Processing\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nVision\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nVision Transformer (ViT)\n\n                           \n                           \n                           \n\n                              \n• \nReceptive Field\n\n                           \n\n                           \n\n                              \n• \nSkip Connections/ResNet\n\n                           \n\n                           \n\n                              \n• \nGPT-4o Native Image Generation\n\n                           \n                           \n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nMultimodal\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nVision Language Models (VLMs)\n\n                           \n\n                           \n\n                              \n• \nVLM Architectures\n\n                           \n\n                           \n\n                              \n• \nComputer Control\n\n                           \n                           \n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nEvaluation\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nEvaluation Metrics\n\n                           \n\n                           \n\n                              \n• \nF-Beta Score\n\n                           \n\n                           \n\n                              \n• \nA/B Testing\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nPractice\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nInterview Questions\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nMLOps\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nData Drift\n\n                           \n\n                           \n\n                              \n• \nMLOps Tooling\n\n                           \n\n                           \n\n                              \n• \nMLOps Testing\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nOn-Device AI\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nModel Compression\n\n                           \n\n                           \n\n                              \n• \nHandling PII Data\n\n                           \n\n                           \n\n                              \n• \nFederated Learning\n\n                           \n\n                           \n\n                              \n• \nDifferential Privacy\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nHyperparameters\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nTuning\n\n                           \n\n                           \n\n                              \n• \nLogging\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nProject Planning/Execution\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nOKR Framework\n\n                           \n\n                           \n\n                              \n• \nRICE Framework\n\n                           \n\n                           \n\n                              \n• \nGantt Charts\n\n                           \n\n                           \n\n                              \n• \nProject Management\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  \n\n                     \n\n                        \n\n                           \n\n                              \nModels\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nBERT\n\n                           \n\n                           \n\n                              \n• \nGPT\n\n                           \n\n                           \n\n                              \n• \nCLIP\n\n                           \n                           \n                           \n\n                              \n• \nMeena\n\n                           \n\n                           \n\n                              \n• \nChatGPT\n\n                           \n\n                           \n\n                              \n• \nGPT-4\n\n                           \n\n                           \n\n                              \n• \nLLaMA\n\n                           \n\n                           \n\n                              \n• \nAlpaca\n\n                           \n\n                           \n\n                              \n• \nGemini\n\n                           \n\n                           \n\n                              \n• \nToolformer\n\n                           \n\n                           \n\n                              \n• \nVisual ChatGPT\n\n                           \n\n                           \n\n                              \n• \nTaskMatrix.AI\n\n                           \n\n                           \n\n                              \n• \nBigBird\n\n                           \n\n                           \n\n                              \n• \nOpenAI o1\n\n                           \n\n                           \n\n                              \n• \nDeepSeek R1\n\n                           \n\n                           \n\n                              \n• \nDeepSeek Janus-Pro\n\n                           \n\n                           \n\n                              \n• \nGemma 3n\n\n                           \n                              \n                        \n\n                     \n\n                  \n\n               \n\n               \n\n               \n\n                  \n\n                     \n                     \n                     \n\n                        \n\n                           \n\n                              \nMiscellaneous\n\n                           \n\n                        \n\n                        \n\n                           \n\n                              \n• \nIlya Sutskever's Top 30\n\n                           \n\n                           \n\n                              \n• \nDebugging Model Training\n\n                           \n\n                           \n\n                              \n• \nChain Rule\n\n                           \n\n                           \n\n                              \n• \nBayes' Theorem\n\n                           \n\n                           \n\n                              \n• \nProbability Calibration\n\n                           \n\n                           \n\n                              \n• \nMulticlass vs. Multilabel Classification\n\n                           \n\n                           \n\n                              \n• \nN-Dimensional Tensor Product\n\n                           \n\n                           \n\n                              \n• \nPyTorch vs. TensorFlow\n\n                           \n\n                           \n\n                              \n• \nApproximate Nearest Neighbors\n\n                           \n\n                           \n\n                              \n• \nTransferability Estimation\n\n                           \n\n                           \n\n                              \n• \nTensorBoard\n\n                           \n\n                           \n\n                              \n• \nHidden Markov Models vs. Naive Bayes\n\n                           \n\n                           \n\n                              \n• \nCNNs for Text Classification\n\n                           \n\n                           \n\n                              \n• \nMaximum Entropy Markov Models\n\n                           \n\n                           \n\n                              \n• \nConditional Random Fields\n\n                           \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                  .miscellaneous {\n                  display: table; /* Shrinks the div to fit the content */\n                  margin: 0 auto 20px; /* Centers the div and adds bottom margin */\n                  padding: 10px; /* Adds inner padding */\n                  text-align: center; /* Aligns content inside */\n                  }\n                  .tg {\n                  table-layout: auto; /* Allows the table to adjust its width dynamically */\n                  white-space: nowrap; /* Ensures no content wraps to the next line */\n                  }\n                  .tg td {\n                  white-space: nowrap; /* Prevents table cell content from wrapping */\n                  }\n               \n\n            \n\n            \n\n               @media screen and (max-width: 768px) {\n               .tg thead {\n               display: table-header-group;\n               }\n               .tg thead th {\n               text-align: center; /* Center-align headers on mobile */\n               }\n               .tg tbody tr td {\n               display: block;\n               text-align: left;\n               margin: -0.5px 0; /* Further reduced line spacing */\n               border-bottom: none;\n               }\n               }\n            \n\n         \n\n      \n\n      \n\n         \n\n      \n\n      \n\n         \n            \n         \n\n            \nStanford AI Courses\n\n            \n\n            \n\n               \n\n                  \n\n                  \nStanford CS229\n\n                  \nMachine Learning\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nStanford CS230\n\n                  \nDeep Learning\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nStanford CS231n\n\n                  \nConvolutional Neural Networks\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nStanford CS224n\n\n                  \nNatural Language Processing\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nCoursera Machine Learning Specialization\n\n                  \nSupervised ML, Unsupervised ML, Advanced ML Algos\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nRecommendation Systems\n\n                  \nCandidate Generation, Ranking, Retrieval, Code Deep-dive\n\n               \n\n            \n\n            \n\n            \n\n               \n\n                  \n\n                  \nCoursera Deep Learning Specialization\n\n                  \nBasics, Hyperparams, Structuring Projects, ConvNets, Sequential Models\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nCoursera Natural Language Specialization\n\n                  \nClassification Models, Vector Space Models, Sequence Models, Attention Models\n\n               \n\n            \n\n            \n           \n            \n        \n            \n\n               \n\n                  \n\n                  \nMultimodal Machine Learning\n\n                  \nRepresentations, Fusion Techniques, Co-learning\n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \n\n      \n\n      \n\n      \n\n         \n\n         \n\n            \nPrimers\n\n            \n\n               \n\n                  \n\n                  \nAI Fundamentals\n\n                  \nConcepts, Definitions, Terms\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nGraph Studio\n\n                  \nPlots for common functions\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nNumPy\n\n                  \nNumerical processing in Python\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nMatplotlib\n\n                  \nPlotting data in Python\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nPandas\n\n                  \nData analysis in Python\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nPython 3\n\n                  \nBasics to advanced\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nPyTorch\n\n                  \nTorch building blocks\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nTensorFlow\n\n                  \nTF2 basics\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nBackprop Guide\n\n                  \nPartial gradient derivations for common layers/loss functions\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nMath\n\n                  \nLinear algebra, differential calculus, distributions\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nRegular Expressions Cheatsheet\n\n                  \nMine data easily with RegEx\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nGit Tutorial\n\n                  \nTame the world's most popular source control beast!\n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \n\n      \n\n      \n\n         \n\n         \n\n            \nCoding\n\n            \n\n               \n\n                  \n\n                  \nAlgorithms\n\n                  \nImplementations in Python 3\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nData Structures\n\n                  \nPython Data Structures and Time Complexities\n\n               \n\n            \n\n            \n                       \n            \n\n               \n\n                  \n\n                  \nAsymptotic Notations\n\n                  \nBig-O, Big-Omega, Small-o, Small-omega, Theta\n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \n\n      \n\n      \n\n         \n\n         \n\n            \nLists\n\n            \n\n               \n\n                  \n\n                  \nPapers List\n\n                  \nResearch papers to review for the latest scoop in AI/ML\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nRead List\n\n                  \nNotes, books, and blogs to build intuition\n\n               \n\n            \n\n            \n\n               \n\n                  \n\n                  \nWatch List\n\n                  \nVideo tutorials to learn AI fundamentals\n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \n\n      \n\n      \n\n      \n\n      \n\n      \n\n         \n\n            \n\n               © \ndocument.write(new Date().getFullYear());\n  Aman Chadha • \namanchadha.com\n • \nPrivacy Policy\n • \nFulfillment Policy", "source": "https://aman.ai"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Approximate Nearest Neighbors -- Similarity Search\n\n  \n\n\n  \n\n  \n\n  \nWhat is Similarity Search?\n    \n\n      \nReal-World Applications\n\n      \nFrom Exact to Approximate Nearest Neighbor Search\n\n    \n\n  \n\n  \nApproximate Nearest Neighbors (ANN)\n    \n\n      \nRole of ANN in Recommendation Systems\n\n    \n\n  \n\n  \nANN Algorithms\n    \n\n      \nTree-Based Methods\n        \n\n          \nKD-Trees\n\n          \nRandomized KD-Forests\n\n          \nAnnoy’s Random Projection Forest\n\n        \n\n      \n\n      \nQuantization-Based Methods\n        \n\n          \nProduct Quantization (PQ)\n\n          \nOptimized Product Quantization (OPQ)\n\n          \nLocality Sensitive Hashing (LSH)\n\n          \nAnisotropic Vector Quantization (AVQ)\n\n        \n\n      \n\n      \nClustering-Based Methods\n        \n\n          \nInverted File Index (IVF)\n\n          \nResidual Vector Quantization (RVQ)\n\n          \nScalable K-Means Clustering (Mini-Batch K-Means)\n\n        \n\n      \n\n      \nGraph-Based Methods\n        \n\n          \nNavigable Small Worlds (NSW)\n\n          \nFast Inference for Graph-Based ANN (FINGER)\n\n          \nHierarchical Navigable Small Worlds (HNSW)\n\n        \n\n      \n\n      \nTabular Comparison\n\n      \nChoosing the right ANN algorithm family\n\n    \n\n  \n\n  \nANN Libraries\n    \n\n      \nFAISS (Facebook AI Similarity Search)\n        \n\n          \nKey Features\n\n          \nSearch Workflow\n\n          \nEvaluation Metrics\n\n          \nPros\n\n          \nCons\n\n        \n\n      \n\n      \nScaNN (Scalable Nearest Neighbors)\n        \n\n          \nKey Features\n\n          \nAnisotropic Vector Quantization (AVQ)\n\n          \nUse Case: Semantic Search\n\n          \nImplementation Considerations\n\n          \nPros\n\n          \nCons\n\n        \n\n      \n\n      \nANNOY (Approximate Nearest Neighbors Oh Yeah)\n        \n\n          \nKey Features\n\n          \nUse Case: Music Recommendation\n\n          \nImplementation Notes\n\n          \nPros\n\n          \nCons\n\n        \n\n      \n\n    \n\n  \n\n  \nComparative Analysis\n    \n\n      \nTabular Comparison\n\n    \n\n  \n\n  \nANN-Benchmarks\n    \n\n      \nPurpose and Scope\n\n      \nKey Features\n\n      \nUsing ANN-Benchmarks\n\n      \nPractical Use Cases\n\n    \n\n  \n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nWhat is Similarity Search?\n\n\n\n\n  \nSimilarity search is a fundamental computational technique that enables the retrieval of items from a dataset that are most similar to a given query item. Instead of relying solely on exact matches, similarity search evaluates how closely data records resemble each other based on their semantic content. This capability underpins numerous real-world applications across diverse domains.\n\n\n\n\n\nReal-World Applications\n\n\n\n\n  \n\n    \nIn modern machine learning and artificial intelligence systems, similarity search serves as a core component in scenarios such as:\n\n\n    \n\n      \nImage retrieval\n: Given a sample image, retrieve visually or contextually similar images from a database (e.g., identifying similar fashion items or duplicate product listings).\n\n      \nDocument retrieval\n: Powering search engines that return semantically relevant documents in response to a textual query.\n\n      \nRecommendation systems\n: Suggesting products, movies, or music that align with a user’s preferences by identifying similar users or items.\n\n      \nFacial recognition and biometric systems\n: Matching a given face or fingerprint to a stored identity profile.\n\n      \nMedical imaging\n: Retrieving similar radiology scans to assist in diagnosis by comparing new cases with historical datasets.\n\n    \n\n  \n\n  \n\n    \nThese applications share a common requirement: the ability to measure and compare the semantic similarity between high-dimensional data objects such as images, text, audio, or even multi-modal inputs.\n\n  \n\n\n\n\n\nFrom Exact to Approximate Nearest Neighbor Search\n\n\n\n\n  \n\n    \nAt the heart of similarity search is the concept of nearest neighbor (NN) search. This involves representing each item (e.g., a document, image, or user profile) as a high-dimensional vector using embeddings derived from machine learning models. The task is to find the nearest vectors to a query vector using a defined distance metric, commonly Euclidean distance or cosine similarity.\n\n  \n\n  \n\n    \nExact Nearest Neighbor (ENN) search\n involves scanning all vectors in the dataset to compute distances to the query vector, and then selecting the closest ones. While accurate, this method becomes computationally expensive and inefficient as the dataset size and dimensionality grow.\n\n  \n\n  \n\n    \nApproximate Nearest Neighbor (ANN) search\n offers a scalable alternative by sacrificing a small degree of accuracy to achieve significant gains in speed and memory efficiency. ANN methods use specialized data structures and algorithms to rapidly locate vectors that are close to the query vector, without exhaustively comparing all options.\n\n  \n\n  \n\n    \nThis trade-off makes ANN especially valuable in real-time applications where speed and scalability are paramount, such as live recommendation engines or large-scale search systems.\n\n  \n\n  \nTo compare and evaluate the performance of different ANN methods, practitioners often refer to standardized benchmarks such as \nANN-Benchmarks\n, which test algorithms across datasets and metrics.\n\n  \nThis primer will focus on the exploration of Approximate Nearest Neighbors in greater detail.\n\n\n\n\n\nApproximate Nearest Neighbors (ANN)\n\n\n\n\n  \nApproximate Nearest Neighbor (ANN) techniques are crucial for enabling fast and scalable similarity search, especially when working with large, high-dimensional datasets. These methods are designed to retrieve near-optimal neighbors for a query point without the computational overhead of exhaustively comparing against every item in the dataset. By slightly compromising on accuracy, ANN methods dramatically improve search speed and memory efficiency.\n\n\n\n\n\nRole of ANN in Recommendation Systems\n\n\n\n\n  \n\n    \nANN methods are widely integrated into modern recommender systems to complement or enhance traditional collaborative and content-based filtering. Their utility arises from addressing several limitations of classical approaches, particularly in large-scale or real-time environments.\nTo understand their impact more concretely, consider how ANN techniques address four critical challenges in recommender system design: scalability, latency, diversity, and the cold start problem:\n\n\n    \n\n      \n\n        \nScalability\n: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.\n\n      \n\n      \n\n        \nReal-Time Recommendation\n: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.\n\n      \n\n      \n\n        \nDiversity and Serendipity\n: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.\n\n      \n\n      \n\n        \nCold Start Handling\n: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.\n\n      \n\n    \n\n  \n\n  \n\n    \nBy integrating ANN techniques into recommender architectures, systems achieve enhanced scalability, responsiveness, recommendation diversity, and robustness to data sparsity—all critical for modern, dynamic applications.\n\n  \n\n\n\n\n\nANN Algorithms\n\n\n\n\n  \nANN search is underpinned by a range of algorithmic strategies designed to support scalable, high-speed querying in high-dimensional spaces. These algorithms are broadly grouped into graph-based, tree-based, quantization-based, and clustering-based families. Each comes with specific implementation considerations, data structures, and optimization techniques.\n\n  \nThe ANN algorithms below have been sorted from simplest to most complex based on their underlying strategy types—starting with tree-based methods (e.g., KD-Tree, Annoy), followed by quantization-based methods (e.g., PQ, OPQ), clustering-based methods (e.g., IVF, RVQ), and finally graph-based methods (e.g., HNSW, FINGER) in the later rows.\n\n\n\n\n\nTree-Based Methods\n\n\n\n\n  \nTree-based methods form one of the earliest and most widely adopted strategies for nearest neighbor search. They operate by recursively dividing the vector space using hyperplanes (such as axis-aligned or randomly oriented splits), resulting in hierarchical structures like binary trees or forests. During querying, these structures enable the algorithm to efficiently prune large portions of the search space, reducing the number of distance computations required.\n\n  \n\n    \nIn real-world scenarios, tree-based methods are frequently used in settings where:\n\n\n    \n\n      \nDimensionality is moderate\n (typically under 100 dimensions), and datasets are either static or change infrequently.\n\n      \nSpeed and interpretability are critical\n, such as in robotics, computer vision, and industrial inspection systems, where tree traversal paths can be inspected or constrained.\n\n      \nLow-latency and real-time processing is required\n, such as in SLAM (Simultaneous Localization and Mapping), object recognition in AR/VR applications, or drone navigation systems, where rapid approximate lookups are essential.\n\n      \nEmbedded or resource-constrained environments\n are involved, where lightweight in-memory or file-based indices (as supported by Annoy) are more suitable than GPU-intensive solutions.\n\n    \n\n  \n\n  \nTree-based methods also serve as foundational components in hybrid pipelines—often combined with graph-based or quantization-based modules to accelerate coarse filtering before refined re-ranking stages.\n\n  \nDespite their limitations in high-dimensional contexts, their speed, simplicity, and practical effectiveness make them a valuable tool in many production-grade systems.\n\n\n\n\n\nKD-Trees\n\n\n\n\n  \n\n    \nPartition Strategy\n: Recursive binary splits along dimensions with maximum variance.\n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nTraverses down the tree to a leaf.\n\n      \nBacktracking is used to check sibling branches when necessary.\n\n    \n\n  \n\n  \n\n    \nImplementation\n:\n\n\n    \n\n      \nFast for dimensions < 30.\n\n      \nCan use priority queues to simulate best-first search.\n\n      \nCommon in libraries like FLANN (Fast Library for Approximate Nearest Neighbors).\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nSimple and interpretable structure.\n\n      \nLow memory overhead.\n\n      \nFast for low-dimensional, static datasets.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nPerformance degrades rapidly in high-dimensional spaces.\n\n      \nNot suitable for dynamic datasets or frequent updates.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nKD-Trees are suited for structured and low-dimensional datasets such as geospatial data, robotics, and image metadata. They are widely used in scientific computing and real-time systems requiring deterministic and interpretable behavior. They’re ideal for exact or nearly exact queries in small- to medium-sized vector collections.\n\n    \n\n  \n\n\n\n\n\nRandomized KD-Forests\n\n\n\n\n  \n\n    \nMultiple KD-Trees\n: Built with randomized split choices or axis shuffling.\n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nQueries traverse multiple trees in parallel.\n\n      \nResults from all trees are merged.\n\n    \n\n  \n\n  \n\n    \nParameters\n:\n\n\n    \n\n      \nn_trees\n: Number of trees to build (impacts accuracy and index size).\n\n      \nleaf_size\n: Minimum number of items in leaf nodes.\n\n      \nsearch_checks\n: Max nodes visited during querying.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nImproves robustness over standard KD-trees.\n\n      \nOffers tunable accuracy-speed trade-offs.\n\n      \nBetter suited to moderate dimensionality (up to ~100 dimensions).\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nHigher memory footprint due to multiple trees.\n\n      \nRequires careful tuning to avoid overfitting or redundancy.\n\n      \nPerformance still degrades in high-dimensional regimes.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nRandomized KD-Forests are effective in handling medium-scale datasets where standard KD-trees fail due to dimensionality. They’re often used in real-time search applications like 3D point cloud matching and visual SLAM in robotics, offering a balance between accuracy and responsiveness.\n\n    \n\n  \n\n\n\n\n\nAnnoy’s Random Projection Forest\n\n\n\n\n  \n\n    \nPartition Strategy\n:\n\n\n    \n\n      \nRandomly select two points, compute their midpoint hyperplane, and recursively partition the dataset.\n\n    \n\n  \n\n  \n\n    \nIndex\n:\n\n\n    \n\n      \nA forest of such trees is constructed.\n\n      \nEach tree introduces different hyperplane splits, improving diversity.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nEach tree yields a list of candidate points.\n\n      \nCombined and sorted using brute-force on small candidate sets.\n\n    \n\n  \n\n  \n\n    \nFile-based Indexing\n:\n\n\n    \n\n      \nIndexes saved to disk as static files and memory-mapped at query time.\n\n      \nEfficient for multi-process access and low RAM environments.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nMemory-efficient via on-disk index loading.\n\n      \nSupports shared access across processes.\n\n      \nSimple to implement and tune (\nn_trees\n, \nsearch_k\n).\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nIndex is static—no support for incremental updates.\n\n      \nLacks GPU and batch processing support.\n\n      \nRecall suffers at high dimensionality or when optimal tree coverage is hard to achieve.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nAnnoy is used extensively in static recommendation systems, such as music and content suggestion engines (e.g., Spotify). Its design favors read-heavy workloads, and it’s especially effective when indexes must be shared across processes or run in constrained environments like embedded systems.\n\n    \n\n  \n\n\n\n\n\nQuantization-Based Methods\n\n\n\n\n  \n\n    \nQuantization-based methods are built on the idea of compressing high-dimensional vectors into compact, lossy representations, typically by mapping them to a discrete set of centroids or codes. These compact codes allow extremely fast approximate distance computation using precomputed lookup tables, making them ideal for billion-scale vector search tasks.\n\n  \n\n  \n\n    \nThese methods are widely used in industry for:\n\n\n    \n\n      \nLarge-scale image/video/text retrieval where space and speed are at a premium.\n\n      \nReal-time inference pipelines that need rapid ranking of semantically similar results.\n\n      \nHybrid ANN pipelines, often paired with clustering (IVF) or re-ranking modules.\n\n      \nScenarios that require deployment on limited-memory devices or optimized GPU environments.\n\n    \n\n  \n\n  \n\n    \nDespite their reliance on lossy compression, modern quantization approaches (e.g., OPQ, AVQ) offer a high degree of accuracy, particularly when tuned carefully.\n\n  \n\n\n\n\n\nProduct Quantization (PQ)\n\n\n\n\n  \n\n    \nOverview\n:\n\n\n    \n\n      \nVector \nx\n is split into \nm\n sub-vectors.\n\n      \nEach sub-vector is quantized into one of \nk\n centroids (learned using k-means).\n\n      \nFinal code is a tuple of centroid indices.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nPrecompute a distance table for each query sub-vector and all subspace centroids.\n\n      \nFinal distance is the sum of subspace distances from lookup tables.\n\n    \n\n  \n\n  \n\n    \nImplementation\n:\n\n\n    \n\n      \nIn FAISS, PQ is implemented with SIMD intrinsics and fused operations.\n\n      \nTypically used with IVF (Inverted File Index) for additional speed-up.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nExcellent compression with minimal storage cost.\n\n      \nEfficient for both CPU and GPU architectures.\n\n      \nFast distance computation using lookups, no full vector arithmetic needed.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nSensitive to data distribution; uniform quantization can lose detail.\n\n      \nMay degrade recall for fine-grained queries or tightly clustered data.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nPQ is commonly used in large-scale image and video retrieval systems. It’s a standard method in FAISS-based deployments at billion-scale for deduplication, facial recognition, and fast similarity ranking in content libraries, where compact representation is essential.\n\n    \n\n  \n\n\n\n\n\nOptimized Product Quantization (OPQ)\n\n\n\n\n  \n\n    \nExtension to PQ\n:\n\n\n    \n\n      \nLearn a rotation matrix to decorrelate input vectors before PQ encoding.\n\n      \nReduces quantization loss, improving accuracy.\n\n    \n\n  \n\n  \n\n    \nTraining\n:\n\n\n    \n\n      \nAlternates between PCA-like optimization and quantization.\n\n      \nComputationally heavier but yields significant accuracy improvements.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nLower distortion than basic PQ, especially for high-dimensional, correlated data.\n\n      \nFlexible, modular integration with other indexing structures (e.g., IVF).\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nSlower training and more complex parameter tuning.\n\n      \nRequires retraining if the data distribution shifts significantly.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nOPQ is deployed in precision-sensitive search scenarios such as e-commerce or financial recommendation systems, where even small gains in accuracy translate to meaningful impact. It’s ideal for embedding pipelines where dimension correlations hurt performance.\n\n    \n\n  \n\n\n\n\n\nLocality Sensitive Hashing (LSH)\n\n\n\n\n  \n\n    \nHashing Scheme\n:\n\n\n    \n\n      \nFamily of hash functions ensures that similar vectors hash to the same bucket with high probability.\n\n      \nFor cosine similarity: random hyperplane hash functions.\n\n    \n\n  \n\n  \n\n    \nMulti-Probe LSH\n:\n\n\n    \n\n      \nDuring query, multiple nearby buckets are probed to improve recall.\n\n    \n\n  \n\n  \n\n    \nChallenges\n:\n\n\n    \n\n      \nWorks best for low to moderate dimensions.\n\n      \nOften memory-intensive and suffers in high-density regions of vector space.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nExtremely fast sublinear search; constant-time hash lookups.\n\n      \nSimple to implement with theoretical performance guarantees.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nPoor recall for complex, high-dimensional datasets.\n\n      \nHigh memory usage for large numbers of hash tables.\n\n      \nTuning hash functions and probing depth is non-trivial.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nLSH is suitable for real-time systems with strict latency requirements, such as fraud detection, duplicate detection, or real-time alerting, where a fast but approximate match is preferable to delayed precision.\n\n    \n\n  \n\n\n\n\n\nAnisotropic Vector Quantization (AVQ)\n\n\n\n\n  \n\n    \nUsed in ScaNN\n:\n\n\n    \n\n      \nUnlike PQ, AVQ allows the shape of quantization cells to adapt to the data distribution.\n\n      \nEach centroid has an anisotropic (elliptical) region rather than spherical.\n\n    \n\n  \n\n  \n\n    \nIndex Construction\n:\n\n\n    \n\n      \nUses SVD-like analysis to deform Voronoi cells.\n\n      \nQuantization boundaries are optimized to better reflect real data spread.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nImproves Maximum Inner Product Search (MIPS) accuracy by focusing on high-value candidates.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nAdapts well to non-uniform data densities.\n\n      \nSuperior recall in semantic search applications compared to isotropic quantizers.\n\n      \nWell-suited to modern ML-generated embeddings.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nMore complex to construct and train than standard PQ.\n\n      \nNot yet widely supported outside of ScaNN.\n\n      \nLonger index build time due to centroid deformation.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nAVQ is tailored for semantic search tasks in NLP and vision systems, where embedding density varies. It powers systems like ScaNN that handle document retrieval, passage re-ranking, or intent classification using deep-learned embeddings.\n\n    \n\n  \n\n\n\n\n\nClustering-Based Methods\n\n\n\n\n  \nClustering-based approaches are a foundational class of ANN algorithms that partition the dataset into discrete groups or clusters using unsupervised learning techniques, most commonly k-means. These clusters serve as coarse partitions of the search space, enabling fast and scalable nearest neighbor search by reducing the number of direct comparisons required at query time.\n\n  \n\n    \nClustering-based methods are particularly effective when used as the first stage in multi-step retrieval architectures. They are often combined with quantization and re-ranking techniques to build high-performance ANN systems for web-scale search and recommendation.  Each of these clustering-based approaches is most effective when integrated into layered ANN architectures. For example:\n\n\n    \n\n      \nFAISS IVF-PQ\n: Combines IVF for coarse partitioning with PQ for fine-grained, compressed distance approximation.\n\n      \nScaNN\n: Employs k-means-style partitioning trees with AVQ and late-stage re-ranking.\n\n      \nTwo-Stage Retrieval Systems\n: Use clustering to shortlist candidates and exact scoring for ranking.\n\n    \n\n  \n\n\n\n\n\nInverted File Index (IVF)\n\n\n\n\n  \n\n    \nPartition Strategy\n:\n\n\n    \n\n      \nThe dataset is partitioned using \nk-means clustering\n into \nnlist\n coarse centroids. Each data point is assigned to its nearest centroid, forming an inverted list for that cluster.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nAt query time, the query vector is compared to all centroids, and the top \nnprobe\n closest centroids are selected.\n\n      \nOnly the vectors in the selected clusters are searched, significantly narrowing down the search space.\n\n    \n\n  \n\n  \n\n    \nIndex Construction\n:\n\n\n    \n\n      \nA training phase precedes indexing, where representative centroids are learned from a sample of the dataset.\n\n      \nVectors are then indexed into inverted lists corresponding to their nearest centroid.\n\n    \n\n  \n\n  \n\n    \nImplementation\n:\n\n\n    \n\n      \nIVF is commonly used in combination with Product Quantization (e.g., IVF-PQ in FAISS) to compress vectors and reduce memory usage while maintaining retrieval performance.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nScalable to billions of vectors.\n\n      \nFast query execution due to aggressive pruning.\n\n      \nIntegrates well with quantization and re-ranking modules.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nRequires careful tuning of \nnlist\n (cluster count) and \nnprobe\n (clusters to scan at query time).\n\n      \nClustering quality impacts recall and precision.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nPowers large-scale similarity search in systems such as Facebook’s visual search infrastructure and semantic embedding-based recommendation engines. Widely deployed in \nFAISS IVF-PQ\n and hybrid indexing pipelines.\n\n    \n\n  \n\n\n\n\n\nResidual Vector Quantization (RVQ)\n\n\n\n\n  \n\n    \nOverview\n:\n\n\n    \n\n      \nRVQ recursively encodes a vector by applying multiple layers of quantization, each capturing the residual error left by the previous layer.\n\n      \nThis hierarchical structure allows for progressively finer approximations of the original vector.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nQueries are encoded using the same residual structure and matched against compound codes generated during indexing.\n\n      \nSuitable for scenarios where single-code quantization (as in PQ) fails to preserve sufficient detail.\n\n    \n\n  \n\n  \n\n    \nIndex Construction\n:\n\n\n    \n\n      \nInvolves training multiple codebooks sequentially, each learning to quantize the residual of the prior stage.\n\n      \nTypically implemented in vector search systems that require higher recall than basic PQ can offer.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nDelivers better accuracy than single-pass quantization.\n\n      \nSupports fast computation with table lookups and SIMD-accelerated operations.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nMore complex to train and tune.\n\n      \nIncreased query-time latency due to deeper decoding stages.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nUsed in high-recall scenarios such as document retrieval, legal and biomedical search engines, and advanced FAISS configurations where accuracy is prioritized over absolute speed.\n\n    \n\n  \n\n\n\n\n\nScalable K-Means Clustering (Mini-Batch K-Means)\n\n\n\n\n  \n\n    \nPurpose\n:\n\n\n    \n\n      \nDesigned to handle clustering on very large datasets by using small, randomly selected subsets (mini-batches) for iterative updates.\n\n      \nEnables efficient, online construction of cluster centroids without processing the entire dataset at once.\n\n    \n\n  \n\n  \n\n    \nSearch Integration\n:\n\n\n    \n\n      \nFrequently used to generate cluster centroids in IVF-like indexing schemes or to enable dynamic re-clustering of streaming data.\n\n    \n\n  \n\n  \n\n    \nTraining Efficiency\n:\n\n\n    \n\n      \nOrders of magnitude faster than traditional k-means for large datasets.\n\n      \nSupports continual training or incremental centroid updates.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nSuitable for massive datasets that exceed memory constraints.\n\n      \nFaster training with minimal accuracy loss compared to full-batch methods.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nQuality of clusters depends on batch size and sampling strategy.\n\n      \nCan converge to suboptimal centroids in non-uniform or complex vector distributions.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nIdeal for dynamic or streaming data environments, such as online recommendation platforms, personalization systems, or large-scale ad retrieval where vector distributions evolve over time.\n\n    \n\n  \n\n\n\n\n\nGraph-Based Methods\n\n\n\n\n  \n\n    \nGraph-based methods construct navigable proximity graphs where each node represents a data point and edges connect it to a selection of its approximate nearest neighbors. These methods enable efficient similarity search through \ngreedy traversal\n, allowing the system to reach relevant regions of the data space without exhaustively scanning all vectors.\n\n  \n\n  \n\n    \nThese algorithms are especially effective in high-dimensional spaces where traditional methods struggle. They are widely used in real-world systems because of:\n\n\n    \n\n      \nExcellent accuracy-speed trade-offs\n, particularly at high recall thresholds.\n\n      \nSupport for dynamic updates\n, which is crucial for evolving datasets.\n\n      \nHigh empirical performance\n across embedding types (text, image, video, audio).\n\n      \nAdoption in commercial-grade vector databases and search systems\n, including Vespa, Milvus, Weaviate, Pinecone, and OpenSearch extensions.\n\n    \n\n  \n\n  \n\n    \nApplications include semantic search, recommendations, online personalization, ad retrieval, fraud detection, and large-scale analytics.\n\n  \n\n\n\n\n\nNavigable Small Worlds (NSW)\n\n\n\n\n  \n\n    \nNSW is a foundational graph-based ANN algorithm that constructs a proximity graph without hierarchical layers, relying instead on local navigation and randomized connectivity to explore the data space.\n\n  \n\n  \n\n    \nData Structure\n: A single-layer navigable graph where each node is connected to a subset of its nearest neighbors based on proximity.\n\n  \n\n  \n\n    \nConstruction\n:\n\n\n    \n\n      \nNodes are added one by one using randomized greedy strategies.\n\n      \nFor each new node, a set of connections is formed to existing nodes that are closest in terms of distance.\n\n      \nEdge formation favors a small-world topology: short paths exist between any two nodes, enabling quick traversal.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nBegins at a randomly selected node.\n\n      \nFollows a greedy walk, moving to the neighbor closest to the query.\n\n      \nTerminates once no neighbor is closer than the current node—reaching a local optimum.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nStraightforward to implement and debug.\n\n      \nRequires minimal configuration—fewer hyperparameters than hierarchical models.\n\n      \nLower memory usage than multilayer graphs.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nNo hierarchical navigation; search can be slower or less reliable.\n\n      \nConvergence to the global nearest neighbor is not guaranteed.\n\n      \nLess suited for very large datasets or high-recall applications.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nNSW is best suited for simpler, smaller-scale applications or environments with limited computational resources. Common in research settings, educational tools, or embedded systems where a lightweight ANN solution is needed.\n\n    \n\n  \n\n\n\n\n\nFast Inference for Graph-Based ANN (FINGER)\n\n\n\n\n  \n\n    \nFINGER is a graph search optimization that accelerates nearest neighbor search by reducing the number of exact distance computations during traversal.\n\n  \n\n  \n\n    \nAlgorithmic Enhancement\n:\n\n\n    \n\n      \nFINGER is not a standalone graph algorithm but a technique that enhances existing graph-based methods such as NSW or HNSW.\n\n    \n\n  \n\n  \n\n    \nTechnique\n:\n\n\n    \n\n      \nDecomposes vectors into components (projections and residuals).\n\n      \nEstimates distances using angular relationships and dot products.\n\n      \nAvoids costly full-vector comparisons for less promising candidates.\n\n    \n\n  \n\n  \n\n    \nIntegration\n:\n\n\n    \n\n      \nApplied exclusively at query time.\n\n      \nDoes not alter the underlying graph; can be used on top of existing indexes.\n\n      \nParticularly beneficial for large graphs with dense connectivity.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nReduces search latency by 20–60% in practice.\n\n      \nEasy to integrate without retraining or rebuilding the index.\n\n      \nCompatible with both simple (NSW) and hierarchical (HNSW) graphs.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nPerformance gains vary with data characteristics (e.g., vector distribution).\n\n      \nAdds complexity to the search logic.\n\n      \nMay slightly reduce recall if approximation is too aggressive.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nFINGER is ideal for latency-critical applications such as real-time recommendations, search-as-you-type interfaces, and live personalization systems. It helps accelerate existing HNSW or NSW graphs without requiring structural changes.\n\n    \n\n  \n\n\n\n\n\nHierarchical Navigable Small Worlds (HNSW)\n\n\n\n\n  \n\n    \nHNSW is a state-of-the-art ANN graph algorithm that improves upon NSW by introducing a hierarchical structure and multiple levels of graph connectivity.\n\n  \n\n  \n\n    \nData Structure\n:\n\n\n    \n\n      \nA multilayer graph in which higher levels connect long-range links and lower levels represent local neighborhoods.\n\n      \nNodes appear at multiple levels, with decreasing density as the level increases.\n\n    \n\n  \n\n  \n\n    \nConstruction\n:\n\n\n    \n\n      \nEach point is assigned a random maximum layer.\n\n      \nInsertions start at the top and descend layer by layer, connecting to the closest nodes at each level.\n\n      \nParameters like \nM\n (max edges per node) and \nefConstruction\n control connectivity.\n\n    \n\n  \n\n  \n\n    \nSearch\n:\n\n\n    \n\n      \nStarts from the topmost layer using a greedy search.\n\n      \nProgressively moves down levels, narrowing the search to increasingly local areas.\n\n      \nefSearch\n determines the number of nodes visited during querying.\n\n    \n\n  \n\n  \n\n    \nImplementation Notes\n:\n\n\n    \n\n      \nWell-optimized in libraries such as NMSLIB, FAISS, and hnswlib.\n\n      \nSupports dynamic insertions and deletions, making it suitable for production systems.\n\n    \n\n  \n\n  \n\n    \nPros\n:\n\n\n    \n\n      \nAchieves high recall at low latency.\n\n      \nScales efficiently to millions of high-dimensional vectors.\n\n      \nTunable for different performance requirements.\n\n      \nRobust in both static and dynamic settings.\n\n    \n\n  \n\n  \n\n    \nCons\n:\n\n\n    \n\n      \nHigher memory usage due to multilayer structure.\n\n      \nLonger index build times, especially with large datasets.\n\n      \nRequires hyperparameter tuning for optimal performance.\n\n    \n\n  \n\n  \n\n    \nUse Case\n:\n\n\n    \n\n      \nHNSW is the industry-standard graph-based ANN algorithm used in high-performance vector search platforms. It powers semantic retrieval in document search, personalized content feeds, similarity-based product recommendation, and large-scale embedding search in databases like Vespa, Weaviate, and Pinecone.\n\n    \n\n  \n\n\n\n\n\nTabular Comparison\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\n\n\nData Structure\n\n\nConstruction Strategy\n\n\nSearch Strategy\n\n\nKey Strengths\n\n\nLimitations\n\n\n\n\n\n\n\n\n\n\n\nTree-Based Methods\n\n\n\n\n\n\n\nKD-Tree\n\n\nBinary tree with axis-aligned splits\n\n\nRecursive partitioning along max-variance axes\n\n\nTree traversal with backtracking\n\n\nFast for low dimensions (<30), simple structure\n\n\nDegrades in high-dimensional spaces\n\n\n\n\n\n\nRandomized KD-Forest\n\n\nMultiple KD-Trees with random splits\n\n\nParallel tree building with randomized axes\n\n\nAggregate candidates from all trees\n\n\nImproved recall over single KD-tree\n\n\nHigher memory and tuning complexity\n\n\n\n\n\n\nAnnoy Forest\n\n\nForest of binary trees via random projections\n\n\nRandom hyperplane splits\n\n\nTree-wise candidate extraction with brute-force refinement\n\n\nMemory-mapped files, low RAM use, fast lookup\n\n\nStatic index, no GPU/batch support\n\n\n\n\n\n\n\nQuantization-Based Methods\n\n\n\n\n\n\n\nProduct Quantization (PQ)\n\n\nCodebooks for each subspace of vector\n\n\nK-means on vector subspaces\n\n\nLookup-table based distance approximation\n\n\nCompact codes, efficient on CPUs/GPUs\n\n\nAccuracy loss due to quantization\n\n\n\n\n\n\nOptimized PQ (OPQ)\n\n\nRotated subspace codebooks\n\n\nLearned rotation + PQ training\n\n\nSame as PQ, with lower quantization error\n\n\nBetter accuracy than PQ, widely supported\n\n\nHeavier training, complex tuning\n\n\n\n\n\n\n\nClustering-Based Methods\n\n\n\n\n\n\n\nInverted File Index (IVF)\n\n\nCluster centroids with inverted lists\n\n\nk-means clustering for coarse partitioning\n\n\nSelect nearest clusters and search within them\n\n\nScalable to billions of vectors, integrates with quantization\n\n\nClustering quality impacts recall and precision\n\n\n\n\n\n\nResidual Vector Quantization (RVQ)\n\n\nHierarchical residual codebooks\n\n\nRecursive quantization of residuals\n\n\nMulti-stage decoding and comparison\n\n\nHigher recall than single-level quantizers\n\n\nIncreased query latency, complex training\n\n\n\n\n\n\nMini-Batch K-Means\n\n\nIncrementally updated cluster centroids\n\n\nIterative mini-batch updates on sampled data\n\n\nUsed to assign points or create IVF partitions\n\n\nEfficient training on large datasets, supports streaming data\n\n\nCluster quality depends on batch configuration\n\n\n\n\n\n\n\nGraph-Based Methods\n\n\n\n\n\n\n\nNSW\n\n\nSingle-layer navigable small-world graph\n\n\nRandomized edge insertion based on proximity\n\n\nGreedy walk guided by local nearest neighbors\n\n\nLightweight, easy to implement, lower memory\n\n\nNo hierarchy, slower convergence, reduced recall in complex data\n\n\n\n\n\n\nFINGER\n\n\nAugmentation over navigable graph\n\n\nNo structural changes; integrates into search phase\n\n\nApproximate distance estimation via vector projections\n\n\nAccelerates search in existing graphs, reduces computation by 20–60%\n\n\nDependent on vector distribution, adds search-time complexity\n\n\n\n\n\n\nHNSW\n\n\nMultilayer navigable proximity graph\n\n\nGreedy layer-wise insertion with long- and short-range links\n\n\nHierarchical greedy search with priority queue\n\n\nHigh recall, dynamic updates, excellent accuracy-speed trade-off\n\n\nHigh memory usage, longer build time, parameter tuning required\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the right ANN algorithm family\n\n\n\n\n  \n\n    \nHere’s a guide to help you choose the right ANN algorithm family based on your data characteristics and application requirements:\n\n\n    \n\n      \n\n        \nTree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)\n: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.\n\n      \n\n      \n\n        \nQuantization-Based Methods (PQ, OPQ, LSH, AVQ)\n: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.\n\n      \n\n      \n\n        \nClustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)\n: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.\n\n      \n\n      \n\n        \nGraph-Based Methods (NSW, FINGER, HNSW)\n: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.\n\n      \n\n    \n\n  \n\n  \n\n    \nChoose based on your primary constraints—dimensionality, scale, update frequency, memory footprint, latency budget, and accuracy targets. Mixing families (for example, IVF-PQ or tree pre-filtering before a graph search) often yields the best balance in large-scale, production ANN pipelines.\n\n  \n\n\n\n\n\nANN Libraries\n\n\n\n\n  \nWhile ANN algorithms provide the foundational theory for efficient similarity search, their widespread use in real-world applications is made possible by powerful open-source libraries. These libraries abstract away the low-level implementation details and offer optimized, production-ready APIs for indexing, querying, and evaluating high-dimensional data.\n\n  \nBelow, we examine three leading libraries: \nFAISS\n, \nScaNN\n, and \nANNOY\n.\n\n\n\n\n\nFAISS (Facebook AI Similarity Search)\n\n\n\n\n  \nFAISS is a highly optimized C++ library with Python bindings, developed by Facebook AI Research to support efficient similarity search over large-scale datasets of dense vectors. It is widely used in both academic research and industrial systems.\n\n\n\n\n\nKey Features\n\n\n\n\n  \n\n    \nMultiple Indexing Strategies\n:\n\n\n    \n\n      \nFlat Index\n: Exhaustive search for exact results.\n\n      \nInverted File Index (IVF)\n: Partitions data using k-means centroids; search occurs in the closest partitions.\n\n      \nProduct Quantization (PQ)\n and \nOptimized PQ (OPQ)\n: Compresses vectors into compact codes.\n\n      \nIVF-PQ/IVF-OPQ\n: Combines inverted indexing with quantization for speed-accuracy trade-offs.\n\n      \nHNSW\n: Integrated for graph-based search.\n\n    \n\n  \n\n  \n\n    \nGPU Acceleration\n:\n\n\n    \n\n      \nFAISS provides CUDA-based implementations for key indexing methods.\n\n      \nEnables real-time search on billion-scale datasets.\n\n    \n\n  \n\n  \n\n    \nScalability\n:\n\n\n    \n\n      \nSupports distributed indexing using sharding.\n\n      \nCan operate on datasets that do not fit into RAM using memory-mapped files.\n\n    \n\n  \n\n\n\n\n\nSearch Workflow\n\n\n\n\n  \nTraining\n: For PQ or IVF, the index must first be trained on a representative sample.\n\n  \nIndexing\n: Vectors are added to the index using \n.add()\n or \n.add_with_ids()\n.\n\n  \nQuerying\n: The \n.search()\n method returns the k nearest neighbors for each query.\n\n  \nTuning\n: Parameters like \nnlist\n (number of clusters), \nnprobe\n (number of partitions to search), and PQ code size significantly affect accuracy and speed.\n\n\n\n\n\nEvaluation Metrics\n\n\n\n\n  \nSpeed\n: Number of queries per second or average query latency.\n\n  \nMemory Usage\n: Especially critical when using PQ or GPU mode.\n\n  \nAccuracy\n: Measured using recall metrics (e.g., recall\\@1, recall\\@10).\n\n\n\n\n\nPros\n\n\n\n\n  \nHigh flexibility and modular design.\n\n  \nGPU acceleration with multi-threaded CPU support.\n\n  \nRich suite of index types and hybrid approaches.\n\n\n\n\n\nCons\n\n\n\n\n  \nCan be complex to configure optimally.\n\n  \nGPU integration requires familiarity with CUDA.\n\n  \nNo built-in support for dynamic (online) index updates; best suited for batch ingestion.\n\n\n\n\n\n\n\n\nScaNN (Scalable Nearest Neighbors)\n\n\n\n\n  \nScaNN is an efficient similarity search library developed by Google Research, tailored for Maximum Inner Product Search (MIPS) and high-accuracy ANN in embedding-based retrieval systems.\n\n\n\n\n\nKey Features\n\n\n\n\n  \n\n    \nHybrid Indexing Architecture\n:\n\n\n    \n\n      \nPartitioning Tree\n: Efficiently narrows the candidate set using k-means clustering or partition trees.\n\n      \nAsymmetric Distance Computation (ADC)\n: For fast dot-product or Euclidean similarity.\n\n      \nAnisotropic Vector Quantization (AVQ)\n: Optimized for high-dimensional and non-uniform data, offering more accurate and balanced indexing.\n\n    \n\n  \n\n\n\n\n\nAnisotropic Vector Quantization (AVQ)\n\n\n\n\n  \n\n    \nAnisotropic Vector Quantization (AVQ) is a core indexing technique used in ScaNN to enhance performance in real-world, high-dimensional datasets. AVQ is specifically designed for anisotropic distributions, where vector densities vary significantly across the embedding space.\n\n  \n\n  \n\n    \nQuantization Grid\n:\n\n\n    \n\n      \nAVQ begins by dividing the space into a grid of quantization cells. Each cell corresponds to a cluster of vectors grouped by proximity under Euclidean distance.\n\n    \n\n  \n\n  \n\n    \nAnisotropic Adjustment\n:\n\n\n    \n\n      \nUnlike standard product quantization, AVQ adapts the \nshape and size\n of each grid cell to reflect the \nlocal density and orientation\n of the data.\n\n      \nThis is achieved via transformations that deform cells based on principal directions, producing elliptical (rather than spherical) boundaries.\n\n    \n\n  \n\n  \n\n    \nIndexing Phase\n:\n\n\n    \n\n      \nDuring index construction, vectors are assigned to their nearest quantization cell using modified centroids.\n\n      \nThis mapping improves clustering accuracy, especially in regions where traditional uniform quantization fails.\n\n    \n\n  \n\n  \n\n    \nSearch Phase\n:\n\n\n    \n\n      \nFor a given query, ScaNN identifies the most relevant cells and retrieves candidate vectors.\n\n      \nThese candidates are further refined through re-ranking using higher-precision computations, typically involving dot products or exact distances.\n\n    \n\n  \n\n  \n\n    \nPerformance Benefits\n:\n\n\n    \n\n      \nAVQ significantly reduces the number of comparisons by focusing the search on well-shaped regions of the vector space.\n\n      \nOn benchmarks like \nglove-100-angular\n, ScaNN has demonstrated superior performance, handling nearly twice the query load at the same accuracy compared to the next-best library (NGT-onng).\n\n      \nThis quantization method is especially useful in deep learning workloads where embeddings exhibit high variance in density.\n\n    \n\n  \n\n\n\n\n\n\n\n\nUse Case: Semantic Search\n\n\n\n\n  \nScaNN excels in use cases where embeddings are generated from text, images, or product metadata using deep learning models. Its design is aligned with the two-tower model, where both queries and items are independently embedded into a shared vector space.\n\n\n\n\n\nImplementation Considerations\n\n\n\n\n  \nTraining\n: Required for quantization and partitioning.\n\n  \nSearch Configuration\n: Parameters such as the number of clusters, quantization depth, and number of re-ranked results need tuning.\n\n  \nDeployment\n: Can be used with TensorFlow Serving or directly integrated into a backend service.\n\n  \nScaNN is open-source and available via \nGitHub\n. It can be installed via Pip and supports both TensorFlow and NumPy inputs.\n\n\n\n\n\nPros\n\n\n\n\n  \nHigh accuracy and throughput for MIPS.\n\n  \nStrong performance on semantically rich embeddings.\n\n  \nOpen-source and actively maintained.\n\n\n\n\n\nCons\n\n\n\n\n  \nLess mature ecosystem than FAISS.\n\n  \nGPU support is limited.\n\n  \nInitial index build time can be high.\n\n\n\n\n\n\n\n\n\n\nANNOY (Approximate Nearest Neighbors Oh Yeah)\n\n\n\n\n  \nDeveloped by Spotify, ANNOY is a lightweight and efficient C++ library with Python bindings for performing fast similarity search using random projection forests. It is particularly suitable for static datasets and read-heavy workloads.\n\n\n\n\n\nKey Features\n\n\n\n\n  \n\n    \nIndexing Method\n:\n\n\n    \n\n      \nConstructs multiple binary trees using random hyperplane splits.\n\n      \nEach tree represents a different partitioning of the space.\n\n    \n\n  \n\n  \n\n    \nMemory Mapping\n:\n\n\n    \n\n      \nIndexes are written to disk and memory-mapped at query time.\n\n      \nEnables extremely fast lookups with minimal RAM usage.\n\n    \n\n  \n\n  \n\n    \nRuntime Parameters\n:\n\n\n    \n\n      \nn_trees\n: Affects accuracy and index size. More trees = better recall.\n\n      \nsearch_k\n: Controls number of nodes checked during search. Higher values = better accuracy, slower speed.\n\n    \n\n  \n\n\n\n\n\nUse Case: Music Recommendation\n\n\n\n\n  \nSpotify uses ANNOY for fast retrieval of similar songs, playlists, or users based on vector representations of user behavior or audio features.\n\n\n\n\n\nImplementation Notes\n\n\n\n\n  \nStatic indexing: Does not support dynamic insertions (though \nannoy2\n aims to address this).\n\n  \nNo GPU support.\n\n  \nNo native batch processing interface; needs custom implementation for throughput optimization.\n\n\n\n\n\nPros\n\n\n\n\n  \nSimple to use and highly portable.\n\n  \nMemory-efficient due to disk-based index.\n\n  \nSuitable for multi-process environments.\n\n\n\n\n\nCons\n\n\n\n\n  \nLimited support for dynamic updates.\n\n  \nSlower than FAISS or ScaNN for very large datasets.\n\n  \nNo GPU acceleration or quantization techniques.\n\n\n\n\n\n\n\n\n\n  \nThese libraries encapsulate the most widely adopted implementations of ANN techniques in industry today. Each has a different performance profile and is better suited to specific use cases:\n    \n\n      \nFAISS\n: Ideal for large-scale, high-performance search with GPU support.\n\n      \nScaNN\n: Excellent for semantically meaningful embeddings and MIPS-heavy workloads.\n\n      \nANNOY\n: Lightweight and efficient for static, read-optimized systems.\n\n    \n\n  \n\n\n\n\n\nComparative Analysis\n\n\n\n\n  \n\n    \nThis section offers a comparison that highlights the fundamental trade-offs between speed, accuracy, memory efficiency, hardware requirements, and integration complexity across major ANN methods and libraries. The most appropriate method depends on factors such as:\n\n  \n\n  \n\n    \nDataset Characteristics\n:\n\n\n    \n\n      \nSize (thousands to billions of vectors)\n\n      \nDimensionality (sparse vs. dense vectors, 32–1000+ dimensions)\n\n      \nDistribution (uniform vs. clustered or anisotropic)\n\n      \nUpdate frequency (static vs. dynamic/real-time ingestion)\n\n    \n\n  \n\n  \n\n    \nPerformance Needs\n:\n\n\n    \n\n      \nLatency requirements (real-time systems vs. batch recommendations)\n\n      \nThroughput (queries per second in multi-user environments)\n\n      \nIndex build time and preprocessing overhead\n\n    \n\n  \n\n  \n\n    \nResource Constraints\n:\n\n\n    \n\n      \nAvailable memory (RAM vs. disk)\n\n      \nCompute acceleration (CPU-only vs. GPU-accelerated systems)\n\n      \nInfrastructure for deployment (cloud-native vs. embedded use cases)\n\n    \n\n  \n\n  \n\n    \nAccuracy Tolerance\n:\n\n\n    \n\n      \nUse cases that demand precise retrieval vs. tolerable approximation\n\n      \nTop-k recall metrics, precision trade-offs\n\n    \n\n  \n\n\n\n\n\nTabular Comparison\n\n\n\n\n\n\n\n\n\n\n\nLibrary\n\n\nDefinition/Functioning\n\n\nPros\n\n\nCons\n\n\n\n\n\n\n\n\n\n\n\nFAISS\n\n\nModular similarity search library supporting flat (brute-force), inverted file (IVF), product quantization (PQ), optimized PQ (OPQ), and graph-based (HNSW) indexing. Offers strong support for hybrid strategies and CUDA-based GPU acceleration.\n\n\n\n\n\n\nHandles billion-scale datasets efficiently with GPU and SIMD acceleration\n\n\nSupports both L2 and inner product distance functions\n\n\nHighly modular: custom pipelines can be built by combining indexing strategies\n\n\nSupports on-disk indexing, IVF quantization, and parallel queries\n\n\n\n\n\n\n\n\n\n\nNo native support for dynamic indexing (most indexes are immutable)\n\n\nAdvanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise\n\n\nLimited integration with non-PyTorch ML stacks (e.g., TensorFlow)\n\n\n\n\n\n\n\n\n\n\n\nScaNN\n\n\nGoogle's ANN framework optimized for Maximum Inner Product Search (MIPS). Uses hybrid strategies: tree-based partitioning, anisotropic vector quantization (AVQ), and re-ranking with exact computation. Especially well-suited for semantic embeddings.\n\n\n\n\n\n\nDesigned for embedding-based search (e.g., NLP, recommendation systems)\n\n\nHighly accurate top-k results in inner product space (MIPS)\n\n\nWell-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines\n\n\nSignificantly faster than LSH or naive MIPS approaches\n\n\n\n\n\n\n\n\n\n\nNo official GPU acceleration (primarily CPU-based)\n\n\nLess flexible than FAISS for custom index architectures\n\n\nLonger index construction time due to partitioning and quantization training\n\n\n\n\n\n\n\n\n\n\n\nANNOY\n\n\nTree-based approach using multiple random projection trees. Each tree splits the space using randomly selected hyperplanes. Designed to trade off build time and search accuracy with minimal memory overhead using memory-mapped files.\n\n\n\n\n\n\nIndex files can be memory-mapped and shared across processes\n\n\nSupports very large datasets on low-memory systems (e.g., embedded devices)\n\n\nSimple API and fast to build indexes for static data\n\n\nIdeal for batch processing and edge deployments\n\n\n\n\n\n\n\n\n\n\nCannot update index after construction (no dynamic insertions)\n\n\nNot designed for GPU acceleration or vector quantization\n\n\nRecall suffers for very high-dimensional data (>200D)\n\n\nNo native batch-query optimization; needs manual parallelization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n    \nThis comparison should guide you toward the best ANN system for your specific technical needs:\n\n\n    \n\n      \nChoose \nFAISS\n for large-scale indexing with advanced GPU acceleration and tight performance tuning.\n\n      \nUse \nScaNN\n if you’re operating on semantic embeddings or deep learning pipelines, particularly with MIPS.\n\n      \nOpt for \nANNOY\n when working with static datasets in constrained memory environments or needing file-based deployment.\n\n      \nPrefer \nHNSW\n when low latency and high recall are critical, and memory usage is acceptable.\n\n      \nConsider \nFINGER\n as a low-overhead performance enhancer if you already employ graph-based indexes.\n\n    \n\n  \n\n\n\n\n\nANN-Benchmarks\n\n\n\n\n  \nANN-Benchmarks\n is an open-source benchmarking platform that provides a standardized and reproducible environment for evaluating the performance of approximate nearest neighbor algorithms. It enables researchers and practitioners to assess trade-offs between speed, accuracy, and memory usage across various libraries and algorithmic approaches.\n\n\n\n\n\nPurpose and Scope\n\n\n\n\n  \nThe benchmark suite is designed to answer key questions such as:\n    \n\n      \nWhich ANN method offers the best speed/accuracy trade-off for a particular type of data?\n\n      \nHow do different libraries perform under the same distance metric?\n\n      \nWhat is the relative performance of quantization vs. graph-based indexing on benchmark datasets?\n\n    \n\n  \n\n\n\n\n\nKey Features\n\n\n\n\n  \n\n    \nCommon Datasets\n:\n\n\n    \n\n      \nIncludes real-world and synthetic datasets like \nSIFT\n, \nGloVe\n, \nFashion-MNIST\n, and \nDeep Image Descriptors\n.\n\n      \nEach dataset varies in dimensionality, density, and semantics, allowing for broad evaluation.\n\n    \n\n  \n\n  \n\n    \nDistance Metrics\n:\n\n\n    \n\n      \nSupports various metrics such as Euclidean distance (L2), angular distance (cosine similarity), and inner product (for MIPS scenarios).\n\n    \n\n  \n\n  \n\n    \nEvaluation Metrics\n:\n\n\n    \n\n      \nRecall@\\(k\\)\n: Measures the fraction of true nearest neighbors found in the top-k results.\n\n      \nQPS (Queries per second)\n: Throughput of the method, showing speed under load.\n\n      \nIndex size and build time\n: Assesses memory footprint and pre-processing requirements.\n\n    \n\n  \n\n  \n\n    \nLeaderboard Interface\n:\n\n\n    \n\n      \nLive comparison of algorithmic results across datasets.\n\n      \nInteractive plots showing recall vs. query time, making it easy to identify optimal configurations.\n\n    \n\n  \n\n\n\n\n\nUsing ANN-Benchmarks\n\n\n\nThe benchmarks can be run locally via Docker and Python, allowing users to test their own ANN implementations or configurations. It supports integrating custom methods into the benchmarking pipeline, which makes it a valuable research tool.\n\n\n\n\n\n\nPractical Use Cases\n\n\n\n\n  \nModel Selection\n: Choose the best ANN library for your specific task and hardware constraints.\n\n  \nAlgorithm Tuning\n: Understand how hyperparameters like \nnlist\n, \nnprobe\n, or \nsearch_k\n affect real-world performance.\n\n  \nRegression Testing\n: Evaluate how updates to ANN methods impact speed or recall.\n\n\n\n\n\nReferences\n\n\n\n\n  \nJegou, Hervé, et al., \nFAISS: A library for efficient similarity search\n, Engineering at Meta, 29 March 2017.\n\n  \nErik Bernhardsson, \nEfficient Indexing of Billion-Scale datasets of deep descriptors\n, 1 October 2015.\n\n  \nAnnouncing ScaNN: Efficient Vector Similarity Search\n\n  \nNearest neighbors and vector models – part 2 – algorithms and data structures\n\n  \nMore-efficient approximate nearest-neighbor search\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{VatsChadha2020DistilledApproximateNearestNeighbors,\n  title   = {Approximate Nearest Neighbors -- Similarity Search},\n  author  = {Chadha, Aman and Jain, Vinija},\n  journal = {Distilled AI},\n  year    = {2022},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/ann-similarity-search/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • GPT-4o Native Image Generation\n\n  \n\n\n  \n\n  \n\n  \nIntroduction\n\n  \nMultimodal Input/Output Sequence Format in GPT-4o\n    \n\n      \nSequence Construction\n\n      \nInterleaved Token Semantics\n\n      \nPatch Representation and Emission\n\n      \nExample Input/Output Patterns\n\n    \n\n  \n\n  \nImage Representation and Latent Modeling\n    \n\n      \nLatent Space Encoding via VAE\n\n      \nPatchification Process\n\n      \nAdvantages Over Discretized Representations\n\n      \nConditioning and Patch Context Handling\n\n      \nLatent Format and Token Emission Strategy\n\n    \n\n  \n\n  \nUnified Transformer Architecture and Attention Patterns\n    \n\n      \nModality-Agnostic Input Pipeline\n        \n\n          \nInput projection layers:\n\n        \n\n      \n\n      \nTransformer Backbone Design\n\n      \nHybrid Attention Masking\n        \n\n          \nExplanation\n\n        \n\n      \n\n      \nIntegration of Modality Markers and Position Awareness\n\n      \nAttention Implementation Details\n\n    \n\n  \n\n  \nTraining Objectives and Modality-Specific Loss Functions\n    \n\n      \nCombined Loss Function\n\n      \nLanguage Modeling Loss (Text Tokens)\n\n      \nDiffusion Objective for Image Patches\n        \n\n          \nForward Diffusion Process\n\n          \nReverse Denoising Loss\n\n        \n\n      \n\n      \nVisualizing the Forward and Reverse Diffusion Process\n\n      \nNoise Schedule and Timestep Sampling\n\n      \nTraining Considerations\n\n      \nDiffusion-Specific Training Behavior\n\n      \nTraining Stability and Loss Balancing\n\n    \n\n  \n\n  \nDiffusion Sampling and Row-by-Row Decoding with Rolling Diffusion\n    \n\n      \nMotivation for Rolling Diffusion\n\n      \nLocal Time Reparameterization\n\n      \nReverse Process with Rolling Denoising\n\n      \nLoss Function for Windowed Denoising\n\n      \nApplication in GPT-4o\n\n    \n\n  \n\n  \nImage Rendering, VAE Decoding, and Final Output Construction\n    \n\n      \nPatch Sequence to Latent Grid Reconstruction\n\n      \nVAE Decoder Architecture\n\n      \nPost-Processing and Image Reconstruction\n\n      \nImage Completion and the \n<EOI>\n Token\n\n      \nTraining Considerations for the Decoder\n\n    \n\n  \n\n  \nIntegration with Text Generation and Modality Interleaving\n    \n\n      \nText and Image Blocks in the Token Stream\n\n      \nMode Switching During Generation\n        \n\n          \nText Mode (Language Modeling)\n\n          \nImage Mode (Diffusion Sampling)\n\n        \n\n      \n\n      \nCausal Context Preservation\n\n      \nPrompt-Image Conditioning (Text → Image)\n\n      \nCaptioning (Image → Text)\n\n      \nMultimodal Documents and Interleaved Outputs\n\n    \n\n  \n\n  \nArchitectural Scaling, Training Regimes, and Implementation Strategy\n    \n\n      \nModel Scaling and Configuration\n\n      \nTraining Data: Multimodal Sampling Strategy\n\n      \nTraining Objectives and Scheduling\n\n      \nOptimizer and Hyperparameter Choices\n\n      \nDiscretization Ablation and Transfusion Superiority\n\n    \n\n  \n\n  \nSummary of Design Decisions and Forward-Looking Considerations\n    \n\n      \nUnified Modality Architecture\n\n      \nDiffusion as the Bridge for Continuous Generation\n\n      \nComparison to Chameleon: Design Tradeoffs\n\n      \nExtensibility to Future Modalities\n        \n\n          \nVideo\n\n          \nAudio\n\n          \n3D and Scene Graphs\n\n        \n\n      \n\n      \nGPT-4o as a Foundation for Future Multimodal Interfaces\n\n    \n\n  \n\n  \nEvaluation, Sample Quality, and Benchmark Comparisons\n    \n\n      \nEvaluation Metrics for Image Quality and Prompt Alignment\n        \n\n          \nFID (Fréchet Inception Distance)\n\n          \nCLIP Score\n\n        \n\n      \n\n      \nTransfusion Benchmark Performance\n\n      \nChameleon Performance Snapshot\n\n      \nImplications for GPT-4o Evaluation\n\n    \n\n  \n\n  \nReferences\n\n\n\n\n\nIntroduction\n\n\n\n\n  \n\n    \nGPT-4o introduces a significant leap in multimodal generative modeling by enabling native image generation within a unified autoregressive decoding process. Unlike previous systems such as ChatGPT, which relied on discrete modality-specific APIs (e.g., calling a DALL·E model externally to generate an image), GPT-4o integrates image synthesis directly into its transformer backbone. It accomplishes this by representing images as sequences of continuous-valued latent patches, decoded using diffusion-based denoising, and interleaved naturally with text tokens within a single sequence of model outputs.\n\n  \n\n  \n\n    \nPrevious image generation from systems like ChatGPT involved ChatGPT calling an image generation tool (DALL·E) on the user’s behalf. While this new Transfusion approach involves GPT-4o outputting an optional sequence of tokens for text, then a special token to signal the generation of an image (\n<BOI>\n), followed by a sequence of ( n ) random image patches which are then filled in using diffusion, and finally a special token to signal the end of the image block (\n<EOI>\n). This interleaving of text tokens and image patches can be repeated as needed, supporting rich multimodal documents. These image patches are subsequently converted into a final image using either a simple linear layer or U-Net up blocks in conjunction with a Variational Autoencoder (VAE) decoder.\n\n  \n\n  \n\n    \nThis integration marks a paradigm shift in the design of large multimodal models: rather than viewing modalities as disjoint domains stitched together by toolchains, GPT-4o processes them as a unified token stream with modality-aware processing logic and a shared transformer core. Both text tokens and image patch embeddings are interpreted by the same sequence model, and the transition between modalities is handled inline, governed by special control tokens such as \n<BOI>\n (Begin of Image) and \n<EOI>\n (End of Image).\n\n  \n\n  \n\n    \nThe architectural foundations of GPT-4o draw heavily on recent breakthroughs in multimodal modeling. Three particularly influential contributions include:\n\n\n    \n\n      \n\n        \nChameleon\n (\nMeta, 2024\n): Chameleon explored early-fusion multimodal transformers that handle text and image data using a shared token vocabulary. Images are discretized into codebook indices via vector-quantized VAEs (VQ-VAEs), allowing them to be represented as sequences of discrete tokens just like text. While this approach simplifies integration, it introduces a significant information bottleneck due to the coarse granularity of quantized image tokens. All images were broken into discrete image tokens chosen from a vocabulary of fixed size, and the image token generation was also done from the same vocabulary. This discretization process may result in information loss and imposes constraints on representational expressiveness, which is one of the biggest drawbacks of the Chameleon architecture.\n\n      \n\n      \n\n        \nTransfusion\n (\nZhou et al., 2024\n): Transfusion presents a novel transformer that is trained using both language modeling (for text) and denoising diffusion (for continuous image latent vectors). It demonstrates that a single transformer model can learn to predict text tokens autoregressively and generate image latents via denoising, within the same training regime. Unlike Chameleon which discretizes images, Transfusion preserves the full fidelity of image representations by modeling them in continuous latent space using pretrained VAEs, and applies denoising diffusion directly to those vectors without quantization. As a result, Transfusion significantly outperforms Chameleon, surpassing it in every combination of text-to-image, image-to-text, and mixed-modality generation tasks. Importantly, Transfusion introduces a hybrid attention masking strategy that allows for causal attention across the full sequence and bidirectional attention within image blocks, enabling rich image generation while preserving the left-to-right flow of text decoding.\n\n      \n\n      \n\n        \nRolling Diffusion\n (\nRuhe et al., 2024\n): Rolling Diffusion proposes a novel denoising strategy where image latents are generated row-by-row or block-by-block, rather than all at once. By reparameterizing the diffusion schedule locally within a sliding window of patches, this technique enables models to generate visual content in a streaming or sequential fashion, closely aligning with the causal nature of text generation. This innovation is particularly valuable for efficient image decoding in models like GPT-4o, where full-image generation in one shot would be prohibitively expensive or inflexible.\n\n      \n\n    \n\n  \n\n  \n\n    \nTogether, these contributions provide the theoretical and empirical groundwork for GPT-4o’s design. From Transfusion, GPT-4o inherits the dual loss framework and unified transformer. From Chameleon, it borrows ideas for modality-agnostic tokenization and attention sharing (while avoiding its quantization drawbacks). And from Rolling Diffusion, it adopts the ability to generate images incrementally, improving both runtime efficiency and interactive applicability.\n\n  \n\n  \n\n    \nThe remainder of this primer explores GPT-4o’s architecture, training objectives, attention patterns, image rendering pipeline, and decoding procedures in detail. Each section will reference architectural diagrams and implementation-specific insights drawn directly from the above papers.\n\n  \n\n\n\n\n\nMultimodal Input/Output Sequence Format in GPT-4o\n\n\n\n\n  \nGPT-4o represents a unified generative modeling approach that outputs both natural language tokens and image content within a single autoregressive token stream. This fundamental design choice enables GPT-4o to process and generate interleaved multimodal documents without relying on external systems for image generation (unlike earlier models that delegated image synthesis to DALL·E-style modules).\n\n\n\n\n\nSequence Construction\n\n\n\n\n  \nAt training and inference time, data samples are formatted as interleaved sequences of discrete and continuous elements. These sequences are tokenized and presented to a single transformer model:\n\n\n\n\n\n[Text tokens] ... <BOI> [Latent patch vectors] <EOI> ... [Text tokens]\n\n\n\n\n\n  \n\n    \nText tokens are standard discrete tokens derived from a Byte Pair Encoding (BPE) tokenizer, consistent with LLaMA-style implementations. Each token is an integer and mapped to a high-dimensional embedding vector.\n\n  \n\n  \n\n    \n<BOI>\n (Begin of Image) and \n<EOI>\n (End of Image) tokens act as boundary markers, explicitly identifying where the image content begins and ends in the token stream.\n\n  \n\n  \n\n    \nLatent patch vectors represent image data, not as pixel arrays, but as compressed, continuous latent vectors in \\(\\mathbb{R}^d\\), generated using a VAE or Latent Diffusion model.\n\n  \n\n  \n\n    \nThis design enables GPT-4o to fluidly alternate between natural language and visual content generation without architectural context switching, similar to Transfusion’s token stream format. The following figure (\nsource\n) offers a high-level illustration of the Transfusion architecture with an example showing the placement of text tokens, BOI, latent vectors, and EOI within a shared transformer input. Transfusion follows a single transformer perceives, processes, and produces data of every modality. Discrete (text) tokens are processed autoregressively and trained on the next token prediction objective. Continuous (image) vectors are processed together in parallel and trained on the diffusion objective. Marker BOI and EOI tokens separate the modalities.\n\n  \n\n\n\n\n\n\n\n\nInterleaved Token Semantics\n\n\n\n\n  \nGPT-4o distinguishes between modalities using:\n    \n\n      \nModality-specific positional encodings\n\n      \nModality markers (\n<BOI>\n, \n<EOI>\n)\n\n      \nAttention masking (causal for text, bidirectional for image)\n\n    \n\n  \n\n  \n\n    \nEach token (whether text or image) is embedded into a shared hidden representation space \\(\\mathbb{R}^d\\) so that all subsequent transformer operations can proceed uniformly.\n\n  \n\n  \n\n    \nDespite being continuous vectors, image patch latents are handled similarly to token embeddings: they are added to position embeddings and passed into the transformer as sequence elements.\n\n  \n\n  \n\n    \nThis idea is rooted in the early-fusion formulation presented in the Chameleon architecture, a family of mixed-modal foundation models capable of generating and reasoning with mixed sequences of arbitrarily interleaved textual and image content.\n\n  \n\n  \nChameleon demonstrated the feasibility of uniformly tokenizing all modalities. GPT-4o inherits this idea but adapts it to continuous embeddings (from VAE-encoded image patches) rather than discretized tokens.\n\n\n\n\n\nPatch Representation and Emission\n\n\n\n\n  \n\n    \nIn the image generation phase, GPT-4o autoregressively emits:\n\n\n    \n\n      \n<BOI>\n token\n — switches the model into image-generation mode\n\n      \nSequence of latent vectors\n — sampled using a diffusion process\n\n      \n<EOI>\n token\n — signals image is complete and resumes text mode\n\n    \n\n  \n\n  \n\n    \nEach latent vector corresponds to a patch from a latent image space, typically derived from a pretrained VAE trained on high-resolution images:\n\n  \n\n\n\n\n\\[z \\in \\mathbb{R}^{n \\times d}\\]\n\n\n\n  \n\\(n\\): number of patches (e.g., for a 16 \\(\\times\\) 16 grid, \\(n = 256\\))\n\n  \n\n    \n\\(d\\): embedding dimension (e.g., \\(d = 32\\))\n\n  \n\n  \nThis latent sequence is processed just like text tokens in the transformer, with the difference that during training, a diffusion-based denoising objective is applied instead of a next-token prediction loss. The following figure (\nsource\n) illustrates the Transfusion architecture’s image-to-latent encoding and decoding pathway including the following steps: (i) VAE encoding, (ii) latent patchification (grid slicing), (iii) Optional linear projection or U-Net down blocks, and (iv) output latents used in transformer.\n\n\n\n\n\n\n\n\nExample Input/Output Patterns\n\n\n\n\n  \nGPT-4o handles diverse multimodal prompt configurations:\n\n\n\n\n\n\n  \nText → Image\n:\n    \n\"Draw a steampunk elephant\" → `<BOI>` [image latents] `<EOI>`\n\n    \n\n  \n\n  \nImage → Text\n:\n    \n`<BOI>` [image latents] `<EOI>` → \"This is a steampunk elephant.\"\n\n    \n\n  \n\n  \nImage + Text → Image\n:\n    \n`<BOI>` [image latents] `<EOI>` \"Now show it in space\" → `<BOI>` [new image] `<EOI>`\n\n    \n\n  \n\n  \nFully interleaved documents\n:\n    \n\"Here’s a diagram:\" `<BOI>` [image] `<EOI>` \"This represents the core architecture.\"\n\n    \n\n  \n\n\n\n\n\n\n  \nBy embedding image and text elements into a shared sequence, GPT-4o is capable of:\n    \n\n      \nContextually grounded image generation\n\n      \nVisual question answering\n\n      \nCaptioning\n\n      \nMultimodal dialogue\n\n      \nFully interleaved generation\n\n    \n\n  \n\n  \nThis kind of seamless interaction is foundational to future interactive AI agents, and GPT-4o offers one of the first viable blueprints for such unified modality handling.\n\n\n\n\n\nImage Representation and Latent Modeling\n\n\n\n\n  \nGPT-4o relies on latent-space representations for image generation, avoiding direct generation in pixel space (which is computationally expensive) or discrete image token generation (which is lossy, as seen in Chameleon). Instead, it draws directly from the latent diffusion modeling framework, using continuous patch embeddings derived from a pretrained VAE.\n\n  \nThis section details how image data is processed, encoded, and decoded using VAE architectures, referencing design elements from both the Transfusion and Rolling Diffusion papers.\n\n\n\n\n\nLatent Space Encoding via VAE\n\n\n\n\n  \n\n    \nThe core mechanism for representing image data in GPT-4o is a variational autoencoder, trained to compress and reconstruct image patches in a continuous latent space.\n\n  \n\n  \n\n    \nAt training and inference time, each image is converted into a sequence of latent vectors via the encoder:\n\n\n\\[z = \\text{VAE}_{\\text{enc}}(x), \\quad \\text{with } z \\in \\mathbb{R}^{n \\times d}\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(x \\in \\mathbb{R}^{H \\times W \\times 3}\\): original RGB image (e.g., 256×256).\n\n          \n\\(z\\): latent encoding of the image.\n\n          \n\\(n\\): number of latent tokens (e.g., 256 for a 16×16 grid).\n\n          \n\\(d\\): dimensionality of each latent vector (e.g., 8, 16, or 32 depending on compression).\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nAfter generation, the latent representation is decoded back into pixel space using the decoder:\n\n\n\\[\\hat{x} = \\text{VAE}_{\\text{dec}}(z)\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(\\hat{x}\\): reconstructed image.\n\n          \n\\(\\text{VAE}_{\\text{dec}}\\): decoder (typically a U-Net or CNN-based upsampler).\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThis architecture is compatible with the Latent Diffusion Models (LDM) described by Rombach et al. (2022), and reused in both Transfusion and Rolling Diffusion.\n\n  \n\n\n\n\n\nPatchification Process\n\n\n\n\n  \n\n    \nThe latent space is structured by dividing the compressed VAE output into a regular spatial grid of patches. For example:\n\n\n    \n\n      \nA 256×256 image → compressed to a 32×32 latent grid by the encoder\n\n      \nEach 2×2 region of the latent space is flattened into one vector (via patchification)\n\n      \nResulting in 256 patch vectors of dimension \\(d\\)\n\n    \n\n  \n\n  \n\n    \nThese vectors are flattened left-to-right, top-to-bottom to produce a 1D sequence:\n\n  \n\n\n\n\n\\[[z_1, z_2, ..., z_n] \\in \\mathbb{R}^{n \\times d}\\]\n\n\n\n  \nPatch embedding options:\n    \n\n      \nSimple linear layer\n (low compute cost, used during scaling)\n\n      \nU-Net Down Blocks\n (used in Transfusion for better local structure retention)\n\n    \n\n  \n\n  \n\n    \nThe image patch vectors are then treated as continuous tokens and passed into the shared Transformer sequence model.\n\n  \n\n  \nImplementation Notes (from Transfusion)\n:\n    \n\n      \nAdd learned positional embeddings or use RoPE (rotary positional encoding).\n\n      \nAdd timestep embeddings during diffusion training.\n\n      \nNormalize latent values (LayerNorm or RMSNorm before transformer input).\n\n    \n\n  \n\n\n\n\n\nAdvantages Over Discretized Representations\n\n\n\n\n  \n\n    \nChameleon, a contemporaneous model from Meta, uses vector-quantized VAE (VQ-VAE) to compress images into discrete codebook indices. This approach allows uniform token handling across modalities but introduces quantization artifacts and information bottlenecks.\n\n  \n\n  \n\n    \nChameleon quantizes images into discrete tokens introduces an upper bound on fidelity, particularly for images with fine-grained detail or high-frequency textures.\n\n  \n\n  \nIn contrast, GPT-4o (via Transfusion-style training) retains full continuous latent space precision, resulting in:\n    \n\n      \nHigher image fidelity\n\n      \nLower FID scores (as shown in Transfusion’s MS-COCO evaluations)\n\n      \nBetter conditioning on prompt embeddings (improved CLIP scores)\n\n    \n\n  \n\n  \nMoreover, continuous latents simplify image loss computation using denoising score-matching, without needing to manage a large discrete vocabulary (e.g., 8192 tokens as in Chameleon).\n\n\n\n\n\nConditioning and Patch Context Handling\n\n\n\n\n  \nWithin the transformer, each latent vector can:\n    \n\n      \nAttend to earlier tokens (text or image)\n\n      \nAttend bidirectionally to other patches within the same image block (defined by \n<BOI>\n and \n<EOI>\n markers)\n\n    \n\n  \n\n  \n\n    \nThis behavior is controlled by a custom attention mask:\n\n\n    \n\n      \nCausal attention globally (maintaining autoregressive property)\n\n      \nFully bidirectional within a \n<BOI>\n–\n<EOI>\n block\n\n    \n\n  \n\n  \n\n    \nThis strategy allows intra-image coherence while preserving the left-to-right generation constraint across modalities.\n\n  \n\n  \nThe following figure (\nsource\n) visualizes the causal + intra-image attention mask.\n\n\n\n\n\n\n\n\nLatent Format and Token Emission Strategy\n\n\n\n\n  \n\n    \nDuring inference, GPT-4o samples the image patch sequence in its latent form, one diffusion trajectory at a time, and reconstructs the final image via the VAE decoder.\n\n  \n\n  \n\n    \nTo simplify implementation:\n\n    \n\n      \nPatch length \\(n\\) is fixed per image (e.g., 256).\n\n      \nPatch dimensionality \\(d\\) is shared with transformer hidden size.\n\n      \nAn optional learnable BOS (beginning-of-sample) latent may be emitted first to stabilize generation.\n\n      \nDiffusion operates on all \\(n\\) patches simultaneously or in rolling windows (discussed in Section 4).\n\n    \n\n  \n\n\n\n\n\nUnified Transformer Architecture and Attention Patterns\n\n\n\n\n  \nThe core of GPT-4o’s image and text generation capabilities lies in its use of a single, shared Transformer architecture to model sequences of interleaved modalities. This section describes how GPT-4o structures its Transformer to support text autoregression, intra-image denoising, and seamless modality transitions — all within the same attention backbone.\n\n  \nThe design builds directly on the Transfusion architecture, enhanced by lessons from Chameleon and architectural stabilization strategies discussed in both.\n\n\n\n\n\nModality-Agnostic Input Pipeline\n\n\n\n\n  \nAll input elements — whether discrete text tokens or continuous image patches — are transformed into a shared embedding space of dimensionality \\(d\\), typically aligned with the model’s hidden size (e.g., 2048 or 4096).\n\n\n\n\n\nInput projection layers:\n\n\n\n\n  \nText tokens\n: Mapped via a token embedding table.\n\n  \nImage patches\n: Projected using one of two techniques:\n    \n\n      \nA \nsimple linear layer\n: suitable for low-compute regimes.\n\n      \nU-Net Down Blocks\n: extract local spatial structure more effectively, as used in Transfusion’s higher-quality configurations.\n\n    \n\n  \n\n  \nThe projected vectors are then augmented with:\n    \n\n      \nRotary positional embeddings (RoPE)\n: for spatial and temporal alignment.\n\n      \nTimestep embeddings\n: when used in diffusion training to condition each vector on its noise level.\n\n    \n\n  \n\n  \nThese embeddings are added before the Transformer layers and ensure that each token — regardless of modality — is given context-aware encoding.\n\n\n\n\n\nTransformer Backbone Design\n\n\n\n\n  \n\n    \nGPT-4o likely uses a variant of the LLaMA-2-style transformer, sharing the following traits with Transfusion and Chameleon:\n\n\n    \n\n      \nLayerNorm variant\n: RMSNorm for stability and scale-insensitive layer normalization.\n\n      \nActivation\n: SwiGLU (gated linear units with Swish) for increased model expressivity.\n\n      \nAttention head scaling\n: high-dimensional queries and keys, enabling large context ranges.\n\n    \n\n  \n\n  \nKey architectural choice: parameter sharing across modalities\n    \n\n      \nAll attention and feedforward layers are shared between modalities.\n\n      \nOnly the input/output projections differ between text and image.\n\n    \n\n  \n\n  \nThis design dramatically simplifies multimodal modeling by not requiring separate encoders or decoders for different modalities, unlike previous approaches (e.g., Flamingo, DALL·E Mini).\n\n\n\n\n\nHybrid Attention Masking\n\n\n\n\n  \n\n    \nA cornerstone of GPT-4o’s unified modeling is the hybrid attention mask used during training and inference. This approach enables:\n\n\n    \n\n      \nCausal attention\n for language modeling.\n\n      \nBidirectional attention within image blocks\n to support diffusion-style denoising.\n\n      \nStrict autoregressive masking across modality boundaries\n, preserving sequence integrity.\n\n    \n\n  \n\n\n\n\n\nExplanation\n\n\n\n\n  \nText tokens (left side) follow strict left-to-right causal attention.\n\n  \nImage patches (middle) attend to each other freely within a BOI–EOI block.\n\n  \nCross-modal edges (text-to-image) are causal — image patches can condition on prior text, but not vice versa.\n\n  \nThis setup is critical to training a Transformer that can smoothly switch between \nnext-token prediction\n and \ndenoising\n objectives without loss of modality context.\n\n\n\n\n\nIntegration of Modality Markers and Position Awareness\n\n\n\n\n  \nGPT-4o uses special tokens to delineate modalities:\n    \n\n      \n<BOI>\n and \n<EOI>\n tokens signal the start and end of image blocks.\n\n      \nThese markers are embedded and fed into the Transformer like regular tokens, allowing the model to learn boundary-aware generation behaviors.\n\n    \n\n  \n\n  \nEach token also receives a positional encoding:\n    \n\n      \nRoPE embeddings: applied per-token across the full sequence.\n\n      \nFor image blocks, these are structured in row-major order (left-to-right, top-to-bottom).\n\n      \nWhen diffusion is active, timestep encodings are added to image latents, supporting noise-conditioning.\n\n    \n\n  \n\n  \nThis approach aligns with Transfusion’s positional encoding strategy, which conditions patch vectors not only on their location but also on their position in the diffusion timeline.\n\n\n\n\n\nAttention Implementation Details\n\n\n\n\n  \nTo implement the hybrid mask effectively:\n    \n\n      \nA binary attention mask is generated at runtime, based on the sequence of modality tokens.\n\n      \nThis mask is injected into the standard self-attention mechanism before softmax:\n\\(\\text{Attention}(Q, K, V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} + M ) V\\)\n        \n\n          \nwhere \\(M\\) is the attention mask with entries:\n            \n\n              \n0 for permitted attention\n\n              \n\\(-\\infty\\) for masked positions\n\n            \n\n          \n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nTo support intra-image bidirectionality, patches within the same image block are identified and grouped. Within each group, bidirectional attention is allowed.\n\n  \n\n  \nThis is straightforward to implement using sliding window logic and token tagging during data preprocessing.\n\n\n\n\n\nTraining Objectives and Modality-Specific Loss Functions\n\n\n\n\n  \n\n    \nA critical innovation in GPT-4o is the use of multiple loss functions applied within a shared Transformer model, tailored to the modality of the output token. Specifically:\n\n\n    \n\n      \nDiscrete text tokens\n are trained using a language modeling (LM) objective.\n\n      \nContinuous image patch vectors\n are trained using a denoising diffusion probabilistic model (DDPM) loss.\n\n    \n\n  \n\n  \n\n    \nThis dual-objective training recipe, first formalized in the Transfusion architecture, allows GPT-4o to optimize the correct learning signal for each modality while sharing the same transformer weights and input sequence stream.\n\n  \n\n\n\n\n\nCombined Loss Function\n\n\n\n\n  \n\n    \nGPT-4o’s total loss is computed as a simple linear combination of the language modeling loss and diffusion loss:\n\n\n\\[\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{LM}} + \\lambda \\cdot \\mathcal{L}_{\\text{DDPM}}\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(\\mathcal{L}_{\\text{LM}}\\): language modeling loss for discrete tokens.\n\n          \n\\(\\mathcal{L}_{\\text{DDPM}}\\): denoising score-matching loss for image patches.\n\n          \n\\(\\lambda\\): weighting coefficient to balance the two losses (often tuned via validation or set to 1.0).\n\n        \n\n      \n\n    \n\n  \n\n  \nThis allows simultaneous optimization of:\n    \n\n      \nLeft-to-right token prediction (for all textual elements)\n\n      \nLatent vector denoising (for image patch reconstruction)\n\n    \n\n  \n\n  \nThis is consistent with Transfusion’s central idea: “combining a discrete distribution loss with a continuous distribution loss to optimize the same model.”\n\n\n\n\n\nLanguage Modeling Loss (Text Tokens)\n\n\n\n\n  \n\n    \nThe LM loss used for text is standard autoregressive next-token prediction:\n\n\n\\[\\mathcal{L}_{\\text{LM}} = \\mathbb{E}_{y}[-\\log P_\\theta(y_i | y_{<i})]\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(y\\): a sequence of discrete tokens (e.g., from a BPE tokenizer).\n\n          \n\\(y_i\\): the token to predict at position \\(i\\).\n\n          \n\\(y_{<i}\\): the prefix up to, but not including, \\(i\\).\n\n          \n\\(P_\\theta\\): the model’s predicted probability distribution, parameterized by \\(\\theta\\).\n\n          \n\\(\\mathbb{E}_y\\): the expectation over the training corpus.\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThis loss is computed per-token, and gradients are backpropagated only through tokens that belong to the text domain.\n\n  \n\n\n\n\n\nDiffusion Objective for Image Patches\n\n\n\n\n  \nGPT-4o uses denoising score matching for learning to generate image patch latents. This is done using a DDPM loss following the formulation from Ho et al. (2020) and adopted in Transfusion and Rolling Diffusion.\n\n\n\n\n\nForward Diffusion Process\n\n\n\n\n  \n\n    \nThe forward process gradually adds Gaussian noise to a clean latent vector:\n\n\n\\[x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(x_0\\): clean latent image patch.\n\n          \n\\(x_t\\): noisy version of \\(x_0\\) at timestep \\(t\\).\n\n          \n\\(\\bar{\\alpha}_t = \\prod_{s=1}^{t}(1 - \\beta_s)\\): cumulative product of the noise schedule.\n\n          \n\\(\\epsilon \\sim \\mathcal{N}(0, I)\\): sampled Gaussian noise.\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThis process is implemented efficiently using the closed-form reparameterization trick that allows sampling \\(x_t\\) in a single step from \\(x_0\\).\n\n  \n\n\n\n\n\nReverse Denoising Loss\n\n\n\n\n  \n\n    \nThe model learns to predict the noise added to each patch via a regression loss:\n\n\n\\[\\mathcal{L}_{\\text{DDPM}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t, c) \\|^2 \\right]\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(x_t\\): the noised patch latent at timestep \\(t\\).\n\n          \n\\(\\epsilon\\): the actual Gaussian noise added to \\(x_0\\).\n\n          \n\\(\\epsilon_\\theta(x_t, t, c)\\): model’s prediction of the noise, conditioned on:\n            \n\n              \n\\(t\\): diffusion step.\n\n              \n\\(c\\): optional context (e.g., text prompt or prior image patches).\n\n            \n\n          \n\n          \n\\(\\| \\cdot \\|^2\\): squared L2 loss.\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThis loss is computed per image, i.e., across all patches in a given BOI–EOI block, rather than per patch token.\n\n  \n\n\n\n\n\nVisualizing the Forward and Reverse Diffusion Process\n\n\n\n\n  \nThe following figure (\nsource\n) illustrates the forward (noising) and reverse (denoising) trajectories used in image patch training.\n\n\n\n\n\n\n\n\n\n  \nThese trajectories are applied in latent space using the VAE-compressed patch representations, allowing for highly efficient image generation.\n\n\n\n\n\nNoise Schedule and Timestep Sampling\n\n\n\n\n  \n\n    \nTransfusion and Rolling Diffusion both use \ncosine noise schedules\n or \nlinear beta schedules\n to determine \\(\\beta_t\\), which controls how much noise is added at each step.\n\n  \n\n  \n\n    \nCommon schedule:\n\n\n\\[\\bar{\\alpha}_t = \\cos^2 \\left( \\frac{t}{T} \\cdot \\frac{\\pi}{2} \\right)\\]\n\n    \n\n      \nAt early steps (low \\(t\\)), noise is minimal → easier samples.\n\n      \nAt later steps (high \\(t\\)), noise increases → training becomes more challenging.\n\n    \n\n  \n\n  \n\n    \nTimestep sampling\n is typically uniform or importance-weighted during training.\n\n  \n\n\n\n\n\nTraining Considerations\n\n\n\n\n  \nGradient accumulation\n is used to manage large batches.\n\n  \nMixed-precision (bf16 or fp16)\n training improves memory efficiency.\n\n  \nModality-specific loss masking\n ensures that:\n    \n\n      \nNo LM loss is applied on latent vectors.\n\n      \nNo DDPM loss is applied on text tokens.\n\n    \n\n  \n\n  \nEach batch may contain:\n    \n\n      \nPure text-only sequences.\n\n      \nText+image sequences.\n\n      \nImage→text (captioning) samples.\n\n      \nFully interleaved documents.\n\n    \n\n  \n\n\n\n\n\nDiffusion-Specific Training Behavior\n\n\n\n\n  \nDuring diffusion training:\n    \n\n      \nThe Transformer conditions on the sequence up to the current patch (text or earlier patches).\n\n      \nThe patch tokens themselves are noisy latents corresponding to time \\(t\\).\n\n      \nThe model predicts noise vectors \\(\\epsilon\\), rather than predicting the patch directly.\n\n    \n\n  \n\n  \nAdditional training details:\n    \n\n      \nThe timestep \\(t\\) is uniformly sampled during training.\n\n      \nA sinusoidal or learnable embedding of \\(t\\) is added to the patch embedding.\n\n      \nTeacher forcing is used: the target for each patch is the actual noise \\(\\epsilon\\) added during the forward step.\n\n    \n\n  \n\n\n\n\n\nTraining Stability and Loss Balancing\n\n\n\n\n  \nChameleon’s findings with regarding to training stability were:\n    \n\n      \nModality mismatch in entropy can cause training instability.\n\n      \nTo stabilize training across text and image losses, Chameleon uses:\n        \n\n          \nQuery-Key Normalization (QK-Norm)\n\n          \nz-loss regularization to prevent logit drift\n\n          \nDropout after attention and MLP layers\n\n        \n\n      \n\n    \n\n  \n\n  \nIn GPT-4o, which avoids discretization, many of these issues are naturally mitigated by:\n    \n\n      \nLower entropy in continuous patch outputs (no softmax required)\n\n      \nSeparate loss paths for text (cross-entropy) and images (regression)\n\n    \n\n  \n\n  \nHowever, hyperparameter tuning (especially for \\(\\lambda\\)) remains critical, as over-weighting the image loss can reduce text fluency, and vice versa.\n\n\n\n\n\nDiffusion Sampling and Row-by-Row Decoding with Rolling Diffusion\n\n\n\n\n  \n\n    \nAt inference time, GPT-4o must efficiently convert generated image patch latents into full high-fidelity images. While standard diffusion models typically denoise all image latents in parallel across multiple timesteps, GPT-4o likely adopts the Rolling Diffusion strategy to enable streamed, row-by-row decoding that scales more effectively for long, multimodal sequences and supports causal generation patterns.\n\n  \n\n  \n\n    \nRolling Diffusion introduces a method for performing sequential denoising over spatial dimensions by reparameterizing diffusion time as a function of patch location. GPT-4o can incorporate this technique to match its autoregressive constraint while still benefiting from the high fidelity of diffusion-based image synthesis.\n\n  \n\n\n\n\n\nMotivation for Rolling Diffusion\n\n\n\n\n  \n\n    \nThe traditional DDPM sampling procedure expects to denoise an entire latent grid \\(z \\in \\mathbb{R}^{n \\times d}\\) in parallel across all patches at every timestep \\(t \\in \\{T, T-1, ..., 1\\}\\). While effective for static image generation, this method is incompatible with:\n\n\n    \n\n      \nAutoregressive token emission\n (GPT-4o generates patch tokens one-at-a-time or in segments).\n\n      \nStreaming generation\n (e.g., real-time interleaved image + text outputs).\n\n      \nMemory constraints\n (sampling all latents at every step is expensive for long sequences).\n\n    \n\n  \n\n  \n\n    \nTo address these constraints, Rolling Diffusion proposes a local, position-aware reparameterization of the diffusion schedule, allowing for sequential (e.g., row-wise or patch-wise) generation.\n\n  \n\n\n\n\n\nLocal Time Reparameterization\n\n\n\n\n  \n\n    \nIn Rolling Diffusion, the noise schedule is made spatially dependent. Instead of a global timestep \\(t\\) being applied uniformly across all patches, a position-adjusted local timestep \\(t_k\\) is computed for each patch \\(k\\):\n\n\n\\[t_k = \\frac{k + t}{W}\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(k\\): index of the patch in the current image sequence.\n\n          \n\\(W\\): total number of patches in a denoising window (e.g., a row).\n\n          \n\\(t\\): global diffusion timestep.\n\n          \n\\(t_k\\): adjusted timestep for patch \\(k\\) — later patches receive more noise.\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThis creates a rolling schedule, where denoising sweeps over the image spatially, producing cleaner outputs as we progress.\n\n  \n\n  \n\n    \nThe following figure (\nsource\n) visualizes the global rolling diffusion process and its local time reparameterizatiod in Rolling Diffusion. The global diffusion denoising time \\(t\\) (vertical axis) is mapped to a local time \\(t_{k}\\) for a frame \\(k\\) (horizontal axis). The local time is then used to compute the diffusion parameters \\(α_{tk}\\) and \\(σ_{tk}\\) . On the right, we show how the same local schedule can be applied to each sequence of frames based on the frame index \\(w\\). The nontrivial part of sampling the generative process only occurs in the sliding window as it gets shifted over the sequence.\n\n  \n\n\n\n\n\n\n\n\n\n  \nThis strategy enables the model to denoise and emit patches incrementally, staying in line with GPT-4o’s autoregressive framework.\n\n\n\n\n\nReverse Process with Rolling Denoising\n\n\n\n\n  \n\n    \nThe reverse step in Rolling Diffusion is a modification of the standard DDPM reverse equation, adapted to patch-wise sequential generation. For a given patch \\(x_t^k\\), the denoised value is computed as:\n\n\n\\[x_{t_k - 1}^k = \\frac{1}{\\sqrt{\\alpha_{t_k}}} \\left(x_{t_k}^k - \\frac{1 - \\alpha_{t_k}}{\\sqrt{1 - \\bar{\\alpha}_{t_k}}} \\epsilon_\\theta(x_{t_k}^k, t_k)\\right) + \\sigma_{t_k} z\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(x_{t_k}^k\\): current noisy latent patch at local time \\(t_k\\).\n\n          \n\\(\\alpha_{t_k}, \\bar{\\alpha}_{t_k}\\): noise schedule parameters for local time \\(t_k\\).\n\n          \n\\(\\epsilon_\\theta(\\cdot)\\): model’s predicted noise for patch \\(k\\) at that timestep.\n\n          \n\\(\\sigma_{t_k}\\): scale of Gaussian noise reintroduced at that step.\n\n          \n\\(z \\sim \\mathcal{N}(0, I)\\): noise sampled per step.\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nOnly a subset of patches (e.g., the current row) is denoised at each step. Previously denoised patches can be cached and fed as context to subsequent steps, allowing generation to proceed efficiently and causally.\n\n  \n\n\n\n\n\nLoss Function for Windowed Denoising\n\n\n\n\n  \n\n    \nDuring training, a corresponding localized loss function is used. The model learns to predict noise for only the patches in the current window:\n\n\n\\[\\mathcal{L}_{\\text{win}, \\theta} = \\sum_{k \\in \\text{win}(t)} a(t_k) \\left\\| x_0^k - \\hat{x}_0^k \\right\\|^2\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(\\text{win}(t)\\): the window of patches being denoised at time \\(t\\).\n\n          \n\\(a(t_k)\\): time-dependent weighting function (e.g., based on signal-to-noise ratio).\n\n          \n\\(x_0^k\\): the original clean patch.\n\n          \n\\(\\hat{x}_0^k\\): reconstruction from model prediction at local time \\(t_k\\).\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThis loss ensures the model focuses on the most relevant patches per step, keeping memory and compute cost bounded.\n\n  \n\n  \n\n    \nThe following figure (\nsource\n) illustrates the patchwise rolling denoising trajectory across a spatial image grid.\n\n  \n\n\n\n\n\n\n\n\nApplication in GPT-4o\n\n\n\n\n  \n\n    \nFor GPT-4o, this decoding strategy offers key advantages:\n\n\n    \n\n      \nAutoregressive compatibility\n: Image patch latents can be denoised and emitted progressively, matching the causal output loop used for text.\n\n      \nStreaming images\n: Patches can be rendered in order (e.g., row-by-row), enabling real-time visualization.\n\n      \nLow memory footprint\n: Only a small working set of patches needs to be active at any timestep.\n\n    \n\n  \n\n  \n\n    \nIn practice, GPT-4o may emit an entire image patch block after denoising it through a rolling window and follow it immediately with an \n<EOI>\n token. The decoder (a VAE) then reconstructs the final image from these denoised latents.\n\n  \n\n  \n\n    \nThis technique bridges the gap between \ndiffusion model fidelity\n and \nlanguage model autoregressiveness\n, which is essential for GPT-4o’s seamless multimodal output behavior.\n\n  \n\n\n\n\n\nImage Rendering, VAE Decoding, and Final Output Construction\n\n\n\n\n  \nOnce GPT-4o has generated the full sequence of image patch latents via its Transformer and diffusion mechanism, the model must decode these latent vectors back into a viewable RGB image. This is handled by a VAE decoder, a lightweight, trainable module that transforms low-dimensional continuous vectors into pixel-level outputs.\n\n  \nThis final rendering step is crucial for preserving image fidelity while maintaining the compactness and speed of latent-space generation. GPT-4o appears to follow the Transfusion decoder architecture, optionally incorporating elements from Rolling Diffusion to support structured decoding and temporal conditioning.\n\n\n\n\n\nPatch Sequence to Latent Grid Reconstruction\n\n\n\n\n  \n\n    \nDuring inference, the generated image latents take the form:\n\n\n\\[[z_1, z_2, \\ldots, z_n] \\in \\mathbb{R}^{n \\times d}\\]\n\n    \n\n      \nwhere:\n        \n\n          \n\\(n\\): total number of patches (e.g., 256 for 16×16 grid)\n\n          \n\\(d\\): latent dimensionality (e.g., 8, 16, 32)\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nThese vectors are arranged into a 2D grid, respecting the original spatial layout (top-to-bottom, left-to-right). This patch grid corresponds to the output size of the VAE encoder and serves as input to the decoder.\n\n  \n\n  \n\n    \nThe process likely follows the same flow as in Transfusion which utilizes the VAE encoder and decoder stack used to convert raw images into latent patch tokens and reconstruct them into RGB space, as indicated above.\n\n  \n\n\n\n\n\nVAE Decoder Architecture\n\n\n\n\n  \n\n    \nThe decoder architecture used in GPT-4o, as suggested in the Transfusion paper, is a lightweight convolutional U-Net-style upsampler. It transforms the compact latent representation into a high-resolution image by progressively upsampling the spatial dimensions.\n\n  \n\n  \nKey components:\n    \n\n      \nResidual blocks\n: for nonlinear transformation at each resolution level.\n\n      \nTransposed convolutions or upsampling layers\n: to increase resolution step-by-step.\n\n      \nGroup norm or layer norm\n: to stabilize training.\n\n      \nTanh output layer\n: to clamp output pixel values to \\([-1, 1]\\).\n\n    \n\n  \n\n  \nOptional additions:\n    \n\n      \nTimestep embedding conditioning\n (used in diffusion guidance)\n\n      \nCross-attention from prompt tokens\n (used in conditional generation settings)\n\n    \n\n  \n\n\n\n\n\n\n  \nNote: In higher-fidelity settings, the decoder can include U-Net Up Blocks trained jointly with the Transformer, as done in the 7B Transfusion model.\n\n\n\n\n\nPost-Processing and Image Reconstruction\n\n\n\n\n  \nThe final output of the decoder is a 3-channel image tensor:\n\n\n\n\n\\[\\hat{x} \\in \\mathbb{R}^{H \\times W \\times 3}\\]\n\n\n\n  \nTypically with \\(H = W = 256\\), depending on the training resolution.\n\n  \nThe image tensor is post-processed as follows:\n    \n\n      \nDe-normalized from \\([-1, 1]\\) to \\([0, 255]\\)\n\n      \nOptionally converted from \nfloat32\n to \nuint8\n\n      \nConverted to standard formats (e.g., PNG, JPEG) for rendering or output\n\n    \n\n  \n\n  \nThis step is generally handled outside the transformer loop and is purely feedforward — it does not influence subsequent text or image token generation.\n\n\n\n\n\nImage Completion and the \n<EOI>\n Token\n\n\n\n\n  \n\n    \nThe end of an image block is marked by the generation of the special \n<EOI>\n token:\n\n\n    \n\n      \nSignals the termination of image patch generation.\n\n      \nTells the decoding pipeline to finalize the image rendering process.\n\n      \nTriggers a switch back into language modeling mode in the autoregressive loop.\n\n    \n\n  \n\n  \nThe presence of \n<EOI>\n ensures the image is fully delimited in the sequence, allowing for:\n    \n\n      \nClear boundaries for downstream token processing.\n\n      \nMultimodal alternation: image → text → image → etc.\n\n    \n\n  \n\n  \nThis enables GPT-4o to support rich multimodal document generation, including captioned illustrations, inline graphics in essays, or visual dialogue.\n\n\n\n\n\nTraining Considerations for the Decoder\n\n\n\n\n  \n\n    \nThe decoder is usually pretrained before GPT-4o training, then frozen during Transformer training to reduce complexity and ensure latent consistency. Transfusion converts images to and from latent representations using a pretrained VAE, and then into patch representations with either a simple linear layer or U-Net down blocks.\n\n  \n\n  \n\n    \nHowever, in large-scale setups (e.g., 7B models trained on 2T tokens), joint fine-tuning of the decoder (especially U-Net blocks) has been shown to improve visual fidelity and compression robustness. GPT-4o likely adopts a hybrid regime:\n\n    \n\n      \nPretrain the decoder on reconstruction loss.\n\n      \nFine-tune with frozen weights for initial scaling.\n\n      \nUnfreeze for high-fidelity final-stage training.\n\n    \n\n  \n\n\n\n\n\nIntegration with Text Generation and Modality Interleaving\n\n\n\n\n  \nOne of GPT-4o’s defining capabilities is its ability to fluidly interleave text and image generation within a single autoregressive sequence. Unlike systems that bolt image generation onto a language model via tool use (e.g., DALL·E via API calls), GPT-4o treats image output as native to the model, making text and image generation indistinguishable from the perspective of the transformer.\n\n  \nThis section describes how modality interleaving is achieved, how control tokens like \n<BOI>\n and \n<EOI>\n are used to guide generation modes, and how causal context is preserved across transitions.\n\n\n\n\n\nText and Image Blocks in the Token Stream\n\n\n\n\n  \nGPT-4o generates output one token at a time. The stream can switch modalities at any position. For example:\n\n\n\n\n\n[\"A majestic mountain.\"] → `<BOI>` [latent image patches] `<EOI>` → [\"It’s located in the Himalayas.\"]\n\n\n\n\n\n  \nEach mode (text vs. image) is identified in the token sequence using modality markers:\n    \n\n      \n<BOI>\n (Begin of Image)\n: triggers image generation.\n\n      \n<EOI>\n (End of Image)\n: signals end of image patch output.\n\n      \nEverything in between is treated as \ncontinuous latent vectors\n.\n\n      \nEverything before/after is \ndiscrete tokens\n from the language model vocabulary.\n\n    \n\n  \n\n  \nThe process of how sequences are constructed with interleaved modalities using BOI and EOI tokens likely follows the proposal in the Transfusion architecture. The model learns to recognize modality boundaries and adapt its behavior accordingly.\n\n\n\n\n\nMode Switching During Generation\n\n\n\n\n  \nGPT-4o dynamically switches between two operational modes:\n\n\n\n\n\nText Mode (Language Modeling)\n\n\n\n\n  \nActivated when generating text tokens.\n\n  \nUses a causal transformer with next-token prediction.\n\n  \nContinues until \n<BOI>\n is generated or the sequence ends.\n\n\n\n\n\nImage Mode (Diffusion Sampling)\n\n\n\n\n  \nActivated when \n<BOI>\n is emitted.\n\n  \nThe model stops predicting from a vocabulary and begins predicting continuous latent vectors.\n\n  \nThese vectors are initialized as Gaussian noise and refined via diffusion over a fixed number of steps \\(T\\).\n\n  \n\n    \nEnds when \n<EOI>\n is generated.\n\n  \n\n  \nThis switching behavior is governed by a decoding algorithm that alternates between LM and DDPM decoding based on token type, likely similar to Transfusion’s decoding algorithm which switches between two modes: LM and diffusion. Once the diffusion process has ended, they append an EOI token to the predicted image and switch back to LM mode.\n\n\n\n\n\nCausal Context Preservation\n\n\n\n\n  \n\n    \nGPT-4o ensures that text and image blocks are causally conditioned on all preceding content, whether textual or visual.\n\n  \n\n  \nExamples:\n    \n\n      \nText following an image can reference or describe it.\n\n      \nAn image generated after text is conditioned on the preceding caption.\n\n      \nEach image block is self-contained: patch latents can attend bidirectionally to each other but only causally to tokens or images that came before it.\n\n    \n\n  \n\n  \nThis is implemented through carefully crafted attention masks, as discussed in \nsec3\n.\n\n\n\n\n\nPrompt-Image Conditioning (Text → Image)\n\n\n\n\n  \nWhen generating an image from a prompt:\n    \n\n      \nThe text prompt (tokens before \n<BOI>\n) is encoded using standard transformer self-attention.\n\n      \nThe image patch generation is conditioned on these tokens via cross-attention and contextual embeddings.\n\n      \nDuring diffusion, the image patch latents attend to the prior text but not to future tokens.\n\n    \n\n  \n\n  \nThis setup allows GPT-4o to:\n    \n\n      \nTranslate rich textual descriptions into images.\n\n      \nPreserve alignment between prompt and image content.\n\n      \nAchieve strong CLIP alignment and FID scores, as shown in Transfusion benchmarks.\n\n    \n\n  \n\n\n\n\n\nCaptioning (Image → Text)\n\n\n\n\n  \nWhen generating text from an image:\n    \n\n      \nThe model first decodes image patches between \n<BOI>\n and \n<EOI>\n.\n\n      \nThese continuous patch vectors are processed and embedded into the transformer’s sequence.\n\n      \nThe next set of tokens is predicted autoregressively, using both the prior text and the embedded image as context.\n\n    \n\n  \n\n  \nThis approach allows GPT-4o to perform tasks such as:\n    \n\n      \nDescriptive captioning\n\n      \nScene understanding\n\n      \nVisual question answering\n\n    \n\n  \n\n  \nThe following figure (\nsource\n) shows examples of multimodal prompts and outputs generated by the Transfusion model, demonstrating text-to-image and image-to-text switching.\n\n\n\n\n\n\n\n\n\n  \nGPT-4o likely uses a similar decoding structure, with image tokens generated via latent diffusion and text tokens sampled autoregressively.\n\n\n\n\n\nMultimodal Documents and Interleaved Outputs\n\n\n\n\n  \nGPT-4o can produce long-form documents with alternating modalities, such as:\n\n\n\n\n\n[\"Here’s the blueprint:\"] → `<BOI>` [Image 1] `<EOI>` → [\"Let’s annotate it.\"] → `<BOI>` [Image 2] `<EOI>` → [\"These two diagrams are connected.\"]\n\n\n\n\n\n  \n\n    \nThe autoregressive transformer handles the entire sequence uniformly, switching losses and decoding behavior based on token type. The use of shared embedding dimensions and positional encodings ensures continuity across modalities.\n\n  \n\n  \n\n    \nThis makes GPT-4o one of the first models capable of natively authoring mixed-media content without needing external rendering calls or modality-specific submodels.\n\n  \n\n\n\n\n\nArchitectural Scaling, Training Regimes, and Implementation Strategy\n\n\n\n\n  \nThe success of GPT-4o’s native multimodal generation hinges not only on its model design but also on scalable training, modality-balanced data sampling, and careful loss orchestration across discrete and continuous domains. The techniques described in both the Transfusion and Chameleon papers offer a glimpse into how GPT-4o might have been trained at scale to support both text and image generation in a unified Transformer.\n\n\n\n\n\nModel Scaling and Configuration\n\n\n\n\n  \n\n    \nGPT-4o likely follows a Transformer architecture family similar to what is presented in Transfusion Section 4.1, with models ranging from hundreds of millions to multiple billions of parameters. These configurations balance depth, width, and attention granularity for scalable multimodal modeling.\n\n  \n\n  \n\n    \nThe following figure (\nsource\n) lists the model sizes and layer configurations used in Transfusion’s experimental suite, which GPT-4o’s variants likely resemble.\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Count\n\n\nLayers\n\n\nEmbedding Dim\n\n\nAttention Heads\n\n\n\n\n\n\n\n\n\n\n0.16B\n\n\n16\n\n\n768\n\n\n12\n\n\n\n\n\n\n0.37B\n\n\n24\n\n\n1024\n\n\n16\n\n\n\n\n\n\n0.76B\n\n\n24\n\n\n1536\n\n\n24\n\n\n\n\n\n\n1.4B\n\n\n24\n\n\n2048\n\n\n16\n\n\n\n\n\n\n7B\n\n\n32\n\n\n4096\n\n\n32\n\n\n\n\n\n\n\n\n\n\n\n\n  \nThese architectures are LLaMA-style transformers with the following enhancements:\n    \n\n      \nSwiGLU activation (nonlinearity with gating)\n\n      \nRoPE (rotary positional encoding) for long-range sequence coherence\n\n      \nRMSNorm or LayerNorm pre-norm configurations\n\n      \nModality-shared attention and MLP layers\n\n    \n\n  \n\n\n\n\n\nTraining Data: Multimodal Sampling Strategy\n\n\n\n\n  \n\n    \nGPT-4o is likely trained on a mixture of text and image-caption pairs, maintaining balance across modalities for every batch. Transfusion samples 0.5T tokens (patches) from two datasets at a 1:1 token ratio. For text, it uses the LLaMA 2 tokenizer and corpus. For images, it uses 380M Shutterstock images and captions, with each image resized to 256×256 resolution.\n\n  \n\n  \nTo achieve 2T tokens total (as in the final 7B model), Transfusion also:\n    \n\n      \nIncludes additional public image-caption datasets (e.g., CC12M)\n\n      \nUpsamples images of people to improve diversity\n\n      \nUses 1T text tokens + ~1T image patches over 3.5B image-caption pairs\n\n    \n\n  \n\n  \nThis aligns with GPT-4o’s capability to handle high-quality images and a broad range of visual concepts.\n\n\n\n\n\nTraining Objectives and Scheduling\n\n\n\n\n  \nAs detailed in earlier sections, GPT-4o uses:\n    \n\n      \nLanguage modeling loss for discrete tokens\n\n      \nDiffusion loss for continuous image patches\n\n    \n\n  \n\n  \nThese are combined with a scalar \\(\\lambda\\) to control balance. In Transfusion, this was statically set, but in practice could be:\n    \n\n      \nScheduled dynamically (e.g., curriculum-based)\n\n      \nModality-dependent (e.g., based on image size or patch count)\n\n    \n\n  \n\n  \nGPT-4o likely employs a curriculum-based warmup where early training emphasizes text (to stabilize generation) and gradually incorporates image diffusion.\n\n\n\n\n\nOptimizer and Hyperparameter Choices\n\n\n\n\n  \n\n    \nImplementation parameters drawn from Transfusion and Chameleon include:\n\n\n    \n\n      \nOptimizer\n: AdamW\n\n      \nLearning rate\n: 1e-4 with cosine decay\n\n      \nWeight decay\n: 0.1\n\n      \nGradient clipping\n: 1.0 (global norm)\n\n      \nBatch size\n: 1M tokens per batch (including both modalities)\n\n      \nDropout\n: 0.1 after attention and MLP layers\n\n      \nPrecision\n: bfloat16 or mixed-precision (depending on TPU/GPU setup)\n\n    \n\n  \n\n  \n\n    \nRegularization and stabilization strategies used in GPT-4o may include:\n\n    \n\n      \nz-loss\n: prevents logit drift in softmax vocab predictions (used in Chameleon)\n\n      \nQuery-Key Normalization (QK-Norm)\n: stabilizes attention variance between text and image blocks\n\n      \nDropToken augmentation\n: probabilistically masking tokens to improve robustness\n\n    \n\n  \n\n\n\n\n\nDiscretization Ablation and Transfusion Superiority\n\n\n\n\n  \nThe following figure (\nsource\n) compares the performance of Transfusion (continuous latents) vs. Chameleon (discrete tokens) on MS-COCO in terms of FID and CLIP scores.\n\n\n\n\n\n\n\n\n\n  \nFindings relevant to GPT-4o:\n    \n\n      \nTransfusion achieves 2× better FID with less than 1/3 the compute.\n\n      \nTransfusion outperforms Chameleon on text-to-image and image-to-text tasks.\n\n      \nDiscretization in Chameleon introduces a fidelity bottleneck that GPT-4o avoids by operating on continuous latents.\n\n    \n\n  \n\n  \nThus, GPT-4o’s use of native VAE-based latent patch generation offers significant quality advantages and better scaling.\n\n\n\n\n\nSummary of Design Decisions and Forward-Looking Considerations\n\n\n\n\n  \nGPT-4o’s native image generation capabilities represent a major advancement in multimodal foundation models. This section recaps the design choices that enable such functionality and outlines how these ideas can naturally extend beyond static images to richer modalities, including video, audio, and fully interactive visual dialogue.\n\n\n\n\n\nUnified Modality Architecture\n\n\n\n\n  \nGPT-4o is built on a single transformer backbone that processes and generates:\n    \n\n      \nDiscrete text tokens via autoregressive language modeling.\n\n      \nContinuous image latents via diffusion-based denoising.\n\n      \nModality switch tokens (\n<BOI>\n, \n<EOI>\n) for toggling modes.\n\n    \n\n  \n\n  \nThis architecture eliminates the need for modality-specific submodules (e.g., separate encoders for text and vision), and instead relies on:\n    \n\n      \nShared attention and MLP blocks.\n\n      \nModality-specific projection layers at input/output boundaries.\n\n      \nA unified token stream, processed in the same way regardless of content.\n\n    \n\n  \n\n  \nThis unified design facilitates seamless interleaving of text and images, supporting applications like:\n    \n\n      \nImage captioning and generation.\n\n      \nMultimodal dialogue (e.g., ask–show–explain).\n\n      \nStructured document synthesis (e.g., scientific reports with figures).\n\n    \n\n  \n\n\n\n\n\nDiffusion as the Bridge for Continuous Generation\n\n\n\n\n  \n\n    \nGPT-4o adopts latent-space diffusion (inspired by Transfusion and Rolling Diffusion) rather than pixel-space generation or discrete-token prediction for images.\n\n  \n\n  \nKey benefits:\n    \n\n      \nHigher fidelity\n: avoids quantization artifacts found in Chameleon-style VQ-VAEs.\n\n      \nLower compute\n: denoising small patch sequences in latent space is cheaper than pixel-level modeling.\n\n      \nScalability\n: enables partial image generation (e.g., streaming rows), real-time visualization, and flexible conditioning.\n\n    \n\n  \n\n  \nRolling Diffusion adds further optimizations by enabling windowed denoising and row-wise patch rollout, aligning perfectly with GPT-4o’s causal transformer nature.\n\n\n\n\n\nComparison to Chameleon: Design Tradeoffs\n\n\n\n\n  \n\n    \nChameleon takes a different approach: it discretizes all inputs, including images, into a shared token vocabulary. While elegant in its simplicity, this introduces performance tradeoffs.\n\n  \n\n  \n\n    \nThe following figure (\nsource\n) shows an example of a mixed-modality sequence generated by Chameleon, highlighting its early-fusion token strategy.\n\n  \n\n\n\n\n\n\n\n\n\n  \nIn contrast, GPT-4o:\n    \n\n      \nPreserves image fidelity by avoiding quantization.\n\n      \nUses separate objectives (LM + DDPM) for each modality, improving specialization.\n\n      \nAdds complexity (e.g., diffusion decoder, special scheduling), but yields significantly better results.\n\n    \n\n  \n\n  \nThe Transfusion vs. Chameleon benchmarks in Section 8 demonstrate this tradeoff clearly: Transfusion (and by extension, GPT-4o) achieves lower FID, higher CLIP, and better sample quality at a fraction of the compute cost.\n\n\n\n\n\nExtensibility to Future Modalities\n\n\n\n\n  \nThe architecture of GPT-4o is modality-general. The principles underpinning its design extend naturally to other types of data:\n\n\n\n\n\nVideo\n\n\n\n\n  \nFrame sequences could be encoded as patch-latent stacks (spatial × temporal).\n\n  \nRolling Diffusion already proposes local time reparameterization, which can extend to temporal denoising over time windows.\n\n  \nA special \n<BOV>\n (Begin of Video) / \n<EOV>\n (End of Video) token could delimit video segments.\n\n\n\n\n\nAudio\n\n\n\n\n  \nSpectrograms or compressed audio latents can be modeled as 2D or 1D patch sequences.\n\n  \nA transformer trained with autoregressive audio modeling (or diffusion in latent audio space) could be interleaved similarly.\n\n\n\n\n\n3D and Scene Graphs\n\n\n\n\n  \nWith continuous patch embeddings, the model could handle voxels, mesh descriptors, or point clouds.\n\n  \nScenes can be represented in latent blocks similar to image rows.\n\n\n\n\n\nGPT-4o as a Foundation for Future Multimodal Interfaces\n\n\n\n\n  \nBy tightly integrating continuous and discrete modalities in a single model with a flexible generation schedule, GPT-4o opens the door to:\n    \n\n      \nVisual conversational agents\n: fluent alternation between showing and telling.\n\n      \nInteractive UIs\n: generating forms, diagrams, buttons, and text inline.\n\n      \nMultimodal programming assistants\n: generating UI previews alongside code.\n\n      \nMultimodal retrieval and synthesis\n: combining structured and unstructured inputs in a cohesive output.\n\n    \n\n  \n\n  \nIts design demonstrates that multimodal intelligence doesn’t require stitching separate models together — it can emerge from co-trained, loss-specialized Transformers with unified autoregressive behavior.\n\n\n\n\n\nEvaluation, Sample Quality, and Benchmark Comparisons\n\n\n\n\n  \nTo assess the effectiveness of GPT-4o’s image generation capabilities, we must examine how similar architectures — notably Transfusion and Chameleon — are evaluated on standard benchmarks. This section outlines the key metrics used, presents comparative performance results, and discusses sample quality in the context of multimodal generative tasks.\n\n\n\n\n\nEvaluation Metrics for Image Quality and Prompt Alignment\n\n\n\n\n  \nThe two most commonly used metrics for evaluating generated images in research literature are:\n\n\n\n\n\nFID (Fréchet Inception Distance)\n\n\n\n  \nMeasures the distributional distance between generated images and real ones.\n\n  \nLower is better.\n\n  \nEvaluated using InceptionNet features extracted from real vs. generated images.\n\n\n\n\n\nCLIP Score\n\n\n\n  \nMeasures semantic alignment between a prompt and generated image.\n\n  \nHigher is better.\n\n  \n\n    \nComputed as cosine similarity between CLIP embeddings of image and prompt.\n\n  \n\n  \nAdditional metrics sometimes used:\n    \n\n      \nIS (Inception Score)\n: assesses realism and diversity.\n\n      \nRec (Reconstruction error)\n: for conditional generation.\n\n      \nHuman preference studies\n: for qualitative evaluations.\n\n    \n\n  \n\n\n\n\n\nTransfusion Benchmark Performance\n\n\n\n\n  \n\n    \nTransfusion conducted comprehensive evaluations on MS-COCO and other standard datasets, using the above metrics.\n\n  \n\n  \n\n    \nThe following figure (\nsource\n) summarizes Transfusion’s performance in terms of FID and CLIP across multiple model sizes.\n\n  \n\n\n\n\n\n\n\n\n\n  \nKey results:\n    \n\n      \nTransfusion achieves significantly lower FID than Chameleon and TokenT5.\n\n      \nCLIP scores are highly competitive, often within 1–2% of DALL·E-2-style models.\n\n      \nLatent-space generation outperforms discrete-token-based generation in every modality pair (text → image, image → text, mixed).\n\n    \n\n  \n\n  \nThese findings support GPT-4o’s use of continuous latent modeling and validate the choice of diffusion over discrete token generation.\n\n\n\n\n\nChameleon Performance Snapshot\n\n\n\n\n  \n\n    \nChameleon also provides evaluation metrics, with a focus on token-level performance across mixed-modality tasks.\n\n  \n\n  \n\n    \nThe following figure (\nsource\n) shows Chameleon’s mixed-modality prompt evaluation interface, which combines image and text inputs/outputs for qualitative inspection.\n\n  \n\n\n\n\n\n\n\n\n\n  \nDespite its simplicity and elegant early-fusion architecture, Chameleon’s discrete tokenization bottleneck reduces image fidelity:\n    \n\n      \nFID lags behind continuous-latent systems.\n\n      \nCLIP alignment drops for high-resolution prompts with subtle details.\n\n      \nQualitative outputs exhibit blocky or aliased textures.\n\n    \n\n  \n\n\n\n\n\nImplications for GPT-4o Evaluation\n\n\n\n\n  \nGPT-4o likely exceeds the performance of Transfusion in production settings, for several reasons:\n    \n\n      \nMore diverse training data\n: potentially 10× larger corpora.\n\n      \nBetter CLIP alignment\n: via joint training or distillation with ViT-G or similar.\n\n      \nOptimized VAE + transformer co-training\n: for tighter compression-reconstruction fidelity.\n\n      \nIn-the-loop human evaluation\n: as used in OpenAI model alignment protocols.\n\n    \n\n  \n\n  \nFurthermore, GPT-4o could support interactive sampling metrics:\n    \n\n      \nGeneration speed vs. quality tradeoffs.\n\n      \nStreaming patch visualizations.\n\n      \nRow-wise FID or CLIP at intermediate decoding stages.\n\n    \n\n  \n\n  \nThese capabilities would place GPT-4o in a different regime of evaluation: less about pure realism, and more about pragmatic coherence, prompt control, and user-alignment.\n\n\n\n\n\nReferences\n\n\n\n\n  \nIntroducing 4o Image Generation\n\n  \nChameleon: Mixed-Modal Early-Fusion Foundation Models\n\n  \nTransfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model\n\n  \nRolling Diffusion Models\n\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/gpt4o-native-image-generation/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nNatural Language Processing • NLP Tasks\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nNamed Entity Recognition (NER)\n    \n\n      \nDefinition and Purpose\n\n      \nProcess and Techniques\n\n      \nCommon Architectures and Models\n\n      \nSub-types of NER\n\n      \nApplications of NER\n\n      \nChallenges and Considerations\n\n      \nFuture Directions\n\n    \n\n  \n\n  \nDependency Parsing\n    \n\n      \nDefinition and Purpose\n\n      \nProcess and Techniques\n\n      \nCommon Architectures and Models\n\n      \nSub-types of Dependency Parsing\n\n      \nApplications of Dependency Parsing\n\n      \nChallenges and Considerations\n\n      \nFuture Directions\n\n    \n\n  \n\n  \nSentiment Analysis\n    \n\n      \nDefinition and Purpose\n\n      \nProcess and Techniques\n\n      \nCommon Architectures and Models\n\n      \nApplications of Sentiment Analysis\n\n      \nChallenges and Considerations\n\n      \nFuture Directions\n\n    \n\n  \n\n  \nText Summarization\n    \n\n      \nDefinition and Purpose\n\n      \nTypes of Text Summarization\n\n      \nMethodologies and Models\n\n      \nApplications of Text Summarization\n\n      \nChallenges and Considerations\n\n      \nFuture Directions\n\n    \n\n  \n\n  \nQuestion Answering\n    \n\n      \nDefinition and Purpose\n\n      \nTypes of Question Answering Systems\n\n      \nMethodologies and Models\n\n      \nApplications of Question Answering\n\n      \nChallenges and Considerations\n\n      \nFuture Directions\n\n      \nFAQs\n        \n\n          \nWhat are the types of Question Answering systems?\n\n          \nWhat is the difference between open- and closed-book QA?\n\n          \nHow are open-book and open-domain QA different?\n\n        \n\n      \n\n    \n\n  \n\n  \nText Classification\n    \n\n      \nDefinition and Purpose\n\n      \nTypes of Text Classification\n\n      \nMethodologies and Models\n\n      \nApplications of Text Classification\n\n      \nChallenges and Considerations\n\n      \nFuture Directions\n\n    \n\n  \n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \nThis primer examines several tasks that can be effectively addressed using Natural Language Processing (NLP).\n\n\n\n\n\nNamed Entity Recognition (NER)\n\n\n\n\n  \nNamed Entity Recognition (NER) is a crucial component of NLP that plays a vital role in information extraction and data - analysis. It focuses on identifying and categorizing named entities within text into predefined categories, which can significantly enhance the understanding and interpretation of large volumes of text.\n\n  \nIn other words, NER stands as a cornerstone in the realm of NLP, offering a pathway to transform unstructured text into structured, actionable data. Its ability to identify and categorize entities accurately opens doors to a myriad of applications across various industries, making it an indispensable tool in the age of information.\n\n  \nHere is a detailed look at NER, including its process, applications, methodologies, and advancements.\n\n\n\n\n\nDefinition and Purpose\n\n\n\n  \nPrimary Goal\n: NER aims to locate and classify named entities mentioned in text into specific categories such as names of individuals, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n\n  \nImportance in NLP\n: As a fundamental task in NLP, NER helps in structuring and categorizing unstructured text, making it a critical component for tasks like data retrieval, analysis, and understanding.\n\n\n\n\n\nProcess and Techniques\n\n\n\n\n  \nNER operates in two stages: (i) detection of a named entity, followed by its (ii) categorization/classification. The following slide, sourced from the \nStanford CS224n course\n, illustrates this:\n\n\n\n\n\n\n\n\n\n  \nEntity Identification\n: The first step in NER is to identify a potential named entity within a body of text. This can be a single word or a sequence of words forming a name.\n\n  \nClassification\n: Once an entity is identified, it’s classified into predefined categories like ‘Person’, ‘Organization’, ‘Location’, etc.\n\n  \nContextual Analysis\n: NER systems use the context around each identified entity for accurate classification. This involves analyzing the surrounding words and understanding the entity’s role within the sentence.\n\n  \nWord Vector Analysis\n: Modern NER systems often use word vectors, representations of words in a multidimensional space, to capture semantic and syntactic meanings.\n\n\n\n\n\nCommon Architectures and Models\n\n\n\n  \nBidirectional LSTM (BiLSTM) with CRF\n: BiLSTM processes text data in both forward and backward directions, capturing context more effectively. The Conditional Random Field (CRF) layer then uses this context to classify entities more accurately.\n\n  \nTransformer-based Models\n: With the advent of Transformer models like BERT, NER systems have significantly improved. These models capture a deeper and more nuanced understanding of context, which is essential for accurate entity recognition and classification.\n\n\n\n\n\nSub-types of NER\n\n\n\n  \nFine-Grained NER\n: This involves categorizing entities into more specific sub-categories, offering a more detailed analysis. For example, instead of just ‘Person’, it might classify entities as ‘Artist’, ‘Politician’, etc.\n\n  \nCross-Lingual NER\n: Identifies and categorizes named entities in multiple languages, crucial for global applications and multilingual data sets.\n\n  \nReal-Time NER\n: Designed for immediate processing of text data, such as in live news feeds or social media streams.\n\n\n\n\n\nApplications of NER\n\n\n\n  \nInformation Retrieval\n: Enhances the accuracy of search engines and databases in finding relevant information based on named entities.\n\n  \nContent Classification\n: Helps in categorizing text data for better content management systems.\n\n  \nCustomer Support and CRM\n: Identifies key entities in customer communications, aiding in efficient and personalized responses.\n\n  \nBusiness Intelligence\n: Extracts useful information from business documents for market analysis, competitor analysis, etc.\n\n  \nHealthcare Data Analysis\n: In medical records, NER can identify and classify terms related to diseases, treatments, medications, etc., aiding in better patient care and research.\n\n\n\n\n\nChallenges and Considerations\n\n\n\n  \nAmbiguity in Entity Classification\n: Differentiating between entities with similar names or those that can fall into multiple categories.\n\n  \nAdaptation to Different Domains\n: Customizing NER systems to work effectively across various domains like legal, medical, or technical fields, each with its unique terminology.\n\n  \nDealing with Slang and Neologisms\n: Especially in social media texts, where new words and informal language are common.\n\n\n\n\n\nFuture Directions\n\n\n\n  \nIntegration with Deep Learning\n: Leveraging more advanced deep learning techniques to enhance the accuracy and adaptability of NER systems.\n\n  \nGreater Contextual Understanding\n: Improving the ability of NER systems to understand entities in a wider context, particularly in complex sentences or documents.\n\n  \nMultimodal NER\n: Incorporating other data types like audio and video for a more comprehensive entity recognition process.\n\n\n\n\n\nDependency Parsing\n\n\n\n  \nDependency Parsing is a critical task in NLP that involves analyzing the grammatical structure of a sentence. It identifies the dependencies between words, determining how each word relates to others in a sentence. This analysis is pivotal in understanding the meaning and context of sentences in natural language texts.\n\n  \nIn other words, Dependency Parsing is a foundational element in NLP, essential for understanding the syntactic structure of language. Its ability to dissect sentences and analyze the grammatical relationships between words is fundamental in various advanced NLP applications. As the field of NLP evolves, dependency parsing continues to adapt and advance, integrating new methodologies and technologies to meet the increasing complexity of human language processing.\n\n  \nHere’s a detailed look at Dependency Parsing, its methodologies, applications, challenges, and advancements.\n\n\n\n\n\nDefinition and Purpose\n\n\n\n  \nPrimary Objective\n: Dependency parsing aims to establish the grammatical structure of sentences by elucidating the relationships between ‘head’ words and words that modify or are dependent on these heads.\n\n  \nImportance in NLP\n: It plays a crucial role in understanding the syntactic structure of sentences, which is fundamental for various NLP tasks like machine translation, sentiment analysis, and information extraction.\n\n\n\n\n\nProcess and Techniques\n\n\n\n  \nParsing Structure\n: The process involves breaking down a sentence into its constituent parts and identifying the type of dependency relations among them, such as subject, object, modifier, etc.\n\n  \nDependency Trees\n: The outcome of dependency parsing is often represented as a tree structure, where nodes represent words, and edges represent the dependencies between them.\n\n  \nDependency Labels\n: Each dependency is labeled with the type of grammatical relation it represents, like ‘nsubj’ for nominal subject, ‘dobj’ for direct object, etc. The following slide, sourced from the \nStanford CS224n course\n, illustrate this:\n\n\n\n\n\n\n\n\nCommon Architectures and Models\n\n\n\n  \nTransition-Based Parsers\n: These parsers process the sentence in a linear fashion, typically from left to right, using a stack to hold words that are waiting to be processed.\n\n  \nGraph-Based Parsers\n: These consider all possible relationships between words in a sentence and select the highest scoring dependency tree based on a scoring function.\n\n  \nNeural Network Models\n: With advances in deep learning, neural network models like BiLSTM have been used to capture the context of the entire sentence, improving the accuracy of dependency parsing. A neural dependency parser, such as the one proposed by Chen and Manning in 2014, inputs parts of speech tags and dependency labels, yielding a structured representation of a sentence’s grammatical dependencies.\n\n  \nTransformer-Based Models\n: Models like BERT encode sentences and then use separate parsers to predict dependencies. They excel at capturing wider sentence context, enhancing parsing accuracy.\n\n\n\n\n\nSub-types of Dependency Parsing\n\n\n\n  \nProjective Parsing\n: Deals with dependencies that can be represented without crossing lines in a 2D plane. Suitable for languages with a more straightforward grammatical structure.\n\n  \nNon-Projective Parsing\n: Addresses complex dependencies, including those with crossing lines, often required in languages with freer word order.\n\n\n\n\n\nApplications of Dependency Parsing\n\n\n\n  \nMachine Translation\n: Helps in understanding the grammatical structure of the source language for accurate translation.\n\n  \nInformation Extraction\n: Essential for extracting structured information from unstructured text.\n\n  \nText Summarization\n: Enables identifying key grammatical structures to extract meaningful sentences for summaries.\n\n  \nSentiment Analysis\n: Helps in understanding the grammatical constructs to accurately determine sentiment.\n\n\n\n\n\nChallenges and Considerations\n\n\n\n  \nHandling Complex Sentences\n: Parsing sentences with intricate structures or ambiguous grammatical relationships can be challenging.\n\n  \nLanguage Variability\n: Different languages exhibit varied and complex grammatical patterns, which poses a challenge for creating universal parsing models.\n\n  \nComputational Efficiency\n: Balancing accuracy with computational efficiency, especially for real-time applications.\n\n\n\n\n\nFuture Directions\n\n\n\n  \nCross-Lingual Dependency Parsing\n: Developing models that can accurately parse sentences in multiple languages without language-specific training.\n\n  \nIntegration with Semantic Analysis\n: Combining syntactic parsing with semantic analysis for a more comprehensive understanding of text.\n\n  \nEnhanced Deep Learning Techniques\n: Leveraging advancements in deep learning to improve the accuracy and efficiency of dependency parsers.\n\n\n\n\n\nSentiment Analysis\n\n\n\n\n  \nSentiment Analysis, also known as opinion mining, is a fascinating and vital aspect of NLP that focuses on identifying and categorizing opinions expressed in text. It aims to discern the sentiment or emotional tone behind a series of words, providing insights into the attitudes, emotions, and opinions embedded within text data.\n\n  \nIn other words, Sentiment Analysis represents a dynamic and rapidly evolving field in NLP. By harnessing the power of sentiment analysis, businesses, researchers, and individuals can gain deeper insights into the vast and ever-growing world of text data, from social media posts and customer reviews to news articles and beyond. As technology continues to advance, sentiment analysis will undoubtedly become even more sophisticated, providing richer and more nuanced understandings of human emotions and opinions.\n\n  \nHere is a detailed exploration of Sentiment Analysis, covering its methodologies, applications, challenges, and advancements.\n\n\n\n\n\nDefinition and Purpose\n\n\n\n  \nCore Objective\n: Sentiment Analysis involves the computational study of opinions, sentiments, and emotions expressed in text, aiming to determine the attitude of a speaker or writer towards a particular topic, product, or the overall sentiment of a document.\n\n  \nImportance in NLP\n: It is crucial for understanding the subjective aspects of language, going beyond mere word recognition to comprehend the nuances of emotional expression.\n\n\n\n\n\nProcess and Techniques\n\n\n\n  \nLevels of Analysis\n:\n    \n\n      \nDocument Level\n: Determines the overall sentiment of an entire document.\n\n      \nSentence Level\n: Assesses the sentiment of individual sentences.\n\n      \nAspect Level\n: Focuses on specific aspects or attributes within the text, like analyzing sentiments about different features of a product.\n\n    \n\n  \n\n  \nSentiment Scoring\n: Often involves assigning a polarity score to the text, indicating positive, negative, or neutral sentiments.\n\n  \nContextual and Linguistic Nuances\n: Recognizing that the same word can have different sentiment implications based on context, and dealing with linguistic nuances like sarcasm and irony.\n\n\n\n\n\nCommon Architectures and Models\n\n\n\n  \nRule-Based Systems\n: Utilize a set of manually crafted rules and lexicons (lists of words and their sentiment scores) to assess sentiment.\n\n  \nMachine Learning Approaches\n:\n    \n\n      \nSupervised Learning\n: Uses labeled datasets to train models on recognizing sentiment-laden text.\n\n      \nUnsupervised Learning\n: Relies on algorithms to identify sentiment patterns without pre-labeled data.\n\n    \n\n  \n\n  \nDeep Learning Models\n:\n    \n\n      \nConvolutional Neural Networks (CNNs)\n: Efficient in extracting local and position-invariant features.\n\n      \nRecurrent Neural Networks (RNNs)\n and \nLSTM (Long Short-Term Memory)\n: Effective for capturing long-range dependencies in text.\n\n      \nTransformer-Based Models\n: Models like BERT, GPT, or RoBERTa offer superior performance in capturing complex contextual relationships.\n\n    \n\n  \n\n\n\n\n\nApplications of Sentiment Analysis\n\n\n\n  \nBrand Monitoring and Product Analysis\n: Companies use sentiment analysis to monitor brand reputation and understand consumer reactions to products and services.\n\n  \nMarket Research\n: Helps in analyzing public sentiment towards market trends, political events, or social issues.\n\n  \nCustomer Service\n: Automates the process of sorting customer feedback and complaints based on their sentiment.\n\n  \nSocial Media Monitoring\n: Tracks public sentiment on social media platforms about various topics, events, or products.\n\n\n\n\n\nChallenges and Considerations\n\n\n\n  \nSarcasm and Irony\n: Detecting and correctly interpreting sarcasm and irony remains a significant challenge.\n\n  \nContextual Variability\n: The sentiment value of words can change drastically depending on the context.\n\n  \nCross-Lingual Sentiment Analysis\n: Developing models that can accurately analyze sentiment in multiple languages.\n\n  \nReal-Time Analysis\n: Providing accurate sentiment analysis in real-time, especially for live data streams like social media.\n\n\n\n\n\nFuture Directions\n\n\n\n  \nEmotion Detection\n: Extending beyond polarity (positive/negative) to detect a range of emotions like joy, anger, or sadness.\n\n  \nMultimodal Sentiment Analysis\n: Incorporating audio, visual, and textual data to provide a more holistic sentiment analysis.\n\n  \nDeep Learning Advancements\n: Leveraging the advancements in deep learning for more accurate and context-aware sentiment analysis.\n\n  \nDomain-Specific Models\n: Tailoring sentiment analysis models for specific industries or sectors for more precise analysis.\n\n\n\n\n\nText Summarization\n\n\n\n\n  \nText Summarization in NLP refers to the process of creating a concise and coherent summary of a larger text while retaining the key information and overall meaning. It’s a challenging task, as it requires understanding the content, context, and structure of the text, and then producing a summary that is both accurate and informative.\n\n  \nPut simply, Text Summarization in NLP is a vital tool for digesting large volumes of text data, providing users with quick and efficient access to key information. As the field evolves, we can expect more advanced models capable of producing summaries that are not only accurate and concise but also nuanced and tailored to specific user needs.\n\n  \nHere’s a detailed exploration of Text Summarization.\n\n\n\n\n\nDefinition and Purpose\n\n\n\n  \nCore Objective\n: The primary goal of text summarization is to produce a condensed version of a text, which captures its essential information, making it easier for readers to grasp the main points quickly.\n\n  \nImportance in NLP\n: With the ever-growing amount of textual data, summarization aids in managing information overload by providing succinct versions of longer documents, enhancing accessibility and comprehension.\n\n\n\n\n\nTypes of Text Summarization\n\n\n\n  \nExtractive Summarization\n: This involves identifying and extracting key phrases and sentences from the original text to create a summary. Essentially, it involves creating a subset of the original content that represents the most important points.\n\n  \nAbstractive Summarization\n: This approach generates a new summary, potentially using words and phrases not present in the original text. It involves understanding the text and then rephrasing and rewriting to create a coherent summary, much like how a human would summarize a document.\n\n\n\n\n\nMethodologies and Models\n\n\n\n  \nRule-Based Systems\n: Early approaches to summarization were based on rule-based systems, which relied on manually crafted rules to identify key sentences or phrases.\n\n  \nMachine Learning Approaches\n:\n    \n\n      \nSupervised Learning\n: Uses labeled datasets to train models to identify important parts of the text for summarization.\n\n      \nUnsupervised Learning\n: Employs algorithms to find patterns in the text that signify importance without the need for labeled data.\n\n    \n\n  \n\n  \nDeep Learning Models\n:\n    \n\n      \nSequence-to-Sequence Models\n: Such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Units) networks, read the input text as a sequence and generate the summary as another sequence.\n\n      \nTransformer-Based Models\n: Models like T5 (Text-To-Text Transfer Transformer) or BART (Bidirectional and Auto-Regressive Transformers) have shown strong performance in abstractive summarization due to their capacity for understanding and generating complex text.\n\n    \n\n  \n\n\n\n\n\nApplications of Text Summarization\n\n\n\n  \nNews Aggregation\n: Summarizing news articles for quick and concise updates.\n\n  \nAcademic Research\n: Summarizing research papers or academic articles for faster literature review and analysis.\n\n  \nBusiness Intelligence\n: Generating summaries of business documents, reports, or emails for efficient information processing and decision-making.\n\n  \nLegal and Medical Document Summarization\n: Condensing legal cases or medical reports for quick reference.\n\n\n\n\n\nChallenges and Considerations\n\n\n\n  \nMaintaining Context and Coherence\n: Ensuring that the summary maintains the original context and flows coherently.\n\n  \nDealing with Redundancy\n: Avoiding redundant information in the summary, especially in extractive summarization.\n\n  \nBias in Source Text\n: Ensuring that the summarization process does not amplify any inherent biases present in the source text.\n\n  \nEvaluation of Summaries\n: Assessing the quality of summaries, as the task can be subjective and context-dependent.\n\n\n\n\n\nFuture Directions\n\n\n\n  \nCross-Language Summarization\n: Developing systems capable of summarizing text in one language and generating summaries in another.\n\n  \nPersonalized Summarization\n: Creating summaries tailored to the specific interests or requirements of the user.\n\n  \nIntegration with Other NLP Tasks\n: Combining summarization with tasks like sentiment analysis or question answering for more context-aware summarization.\n\n  \nImproving Abstraction Capabilities\n: Advancing the ability of models to generate more human-like, coherent, and contextually relevant abstractive summaries.\n\n\n\n\n\nQuestion Answering\n\n\n\n\n  \nQuestion Answering (QA) is a specialized domain within NLP focused on building systems that automatically answer questions posed by humans in natural language. It’s a complex task that combines understanding human language, context interpretation, and often retrieving information from various sources.\n\n  \nHere’s a detailed exploration of Question Answering in NLP.\n\n\n\n\n\nDefinition and Purpose\n\n\n\n  \nCore Objective\n: The primary goal of QA systems is to provide accurate, concise, and relevant answers to questions posed in natural language.\n\n  \nImportance in NLP\n: QA systems are at the forefront of making information accessible and understandable to users, bridging the gap between human queries and the vast amount of data available in text form.\n\n\n\n\n\nTypes of Question Answering Systems\n\n\n\n  \nFactoid QA\n: Answers simple, fact-based questions like “What is the capital of France?”. These require direct retrieval of facts from a database or text.\n\n  \nList QA\n: Involves questions expecting a list of items as answers, such as “List the novels written by Jane Austen.”\n\n  \nDefinition QA\n: Provides definitions or explanations for terms or concepts.\n\n  \nReasoning QA\n: Requires logical reasoning, inference, and understanding of context. These are more complex, for example, “Why does the Earth experience seasons?”\n\n  \nConversational QA\n: Involves answering questions in a conversational context, where each question might relate to previous ones in the conversation.\n\n\n\n\n\nMethodologies and Models\n\n\n\n  \nRetrieval-Based QA\n: Involves retrieving an answer from a structured database or a set of documents. This is more common in factoid QA.\n\n  \nGenerative QA\n: Generates answers based on understanding and processing the question, often used in more complex QA tasks.\n\n  \nNeural Network Models\n: Deep learning models, particularly those based on Transformer architecture like BERT, GPT, or T5, have significantly advanced the field of QA. These models are pre-trained on a large corpus of text and fine-tuned for specific QA tasks.\n\n  \nEnd-to-End Learning\n: Recent approaches involve training models that can handle the entire QA process in a single step, from understanding the question to providing the answer.\n\n\n\n\n\nApplications of Question Answering\n\n\n\n  \nCustomer Support\n: Automated systems provide quick responses to customer queries, improving efficiency and customer satisfaction.\n\n  \nEducational Tools\n: Assisting students in learning by providing instant answers to academic queries.\n\n  \nSearch Engines\n: Enhancing search engine capabilities by directly answering queries instead of just listing relevant documents.\n\n  \nHealthcare Assistance\n: Providing quick answers to common medical queries, aiding both patients and healthcare professionals.\n\n\n\n\n\nChallenges and Considerations\n\n\n\n  \nUnderstanding Context\n: QA systems must understand the context within which a question is asked, especially in conversational QA.\n\n  \nAmbiguity and Vagueness\n: Handling ambiguous or vague questions that might have multiple valid answers.\n\n  \nDomain-Specific Knowledge\n: Specialized domains like law or medicine require the system to have domain-specific knowledge for accurate answers.\n\n  \nLanguage Variety and Slang\n: Effectively interpreting questions phrased in different dialects, colloquial language, or slang.\n\n\n\n\n\nFuture Directions\n\n\n\n  \nImproved Contextual Understanding\n: Enhancing the ability of QA systems to understand and remember context over longer conversations.\n\n  \nCross-Lingual QA\n: Developing systems that can answer questions in multiple languages.\n\n  \nIntegration with Voice-Based Systems\n: Combining QA with speech recognition for voice-activated systems, like digital assistants.\n\n  \nPersonalized QA\n: Tailoring answers based on the user’s profile, previous queries, and preferences.\n\n\n\n\n\nFAQs\n\n\n\nWhat are the types of Question Answering systems?\n\n\n\n\n  \n\n    \nQuestion Answering (QA) systems come in various types, each designed to handle specific kinds of queries or data sources. Here’s a detailed look at five notable types: Closed-Book QA, Open-Book QA, Closed-Domain QA, Open-Domain QA, and Visual QA.\n\n\n    \n\n      \nClosed-Book QA\n:\n        \n\n          \nDescription\n: In Closed-Book QA, the system answers questions based on knowledge it has internalized during its training. It does not access any external information sources or databases while answering.\n\n          \nOperation\n: The system relies on what it has ‘learned’ and stored in its parameters through extensive training on a wide range of data.\n\n          \nApplications\n: Ideal for scenarios where quick, factual responses are needed, and the range of expected questions is within the scope of the model’s training.\n\n          \nLimitations\n: The accuracy and depth of answers are limited to the content and quality of its training data. It might not be current or comprehensive for all possible questions.\n\n        \n\n      \n\n      \nOpen-Book QA\n:\n        \n\n          \nDescription\n: Open-Book QA systems answer questions by referring to external data sources such as the internet, databases, or specific documents.\n\n          \nOperation\n: When posed with a question, these systems search for relevant information in external sources, process it, and then generate an answer.\n\n          \nApplications\n: Useful for questions requiring up-to-date information or topics that are too broad or current to be covered entirely in the training data.\n\n          \nLimitations\n: The effectiveness depends on the system’s ability to access, search, and understand relevant external information.\n\n        \n\n      \n\n      \nClosed-Domain QA\n:\n        \n\n          \nDescription\n: This type of system specializes in answering questions within a specific field or domain, such as medicine, law, or a particular set of literature.\n\n          \nOperation\n: It is trained with domain-specific data, enabling it to understand and process queries relevant to that domain deeply.\n\n          \nApplications\n: Particularly useful in professional or academic fields where expertise in a specific subject matter is required.\n\n          \nLimitations\n: Its scope is limited to the predefined domain, and it may not perform well on questions outside of that domain.\n\n        \n\n      \n\n      \nOpen-Domain QA\n:\n        \n\n          \nDescription\n: Open-Domain QA systems are designed to answer questions across a wide range of topics, not limited to any specific subject area.\n\n          \nOperation\n: These systems are typically trained on a diverse set of data from various fields and may use both closed-book and open-book methods to generate answers.\n\n          \nApplications\n: Ideal for general-purpose question answering where the queries can span a wide array of subjects.\n\n          \nLimitations\n: While versatile, they might not have the depth of knowledge in specific areas compared to closed-domain systems.\n\n        \n\n      \n\n      \nVisual QA\n:\n        \n\n          \nDescription\n: Visual QA involves answering questions based on visual content such as images or videos.\n\n          \nOperation\n: These systems analyze visual input, understand the context, and then answer questions related to that input.\n\n          \nApplications\n: Useful in scenarios where the query is about the content or context of a visual input, like identifying objects, actions, or explaining scenes in an image or video.\n\n          \nLimitations\n: The accuracy depends heavily on the system’s ability to interpret visual data correctly, which can be challenging due to the complexity and variability of visual content.\n\n        \n\n      \n\n    \n\n  \n\n  \n\n    \nEach of these QA systems has its unique strengths and is suited for different applications. The development and improvement of these systems are ongoing, driven by advances in machine learning, natural language processing, and computer vision.\n\n  \n\n\n\n\n\nWhat is the difference between open- and closed-book QA?\n\n\n\n\n  \nThe difference between open-book and closed-book question answering (QA) lies primarily in how information is accessed and utilized to answer questions.\n    \n\n      \nClosed-Book QA\n: In closed-book qQA, the system answers questions based solely on information it has been pre-trained on. It doesn’t have access to external sources or databases during the answering process. The knowledge is, in a sense, “memorized” or encoded in the model’s parameters through its training data. This method requires the model to have a large and comprehensive training dataset so that it can cover a wide range of topics. The model’s ability to answer questions is limited to what it has been trained on. Closed-book QA, unlike open-book QA, does not typically involve providing a specific context or passage with the question. The system relies on its pre-trained knowledge to answer questions, without external text references. In this model, the answers are generated based on the information encoded in the system’s parameters, without the need for external text for reference.\n\n      \nOpen-Book QA\n: Open-book question answering, on the other hand, involves the system actively retrieving information from external sources or databases to answer questions. This can include looking up information on the internet, accessing specific databases, or referring to external documents. In this approach, a specific context or passage is provided along with the question. The system uses this context this external information in real-time to formulate an answer. The context can be a paragraph, a document, or a set of documents from which the system retrieves information relevant to the question. This approach is particularly useful for complex questions where the answer depends heavily on understanding and analyzing a given text, allowing the system to access the most current and detailed information available, even if it wasn’t included in its original training data.\n\n    \n\n  \n\n  \nIn summary, closed-book QA relies on a model’s internal knowledge gained during training, while open-book QA involves external information retrieval to supplement the model’s knowledge base. Open-book QA tends to be more dynamic and up-to-date, whereas closed-book QA depends heavily on the breadth and quality of the training data.\n\n\n\n\n\nHow are open-book and open-domain QA different?\n\n\n\n\n  \n\n    \nOpen-book QA and open-domain QA are not the same, though they might sound similar. Each term refers to different aspects of question answering systems:\n\n\n    \n\n      \n\n        \nOpen-Book QA\n: Open-book question answering involves the use of external information sources to answer questions. In this approach, the system actively retrieves information from external documents, databases, or the internet to respond to queries. The focus is on the system’s ability to look up and synthesize information from outside its pre-trained knowledge base.\n\n      \n\n      \n\n        \nOpen-Domain QA\n: Open-domain question answering, on the other hand, refers to the system’s ability to answer questions across a wide range of topics or domains, without being restricted to a specific subject area. This can be achieved either through closed-book methods (where the model relies on its internal knowledge acquired during training) or open-book methods (where it looks up information externally). The key aspect of open-domain QA is its versatility and breadth in handling questions from various fields, be it science, history, popular culture, etc.\n\n      \n\n    \n\n  \n\n  \n\n    \nIn summary, open-book QA is about the source of the information (external resources), while open-domain QA is about the scope of the topics (broad and unrestricted). A question answering system can be both open-book and open-domain if it uses external resources to answer questions on a wide range of topics.\n\n  \n\n\n\n\n\nText Classification\n\n\n\n\n  \nText Classification, also known as text categorization, is a fundamental task in NLP that involves assigning predefined categories or labels to text. It’s widely used to organize, structure, and make sense of large volumes of unstructured textual data. This process enables efficient handling and analysis of text for various applications.\n\n  \nPut simply, Text Classification is a critical aspect of NLP, playing a pivotal role in structuring and understanding textual data. As the field advances, text classification methods are becoming more sophisticated, moving towards more accurate, context-aware, and language-agnostic systems. These advancements are making it increasingly possible to harness the full potential of textual data across various domains.\n\n  \nHere’s a detailed exploration of Text Classification.\n\n\n\n\n\nDefinition and Purpose\n\n\n\n  \nCore Objective\n: Text Classification aims to automatically classify or categorize text into one or more predefined categories or classes based on its content.\n\n  \nImportance in NLP\n: It’s essential for organizing and structuring large datasets of text, making it easier to process and analyze information efficiently.\n\n\n\n\n\nTypes of Text Classification\n\n\n\n  \nBinary Classification\n: Involves categorizing text into two categories (e.g., spam or not spam).\n\n  \nMulti-Class Classification\n: Classifies text into one of multiple categories (e.g., classifying news articles into categories like sports, politics, entertainment).\n\n  \nMulti-Label Classification\n: Each text can belong to multiple categories simultaneously (e.g., a news article that is categorized as both technology and business).\n\n\n\n\n\nMethodologies and Models\n\n\n\n  \nRule-Based Systems\n: Early text classification systems used manually crafted rules based on keywords or phrases.\n\n  \nMachine Learning Approaches\n:\n    \n\n      \nSupervised Learning\n: Uses labeled datasets to train models to classify texts. Common algorithms include Naive Bayes, Support Vector Machines (SVM), and Decision Trees.\n\n      \nUnsupervised Learning\n: Identifies patterns or clusters in the data without using pre-labeled examples.\n\n    \n\n  \n\n  \nDeep Learning Models\n:\n    \n\n      \nConvolutional Neural Networks (CNNs)\n: Effective for capturing local and position-invariant features in text.\n\n      \nRecurrent Neural Networks (RNNs)\n and their variants like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Units): Suitable for capturing the sequential nature of text.\n\n      \nTransformer-Based Models\n: Such as BERT (Bidirectional Encoder Representations from Transformers) and XLNet, these models have revolutionized text classification with their ability to capture the context of each word in a sentence.\n\n    \n\n  \n\n\n\n\n\nApplications of Text Classification\n\n\n\n  \nSpam Detection\n: Identifying and filtering out spam emails.\n\n  \nSentiment Analysis\n: Categorizing text by sentiment, such as positive, negative, or neutral.\n\n  \nTopic Labeling\n: Automatically labeling topics of articles or posts.\n\n  \nLanguage Detection\n: Classifying text by the language it’s written in.\n\n\n\n\n\nChallenges and Considerations\n\n\n\n  \nImbalanced Data\n: Often, datasets are imbalanced, with some classes having significantly more examples than others, leading to biased models.\n\n  \nContextual Ambiguity\n: Words or phrases can have different meanings in different contexts, posing a challenge for accurate classification.\n\n  \nHandling Slang and Abbreviations\n: Particularly in social media text, where unconventional language is common.\n\n  \nMultilingual and Cross-Lingual Classification\n: Classifying text written in different languages or developing models that can classify text across languages.\n\n\n\n\n\nFuture Directions\n\n\n\n  \nTransfer Learning and Pre-Trained Models\n: Leveraging models trained on large datasets to improve performance on specific classification tasks, even with smaller datasets.\n\n  \nFine-Tuning and Domain Adaptation\n: Adapting pre-trained models to specific domains or topics for more accurate classification.\n\n  \nCross-Lingual Learning\n: Building models that can understand and classify text in multiple languages.\n\n  \nIntegrating Contextual Information\n: Incorporating additional contextual information for more nuanced classification, such as the author’s profile or related metadata.\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2021Distilled,\n  title   = {NLP Tasks},\n  author  = {Jain, Vinija and Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2021},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/nlp-tasks/"},
{"page_content": "aman.ai\n • \nfulfillment policy\n\n        \n        \nLast updated August 4, 2024\n        \n\n        \nRefund Policy\n\n        \nWe are committed to providing the highest quality of service and ensuring customer satisfaction. Please review our refund policy to understand under what conditions you can receive a refund:\n\n        \nEligibility for Refunds:\n\n        \n\n            \nRefunds are available for services purchased within 14 days, provided that the services have not been rendered.\n\n        \n\n        \nProcess for Requesting a Refund:\n\n        \n\n            \nTo request a refund, please contact our customer support team via email at \nsupport [at] aman.ai\n with your order details and the reason for the refund request.\n\n            \nOur team will review your request and respond within 5 business days.\n\n        \n\n        \nNon-Refundable Services:\n\n        \n\n            \nServices that have been rendered and delivered.\n\n            \nCustom or personalized services where work has already commenced.\n\n        \n\n\n        \nDelivery Policy\n\n        \nWe ensure prompt delivery of our electronic services. Here’s what you need to know about our delivery process:\n\n        \nElectronic Delivery:\n\n        \n\n            \nAll services are delivered electronically, unless explicitly stated otherwise.\n\n        \n\n        \nDelivery Timeline:\n\n        \n\n                \nIf a timeline is agreed upon in an agreement, services are rendered accordingly. Otherwise, services are typically delivered within 24 hours from the time of order confirmation, unless explicitly stated otherwise.\n\n                \nIn rare instances where there might be a delay, you will be notified immediately, and we will provide an updated delivery time.\n\n        \n\n\n        \nReturn Policy\n\n        \nGiven the nature of our services, returns are generally not applicable. However, we are committed to resolving any issues you may encounter:\n\n        \nService Issues:\n\n        \n\n            \nIf you experience any problems with the service delivered, please contact our customer support team immediately.\n\n            \nWe will work diligently to resolve any issues and ensure you receive the service as promised.\n\n        \n\n\n        \nCancellation Policy\n\n        \nWe understand that sometimes plans change, and you may need to cancel a service. Here are our cancellation guidelines:\n\n        \nCancellation of Services:\n\n        \n\n            \nYou can cancel any service within 24 hours of placing the order for a full refund, provided that the services have not been rendered.\n\n            \nCancellations requested after 24 hours but before the service is rendered will be eligible for a partial refund determinable based on a review of the work already completed.\n\n        \n\n        \nCancellation Process:\n\n        \n\n            \nTo cancel a service, please contact our customer support team at \nsupport [at] aman.ai\n with your order details and reason for cancellation.\n\n            \nOur team will process your cancellation request and provide confirmation within 2 business days.\n\n        \n\n        \nSubscription Cancellations:\n\n        \n\n            \nIf you have subscribed to a recurring service, you can cancel the subscription at any time.\n\n            \nCancellations of subscriptions will take effect at the end of the current billing cycle, and no further charges will be made.\n\n        \n\n\n        \nBy providing clear and fair policies, we aim to build trust and ensure a positive experience for all our customers. If you have any questions or need further clarification, please do not hesitate to contact our customer support team.", "source": "https://aman.ai/fulfillment-policy.htm"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nNatural Language Processing • Word Vectors/Embeddings\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nMotivation\n\n  \nWord Embeddings\n\n  \nConceptual Framework of Word Embeddings\n\n  \nRelated: WordNet\n\n  \nBackground: Synonymy and Polysemy (Multi-Sense)\n    \n\n      \nSynonymy\n        \n\n          \nCharacteristics of Synonymy\n\n        \n\n      \n\n      \nPolysemy (Multi-Sense)\n        \n\n          \nCharacteristics of Polysemy\n\n        \n\n      \n\n      \nKey Differences Between Synonymy and Polysemy\n\n      \nWhy Are Synonymy and Polysemy Important?\n\n      \nChallenges\n\n    \n\n  \n\n  \nWord Embedding Techniques\n    \n\n      \nBag of Words (BoW)\n        \n\n          \nConcept\n\n          \nSteps to Create BoW Embeddings\n\n          \nExample\n\n          \nLimitations of BoW\n            \n\n              \nLack of Contextual Information\n\n              \nHigh Dimensionality\n\n              \nLack of Handling of Polysemy and Synonymy\n\n              \nFixed Vocabulary\n\n              \nFeature Independence Assumption\n\n              \nScalability Issues\n\n              \nNo Weighting Mechanism\n\n              \nLack of Generalization\n\n              \nExamples of Limitations\n\n              \nSummary\n\n            \n\n          \n\n        \n\n      \n\n      \nTerm Frequency-Inverse Document Frequency (TF-IDF)\n        \n\n          \nTerm Frequency (TF)\n\n          \nInverse Document Frequency (IDF)\n\n          \nExample\n            \n\n              \nSteps to Calculate TF-IDF\n\n              \nDocument Collection\n\n              \nCalculate Term Frequency (TF)\n\n              \nCalculate Document Frequency (DF) and Inverse Document Frequency (IDF)\n\n              \nCalculate TF-IDF for Each Word\n\n              \nExplanation of Table\n\n              \nKey Observations\n\n            \n\n          \n\n          \nLimitations of TF-IDF\n            \n\n              \nLack of Context and Word Order\n\n              \nDoes Not Account for Polysemy\n\n              \nLack of Semantic Understanding\n\n              \nBias Towards Rare Terms\n\n              \nVocabulary Limitation\n\n              \nNormalization Issues\n\n              \nRequires a Large and Representative Corpus\n\n              \nNo Distinction Between Different Types of Documents\n\n              \nPoor Performance with Short Texts\n\n            \n\n          \n\n        \n\n      \n\n      \nBest Match 25 (BM25)\n        \n\n          \nKey Components of BM25\n\n          \nExample\n\n          \nBM25: Evolution of TF-IDF\n            \n\n              \nBM25\n\n              \nTF-IDF\n\n              \nExample\n\n            \n\n          \n\n          \nLimitations of BM25\n            \n\n              \nParameter Sensitivity\n\n              \nNon-Handling of Semantic Similarities\n\n              \nIneffectiveness with Short Queries or Documents\n\n              \nLength Normalization Challenges\n\n              \nQuery Term Independence\n\n              \nDifficulty with Rare Terms\n\n              \nPerformance in Specialized Domains\n\n              \nIgnoring Document Quality\n\n              \nVulnerability to Keyword Stuffing\n\n              \nIncompatibility with Complex Queries\n\n            \n\n          \n\n        \n\n      \n\n      \nWord2Vec\n        \n\n          \nMotivation behind Word2Vec: The Need for Context-based Semantic Understanding\n\n          \nCore Idea\n\n          \nWord2Vec Architectures\n\n          \nTraining and Optimization\n\n          \nEmbedding and Semantic Relationships\n\n          \nDistinction from Traditional Models\n\n          \nSemantic Nature of Word2Vec Embeddings\n\n          \nKey Limitations and Advances in Word2Vec and Word Embeddings\n\n          \nFurther Learning Resources\n\n        \n\n      \n\n      \nGlobal Vectors for Word Representation (GloVe)\n        \n\n          \nOverview\n\n          \nHow GloVe Works\n\n          \nExample\n\n          \nSignificance of GloVe\n\n          \nLimitations of GloVe\n            \n\n              \nLack of Context-Sensitivity\n\n              \nInefficient for Rare Words\n\n              \nCorpus Dependence\n\n              \nComputational Cost\n\n              \nLimited to Word-Level Representation\n\n              \nInability to Handle OOV (Out-of-Vocabulary) Words\n\n            \n\n          \n\n        \n\n      \n\n      \nfastText\n        \n\n          \nOverview\n\n          \nCore Features of fastText\n\n          \nExample\n\n          \nLimitations of fastText\n            \n\n              \nLimited Contextual Awareness\n\n              \nSensitivity to Subword Granularity\n\n              \nInability to Model Long-Distance Dependencies\n\n              \nScalability and Resource Requirements\n\n              \nLack of Language-Specific Optimizations\n\n              \nLimited Performance in Highly Context-Dependent Tasks\n\n            \n\n          \n\n        \n\n      \n\n      \nBERT Embeddings\n\n      \nHandling Polysemous Words – Key Limitation of BoW, TF-IDF, BM25, Word2Vec, GloVe, and fastText\n        \n\n          \nBag of Words (BoW)\n\n          \nTF-IDF (Term Frequency-Inverse Document Frequency)\n\n          \nBM25\n\n          \nWord2Vec\n\n          \nGloVe\n\n          \nfastText\n\n          \nBERT\n\n        \n\n      \n\n    \n\n  \n\n  \nExample: BoW, TF-IDF, BM25, Word2Vec, GloVe, fastText, and BERT Embeddings\n    \n\n      \nBag of Words (BoW) Representation for “Cat”\n\n      \nTF-IDF Embedding for “Cat”\n\n      \nBM25 Embedding for “Cat”\n\n      \nWord2Vec Embedding for “Cat”\n\n      \nGloVe Embedding for “Cat”\n\n      \nfastText Embedding for “Cat”\n\n      \nBERT Embedding for “Cat”\n\n    \n\n  \n\n  \nSummary: Types of Embeddings\n    \n\n      \nBag-of-Words-based Embeddings\n\n      \nPredictive Word Embeddings\n\n      \nContextual and Sequential Data Embeddings\n\n      \nContextual Embeddings\n\n      \nSentence/Document Embeddings\n\n      \nPositional Embeddings\n\n      \nRelative Embeddings\n\n    \n\n  \n\n  \nComparative Analysis of BoW, TF-IDF, BM25, Word2Vec, GloVe, fastText, and BERT Embeddings\n    \n\n      \nBag of Words (BoW)\n\n      \nTerm Frequency-Inverse Document Frequency (TF-IDF)\n\n      \nBM25 (Best Matching 25)\n\n      \nWord2Vec\n\n      \nGloVe (Global Vectors for Word Representation)\n\n      \nfastText\n\n      \nBERT (Bidirectional Encoder Representations from Transformers)\n\n      \nComparative Summary\n        \n\n          \nCount-Based Techniques (TF-IDF and BM25)\n            \n\n              \nPros\n\n              \nCons\n\n            \n\n          \n\n          \nCo-occurrence Based/Static Embedding Techniques (Word2Vec, GloVe, fastText)\n            \n\n              \nPros\n\n              \nCons\n\n            \n\n          \n\n          \nContextualized Representation Techniques (BERT, ELMo)\n            \n\n              \nPros\n\n              \nCons\n\n            \n\n          \n\n        \n\n      \n\n      \nKey Takeaways\n\n    \n\n  \n\n  \nFAQs\n    \n\n      \nWhat does the “Continuous” in Word2Vec’s Continuous Bag of Words and Continuous Skipgram refer to?\n        \n\n          \nTraditional Bag of Words\n\n          \nContinuous Bag of Words\n\n          \nContinuous Skipgram\n\n          \nKey Advantages of CBOW and Skipgram\n\n          \nSummary\n\n        \n\n      \n\n      \nHow are Word2Vec, GloVe, and fastText Co-occurrence-based Embedding Techniques?\n        \n\n          \nWord2Vec\n\n          \nGloVe\n\n          \nFastText\n\n          \nSummary of Co-occurrence Based Techniques\n\n        \n\n      \n\n      \nDoes Word2Vec use word-level or sub-word-level tokenization?\n        \n\n          \nKey Points\n\n          \nSub-word-level Tokenization\n\n        \n\n      \n\n    \n\n  \n\n  \nRelated: Matryoshka Representation Learning\n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \nWord embeddings are a fascinating aspect of modern computational linguistics, particularly in the domain of Natural Language Processing (NLP). These embeddings serve as the foundation for interpreting and processing human language in a format that computers can understand and utilize. Here, we delve into an overview of word embeddings, focusing on their conceptual framework and practical applications.\n\n\n\n\n\nMotivation\n\n\n\n\n  \nJ.R. Firth’s Insight and Distributional Semantics\n: The principle of distributional semantics is encapsulated in J.R. Firth’s famous quote (below), which highlights the significance of contextual information in determining word meaning and captures the importance of contextual information in defining word meanings. This principle is a cornerstone in the development of word embeddings.\n\n\n\n\n\n\n  \n“You shall know a word by the company it keeps.”\n\n\n\n\n\n\n  \nRole in AI and NLP\n: Situated at the heart of AI, NLP aims to bridge the gap between human language and machine understanding. The primary motivation for developing word embeddings within NLP is to create a system where computers can not only recognize but also understand and interpret the subtleties and complexities of human language, thus enabling more natural and effective human-computer interactions.\n\n  \nAdvancements in NLP\n: The evolution of NLP, especially with the integration of deep learning methods, has led to significant enhancements in various language-related tasks, underscoring the importance of continuous innovation in this field.\n\n  \nHistorical Context and Evolution\n: With over 50 years of development, originating from linguistics, NLP has grown to embrace sophisticated models that generate word embeddings. The motivation for this evolution stems from the desire to more accurately capture and represent the nuances and complexities of human language in digital form.\n\n  \nWord embeddings as a lens for nuanced language interpretation\n: Word embeddings, underpinned by the concept of distributional semantics, represent word meanings through vectors of real numbers. While not perfect, this method provides a remarkably effective means of interpreting and processing language in computational systems. The ongoing developments in this field continue to enhance our ability to model and understand natural language in a digital context.\n\n\n\n\n\nWord Embeddings\n\n\n\n\n  \nWord embeddings, also known as word vectors, provide a dense, continuous, and compact representation of words, encapsulating their semantic and syntactic attributes. They are essentially real-valued vectors, and the proximity of these vectors in a multidimensional space is indicative of the linguistic relationships between words.\n\n\n\n\n\n\n  \nAn embedding is a point in an \\(N\\)-dimensional space, where \\(N\\) represents the number of dimensions of the embedding.\n\n\n\n\n\n\n  \n\n    \nThis concept is rooted in the Distributional Hypothesis, which posits that words appearing in similar contexts are likely to bear similar meanings. Consequently, in a high-dimensional vector space, vectors representing semantically related words (e.g., ‘apple’ and ‘orange’, both fruits) are positioned closer to each other compared to those representing semantically distant words (e.g., ‘apple’ and ‘dog’).\n\n  \n\n  \n\n    \nWord embeddings are constructed by forming dense vectors for each word, chosen in such a way that they resemble vectors of contextually similar words. This process effectively embeds words in a high-dimensional vector space, with each dimension contributing to the representation of a word’s meaning. For example, the concept of ‘banking’ is distributed across all dimensions of its vector, with its entire semantic essence embedded within this multidimensional space.\n\n  \n\n\n\n\n\n\n\n\n\n  \nThe term ‘embedding’ in this context refers to the transformation of discrete words into continuous vectors, achieved through word embedding algorithms. These algorithms are designed to convert words into vectors that encapsulate a significant portion of their semantic content. A classic example of the effectiveness of these embeddings is the vector arithmetic that yields meaningful analogies, such as \n'king' - 'man' + 'woman' ≈ 'queen'\n. The figure below (\nsource\n) shows distributional vectors represented by a \\(\\mathbf{D}\\)-dimensional vector where \\(\\mathbf{D}<<\\mathbf{V}\\), where \\(\\mathbf{V}\\) is size of the vocabulary.\n\n\n\n\n\n\n\n\n\n  \n\n    \nThe term ‘embedding’ in this context refers to the transformation of discrete words into continuous vectors, achieved through word embedding algorithms. These algorithms are designed to convert words into vectors that encapsulate a significant portion of their semantic content. A classic example of the effectiveness of these embeddings is the vector arithmetic that yields meaningful analogies, such as \n'king' - 'man' + 'woman' ≈ 'queen'\n.\n\n  \n\n  \n\n    \nWord embeddings are typically pre-trained on large, unlabeled text corpora. This training often involves optimizing auxiliary objectives, like predicting a word based on its contextual neighbors, as demonstrated in \nWord2Vec\n by \nMikolov et al. (2013)\n. Through this process, the resultant word vectors encapsulate both syntactic and semantic properties of words.\n\n  \n\n  \n\n    \nThe effectiveness of word embeddings lies in their ability to capture similarities between words, making them invaluable in NLP tasks. This is typically done by using similarity measures such as cosine similarity to quantify how close or distant the meanings of different words are in a vector space.\n\n  \n\n  \n\n    \nOver the years, the creation of word embeddings has generally relied on shallow neural networks rather than deep ones. However, these embeddings have become a fundamental layer in deep learning-based NLP models. This use of embeddings is a key difference between traditional word count models and modern deep learning approaches, contributing to state-of-the-art performance across a variety of NLP tasks (Bengio and Usunier, 2011; Socher et al., 2011; Turney and Pantel, 2010; Cambria et al., 2017).\n\n  \n\n  \n\n    \nIn summary, word embeddings not only efficiently encapsulate the semantic and syntactic nuances of language but also play a pivotal role in enhancing the computational efficiency of numerous natural language processing tasks.\n\n  \n\n\n\n\n\nConceptual Framework of Word Embeddings\n\n\n\n\n  \nContinuous Knowledge Representation\n:\n    \n\n      \nInformation can be categorized as either existing in continuous streams or in discrete chunks. Large Language Models (LLMs), such as \nBERT\n, \nGPT\n, and others, exemplify continuous knowledge representation. This approach contrasts with traditional methods that often handle data in discrete units.\n\n    \n\n  \n\n  \nNature of LLM Embeddings\n:\n    \n\n      \nLLM embeddings are essentially dense, continuous, real-valued vectors situated within a high-dimensional space. For instance, in the case of \nBERT\n, these vectors are 768-dimensional. This concept can be analogized to geographical coordinates on a map. Just as longitude and latitude offer specific locational references on a two-dimensional plane, embeddings provide approximations of positions within a multi-dimensional semantic space. This space is constructed from the interconnections among words across vast internet resources.\n\n    \n\n  \n\n  \nCharacteristics of Embedding Vectors\n:\n    \n\n      \nSince these vectors are continuous, they permit an infinite range of values within specified intervals. This continuity results in a certain ‘fuzziness’ in the embeddings’ coordinates, allowing for nuanced and context-sensitive interpretation of word meanings.\n\n    \n\n  \n\n  \nExample of LLM Embedding Functionality\n:\n    \n\n      \nConsider the LLM embedding for a phrase like ‘Jennifer Aniston’. This embedding would be a multi-dimensional vector leading to a specific location in a vast ‘word-space’, comprising several billion parameters. Adding another concept, such as ‘TV series’, to this vector could shift its position towards the vector representing ‘Friends’, illustrating the dynamic and context-aware nature of these embeddings. However, this sophisticated mechanism is not without its challenges, as it can sometimes lead to unpredictable or ‘hallucinatory’ outputs.\n\n    \n\n  \n\n\n\n\n\nRelated: WordNet\n\n\n\n\n  \nOne of the initial attempts to digitally encapsulate a word’s meaning was through the development of \nWordNet\n. WordNet functioned as an extensive thesaurus, encompassing a compilation of synonym sets and hypernyms, the latter representing a type of hierarchical relationship among words.\n\n  \nDespite its innovative approach, WordNet encountered several limitations:\n    \n\n      \nInefficacy in capturing the full scope of word meanings.\n\n      \nInadequacy in reflecting the subtle nuances associated with words.\n\n      \nAn inability to incorporate evolving meanings of words over time.\n\n      \nChallenges in maintaining its currency and relevance in an ever-changing linguistic landscape.\n\n    \n\n  \n\n  \nMoreover, WordNet employed the principles of distributional semantics, which posits that a word’s meaning is largely determined by the words that frequently appear in close proximity to it.\n\n  \nSubsequently, the field of NLP witnessed a paradigm shift with the advent of word embeddings. These embeddings marked a significant departure from the constraints of traditional lexical databases like WordNet. Unlike its predecessors, word embeddings provided a more dynamic and contextually sensitive approach to understanding language. By representing words as vectors in a continuous vector space, these embeddings could capture a broader array of linguistic relationships, including semantic similarity and syntactic patterns.\n\n  \nToday, word embeddings continue to be a cornerstone technology in NLP, powering a wide array of applications and tasks. Their ability to efficiently encode word meanings into a dense vector space has not only enhanced the performance of various NLP tasks but also has laid the groundwork for more advanced language processing and understanding technologies.\n\n\n\n\n\nBackground: Synonymy and Polysemy (Multi-Sense)\n\n\n\n\n  \nSynonymy deals with words that share similar meanings, while polysemy refers to a single word that carries multiple related meanings. Both phenomena play critical roles in language structure and use, contributing to its richness and adaptability.\n\n\n\n\n\nSynonymy\n\n\n\n\n  \nSynonymy refers to the linguistic phenomenon where two or more words have the same or very similar meanings. Synonyms are words that can often be used interchangeably in many contexts, although subtle nuances, connotations, or stylistic preferences might make one more appropriate than another in specific situations.\n\n\n\n\n\nCharacteristics of Synonymy\n\n\n\n\n  \nComplete Synonymy\n: This is when two words mean exactly the same thing in all contexts, with no differences in usage or connotation. However, true cases of complete synonymy are extremely rare.\n    \n\n      \nExample: \ncar\n and \nautomobile\n.\n\n    \n\n  \n\n  \nPartial Synonymy\n: In most cases, synonyms share similar meanings but might differ slightly in terms of usage, formality, or context.\n    \n\n      \nExample: \nbig\n and \nlarge\n are generally synonymous but might be preferred in different contexts (e.g., “big mistake” vs. “large building”).\n\n    \n\n  \n\n  \nDifferent Nuances\n: Even if two words are synonyms, one might carry different emotional or stylistic undertones.\n    \n\n      \nExample: \nchildish\n vs. \nchildlike\n. Both relate to behaving like a child, but \nchildish\n often has a negative connotation, while \nchildlike\n tends to be more positive.\n\n    \n\n  \n\n  \nDialects and Variations\n: Synonyms can vary between regions or dialects.\n    \n\n      \nExample: \nelevator\n (American English) and \nlift\n (British English).\n\n    \n\n  \n\n\n\n\n\n\n  \nSynonymy is a vital aspect of language as it provides speakers with a choice of words, adding richness, variety, and flexibility to expression.\n\n\n\n\n\nPolysemy (Multi-Sense)\n\n\n\nPolysemy\n occurs when a single word or expression has multiple meanings or senses that are related by extension. Unlike homonyms, where words have the same spelling or pronunciation but unrelated meanings (like \nbat\n – the animal, and \nbat\n – the sporting equipment), polysemous words have senses that are conceptually or historically linked.\n\n\n\nCharacteristics of Polysemy\n\n\n\n\n  \nMultiple Related Meanings\n: A polysemous word can have different meanings that share a common origin or conceptual link.\n    \n\n      \nExample: The word \nbank\n can refer to:\n        \n\n          \na financial institution (e.g., “I deposited money in the bank”),\n\n          \nthe side of a river (e.g., “We had a picnic on the river bank”).\nThese meanings, though different, share a root concept of accumulation or collection (of money or land).\n\n        \n\n      \n\n    \n\n  \n\n  \nSemantic Extension\n: Often, the different meanings of a polysemous word arise from metaphorical or functional extensions of its original sense.\n    \n\n      \nExample: \nHead\n:\n        \n\n          \nA physical part of the body (literal meaning),\n\n          \nThe leader of an organization (metaphorical extension, as the head is seen as the top or control center of the body),\n\n          \nThe top or front of something (e.g., “the head of the line”).\n\n        \n\n      \n\n    \n\n  \n\n  \nContext-Dependent Interpretation\n: The correct meaning of a polysemous word is usually determined by its context.\n    \n\n      \nExample: The word \nrun\n can mean:\n        \n\n          \nMoving quickly on foot (“She runs every morning”),\n\n          \nOperating a machine (“The car runs smoothly”),\n\n          \nManaging something (“He runs the business”).\n\n        \n\n      \n\n    \n\n  \n\n  \nCognitive Efficiency\n: Polysemy allows for efficient use of language by reusing existing words in new, related ways rather than inventing entirely new terms for each concept.\n\n\n\n\n\nKey Differences Between Synonymy and Polysemy\n\n\n\n\n  \nSynonymy involves different words that have similar or identical meanings.\n    \n\n      \nExample: \nhappy\n and \njoyful\n.\n\n    \n\n  \n\n  \nPolysemy involves one word that has multiple related meanings.\n    \n\n      \nExample: \nbright\n (meaning both intelligent and full of light).\n\n    \n\n  \n\n\n\n\n\nWhy Are Synonymy and Polysemy Important?\n\n\n\n\n  \nSynonymy enriches the language by giving speakers choices in expression, allowing for stylistic variety, precision, and emotional nuance.\n\n  \nPolysemy reflects the natural evolution and flexibility of language. Words develop multiple meanings over time, often through metaphorical or cultural associations, making language more adaptable to new contexts.\n\n\n\n\n\nChallenges\n\n\n\n\n  \nAmbiguity\n: Both synonymy and polysemy can create ambiguity in communication.\n    \n\n      \nFor example, in polysemy, a sentence like “She banked by the river” could cause confusion without proper context (financial transaction or sitting by the river bank?).\n\n    \n\n  \n\n  \nDisambiguation in Language Processing\n: In fields like natural language processing (NLP) and linguistics, distinguishing between different senses of polysemous words or selecting the correct synonym for a given context is a key challenge.\n\n\n\n\n\nWord Embedding Techniques\n\n\n\n\n  \nAccurately representing the meaning of words is a crucial aspect of NLP. This task has evolved significantly over time, with various techniques being developed to capture the nuances of word semantics.\n\n  \n\n    \nCount-based methods like TF-IDF and BM25 focus on word frequency and document uniqueness, offering basic information retrieval capabilities. Co-occurrence based techniques such as Word2Vec, GloVe, and fastText analyze word contexts in large corpora, capturing semantic relationships and morphological details. Contextualized models like BERT and ELMo provide dynamic, context-sensitive embeddings, significantly enhancing language understanding by generating varied representations for words based on their usage in sentences. Details of the aforementioned taxonomy are as follows:\n\n\n    \n\n      \n\n        \nCount-Based Techniques (TF-IDF and BM25)\n: With their roots in the field of information retrieval, these methods focus on the frequency of words in documents. TF-IDF emphasizes words that are unique to a document in a corpus, while BM25 refines this approach with probabilistic modeling, considering document length and term saturation. They are foundational in information retrieval but lack semantic richness.\n\n      \n\n      \n\n        \nCo-occurrence Based/Static Embedding Techniques (Word2Vec, GloVe, fastText)\n: These techniques generate embeddings by analyzing how words co-occur in large text corpora. Word2Vec and GloVe create word vectors that capture semantic relationships, while fastText extends this by considering subword information, enhancing understanding of morphological structures.\n\n      \n\n      \n\n        \nContextualized/Dynamic Representation Techniques (BERT, ELMo)\n: BERT and ELMo represent advanced embedding techniques, providing context-sensitive word representations. Unlike static embeddings, they generate different vectors for a word based on its surrounding context, leading to a deeper understanding of language nuances and ambiguities. These models have significantly improved performance in a wide range of NLP tasks.\n\n      \n\n    \n\n  \n\n\n\n\n\nBag of Words (BoW)\n\n\n\nConcept\n\n\n\n\n  \nBag of Words (BoW) is a simple and widely used technique for text representation in natural language processing (NLP). It represents text data (documents) as vectors of word counts, disregarding grammar and word order but keeping multiplicity. Each unique word in the corpus is a feature, and the value of each feature is the count of occurrences of the word in the document.\n\n\n\n\n\nSteps to Create BoW Embeddings\n\n\n\n\n  \nTokenization:\n\n    \n\n      \nSplit the text into words (tokens).\n\n    \n\n  \n\n  \nVocabulary Building:\n\n    \n\n      \nCreate a vocabulary list of all unique words in the corpus.\n\n    \n\n  \n\n  \nVector Representation:\n\n    \n\n      \nFor each document, create a vector where each element corresponds to a word in the vocabulary. The value is the count of occurrences of that word in the document.\n\n    \n\n  \n\n\n\n\n\nExample\n\n\n\n\n  \nConsider a corpus with the following two documents:\n    \n\n      \n“The cat sat on the mat.”\n\n      \n“The dog sat on the log.”\n\n    \n\n  \n\n  \n\n    \nSteps:\n\n\n    \n\n      \nTokenization:\n\n        \n\n          \nDocument 1: \n[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n\n          \nDocument 2: \n[\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"]\n\n        \n\n      \n\n      \nVocabulary Building:\n\n        \n\n          \nVocabulary: \n[\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"log\"]\n\n        \n\n      \n\n      \nVector Representation:\n\n        \n\n          \nDocument 1: \n[2, 1, 1, 1, 1, 0, 0]\n\n          \nDocument 2: \n[2, 0, 1, 1, 0, 1, 1]\n\n        \n\n      \n\n    \n\n\n    \n\n      \nThe resulting BoW vectors are:\n        \n\n          \nDocument 1: \n[2, 1, 1, 1, 1, 0, 0]\n\n          \nDocument 2: \n[2, 0, 1, 1, 0, 1, 1]\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nLimitations of BoW\n\n\n\n\n  \nBag of Words (BoW) embeddings, despite their simplicity and effectiveness in some applications, have several significant limitations. These limitations can impact the performance and applicability of BoW in more complex natural language processing (NLP) tasks. Here’s a detailed explanation of these limitations:\n\n\n\n\n\nLack of Contextual Information\n\n\n\n\n  \nWord Order Ignored:\n\n    \n\n      \nBoW embeddings do not take into account the order of words in a document. This means that “cat sat on the mat” and “mat sat on the cat” will have the same BoW representation, despite having different meanings.\n\n    \n\n  \n\n  \nLoss of Syntax and Semantics:\n\n    \n\n      \nThe embedding does not capture syntactic and semantic relationships between words. For instance, “bank” in the context of a financial institution and “bank” in the context of a riverbank will have the same representation.\n\n    \n\n  \n\n\n\n\n\nHigh Dimensionality\n\n\n\n\n  \nLarge Vocabulary Size:\n\n    \n\n      \nThe dimensionality of BoW vectors is equal to the number of unique words in the corpus, which can be extremely large. This leads to very high-dimensional vectors, resulting in increased computational cost and memory usage.\n\n    \n\n  \n\n  \nSparsity:\n\n    \n\n      \nMost documents use only a small fraction of the total vocabulary, resulting in sparse vectors with many zero values. This sparsity can make storage and computation inefficient.\n\n    \n\n  \n\n\n\n\n\nLack of Handling of Polysemy and Synonymy\n\n\n\n\n  \nPolysemy:\n\n    \n\n      \nPolysemous words (same word with multiple meanings) are treated as a single feature, failing to capture their different senses based on context. Traditional word embedding algorithms assign a distinct vector to each word, which makes them unable to account for polysemy. For instance, the English word “bank” translates to two different words in French—”banque” (financial institution) and “banc” (riverbank)—capturing its distinct meanings.\n\n    \n\n  \n\n  \nSynonymy:\n\n    \n\n      \nSynonyms (different words with similar meaning) are treated as completely unrelated features. For example, “happy” and “joyful” will have different vector representations even though they have similar meanings.\n\n    \n\n  \n\n\n\n\n\nFixed Vocabulary\n\n\n\n\n  \nOOV (Out-of-Vocabulary) Words:\n BoW cannot handle words that were not present in the training corpus. Any new word encountered will be ignored or misrepresented, leading to potential loss of information.\n\n\n\n\n\nFeature Independence Assumption\n\n\n\n\n  \nNo Inter-Feature Relationships:\n BoW assumes that the presence or absence of a word in a document is independent of other words. This independence assumption ignores any potential relationships or dependencies between words, which can be crucial for understanding context and meaning.\n\n\n\n\n\nScalability Issues\n\n\n\n\n  \nComputational Inefficiency:\n As the size of the corpus increases, the vocabulary size also increases, leading to scalability issues. High-dimensional vectors require more computational resources for processing, storing, and analyzing the data.\n\n\n\n\n\nNo Weighting Mechanism\n\n\n\n\n  \nEqual Importance:\n In its simplest form, BoW treats all words with equal importance, which is not always appropriate. Common but less informative words (e.g., “the”, “is”) are treated the same as more informative words (e.g., “cat”, “bank”).\n\n\n\n\n\nLack of Generalization\n\n\n\n\n  \nPoor Performance on Short Texts:\n BoW can be particularly ineffective for short texts or documents with limited content, where the lack of context and the sparse nature of the vector representation can lead to poor performance.\n\n\n\n\n\nExamples of Limitations\n\n\n\n\n  \nExample of Lack of Contextual Information:\n\n    \n\n      \nConsider two sentences: “Apple is looking at buying a U.K. startup.” and “Startup is looking at buying an Apple.” Both would have similar BoW representations but convey different meanings.\n\n    \n\n  \n\n  \nExample of High Dimensionality and Sparsity:\n\n    \n\n      \nA corpus with 100,000 unique words results in BoW vectors of dimension 100,000, most of which would be zeros for any given document.\n\n    \n\n  \n\n\n\n\n\nSummary\n\n\n\n\n  \nWhile BoW embeddings provide a straightforward and intuitive way to represent text data, their limitations make them less suitable for complex NLP tasks that require understanding context, handling large vocabularies efficiently, or dealing with semantic and syntactic nuances. More advanced techniques like TF-IDF, word embeddings (e.g., Word2Vec, GloVe, fastText), and contextual embeddings (e.g., ELMo, BERT) address many of these limitations by incorporating context, reducing dimensionality, and capturing richer semantic information.\n\n\n\n\n\nTerm Frequency-Inverse Document Frequency (TF-IDF)\n\n\n\n\n  \nTerm Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. It is a fundamental technique in text processing that ranks the relevance of documents to a specific query, commonly applied in tasks such as document classification, search engine ranking, information retrieval, and text mining.\n\n  \nThe TF-IDF value increases proportionally with the number of times a word appears in the document, but this is offset by the frequency of the word in the corpus, which helps to control for the fact that some words (e.g., “the”, “is”, “and”) are generally more common than others.\n\n\n\n\n\nTerm Frequency (TF)\n\n\n\n\n  \nTerm Frequency measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (the total number of terms in the document) as a way of normalization:\n\n\n\n\n\\[\\text{TF(t)} = \\frac{\\text{Number of times term }t\\text{ appears in a document}}{\\text{Total number of terms in the document}}\\]\n\n\nInverse Document Frequency (IDF)\n\n\n\n\n  \nInverse Document Frequency measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms, like “is”, “of”, and “that”, may appear a lot of times but have little importance. Thus, we need to weigh down the frequent terms while scaling up the rare ones, by computing the following:\n\n\n\n\n\\[\\text{IDF(t)} = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term }t\\text{ in it}} \\right)\\]\n\n\nExample\n\n\n\nSteps to Calculate TF-IDF\n\n\n\n\n  \nStep 1:\n \nTF (Term Frequency):\n Number of times a word appears in a document divided by the total number of words in that document.\n\n  \nStep 2:\n \nIDF (Inverse Document Frequency):\n Calculated as \nlog(N / df)\n, where:\n    \n\n      \nN\n is the total number of documents in the collection.\n\n      \ndf\n is the number of documents containing the word.\n\n    \n\n  \n\n  \nStep 3:\n \nTF-IDF:\n The product of TF and IDF.\n\n\n\n\n\nDocument Collection\n\n\n\n\n  \nDoc 1:\n “The sky is blue.”\n\n  \nDoc 2:\n “The sun is bright.”\n\n  \nTotal documents (\nN\n): 2\n\n\n\n\n\nCalculate Term Frequency (TF)\n\n\n\n\n    \n\n        \n\n            \n\n                \nWord\n\n                \nTF in Doc 1 (\"The sky is blue\")\n\n                \nTF in Doc 2 (\"The sun is bright\")\n\n            \n\n        \n\n        \n\n            \n\n                \nthe\n\n                \n1/4\n\n                \n1/5\n\n            \n\n            \n\n                \nsky\n\n                \n1/4\n\n                \n0/5\n\n            \n\n            \n\n                \nis\n\n                \n1/4\n\n                \n1/5\n\n            \n\n            \n\n                \nblue\n\n                \n1/4\n\n                \n0/5\n\n            \n\n            \n\n                \nsun\n\n                \n0/4\n\n                \n1/5\n\n            \n\n            \n\n                \nbright\n\n                \n0/4\n\n                \n1/5\n\n            \n\n        \n\n    \n\n\n\n\n\nCalculate Document Frequency (DF) and Inverse Document Frequency (IDF)\n\n\n\n\n    \n\n        \n\n            \n\n                \nWord\n\n                \nDF (in how many docs)\n\n                \nIDF (log(N/DF))\n\n            \n\n        \n\n        \n\n            \n\n                \nthe\n\n                \n2\n\n                \nlog(2/2) = 0\n\n            \n\n            \n\n                \nsky\n\n                \n1\n\n                \nlog(2/1) ≈ 0.693\n\n            \n\n            \n\n                \nis\n\n                \n2\n\n                \nlog(2/2) = 0\n\n            \n\n            \n\n                \nblue\n\n                \n1\n\n                \nlog(2/1) ≈ 0.693\n\n            \n\n            \n\n                \nsun\n\n                \n1\n\n                \nlog(2/1) ≈ 0.693\n\n            \n\n            \n\n                \nbright\n\n                \n1\n\n                \nlog(2/1) ≈ 0.693\n\n            \n\n        \n\n    \n\n\n\n\n\nCalculate TF-IDF for Each Word\n\n\n\n\n    \n\n        \n\n            \n\n                \nWord\n\n                \nTF in Doc 1\n\n                \nIDF\n\n                \nTF-IDF in Doc 1\n\n                \nTF in Doc 2\n\n                \nIDF\n\n                \nTF-IDF in Doc 2\n\n            \n\n        \n\n        \n\n            \n\n                \nthe\n\n                \n1/4\n\n                \n0\n\n                \n0\n\n                \n1/5\n\n                \n0\n\n                \n0\n\n            \n\n            \n\n                \nsky\n\n                \n1/4\n\n                \nlog(2) ≈ 0.693\n\n                \n(1/4) * 0.693 ≈ 0.173\n\n                \n0/5\n\n                \nlog(2) ≈ 0.693\n\n                \n0\n\n            \n\n            \n\n                \nis\n\n                \n1/4\n\n                \n0\n\n                \n0\n\n                \n1/5\n\n                \n0\n\n                \n0\n\n            \n\n            \n\n                \nblue\n\n                \n1/4\n\n                \nlog(2) ≈ 0.693\n\n                \n(1/4) * 0.693 ≈ 0.173\n\n                \n0/5\n\n                \nlog(2) ≈ 0.693\n\n                \n0\n\n            \n\n            \n\n                \nsun\n\n                \n0/4\n\n                \nlog(2) ≈ 0.693\n\n                \n0\n\n                \n1/5\n\n                \nlog(2) ≈ 0.693\n\n                \n(1/5) * 0.693 ≈ 0.139\n\n            \n\n            \n\n                \nbright\n\n                \n0/4\n\n                \nlog(2) ≈ 0.693\n\n                \n0\n\n                \n1/5\n\n                \nlog(2) ≈ 0.693\n\n                \n(1/5) * 0.693 ≈ 0.139\n\n            \n\n        \n\n    \n\n\n\n\n\nExplanation of Table\n\n\n\n  \nThe \nTF\n column shows the term frequency for each word in each document.\n\n  \nThe \nIDF\n column shows the inverse document frequency for each word.\n\n  \nThe \nTF-IDF\n columns for Doc 1 and Doc 2 show the final TF-IDF score for each word, calculated as \nTF * IDF\n.\n\n\n\n\n\nKey Observations\n\n\n\n  \nWords like “the” and “is” have an IDF of 0 because they appear in both documents, making them less distinctive.\n\n  \nWords like “blue,” “sun,” and “bright” have higher TF-IDF values because they appear in only one document, making them more distinctive for that document.\n    \n\n      \nThe TF-IDF score for “blue” in Doc 1 is thus a measure of its importance in that document, within the context of the given document collection. This score would be different in a different document or a different collection, reflecting the term’s varying importance.\n\n    \n\n  \n\n\n\n\n\nLimitations of TF-IDF\n\n\n\n\n  \nWhile TF-IDF is a powerful tool for certain applications, the limitations highlighted below make it less suitable for tasks that require deep understanding of language, such as semantic search, word sense disambiguation, or processing of very short or dynamically changing texts. This has led to the development and adoption of more advanced techniques like word embeddings and neural network-based models in natural language processing.\n\n\n\n\n\nLack of Context and Word Order\n\n\n\n\n  \nTF-IDF treats each word in a document independently and does not consider the context in which a word appears. This means it cannot capture the meaning of words based on their surrounding words or the overall semantic structure of the text. The word order is also ignored, which can be crucial in understanding the meaning of a sentence.\n\n\n\n\n\nDoes Not Account for Polysemy\n\n\n\n\n  \nWords with multiple meanings (polysemy) are treated the same regardless of their context. For example, the word “bank” would have the same representation in “river bank” and “savings bank”, even though it has different meanings in these contexts.\n\n\n\n\n\nLack of Semantic Understanding\n\n\n\n\n  \nTF-IDF relies purely on the statistical occurrence of words in documents, which means it lacks any understanding of the semantics of the words. It cannot capture synonyms or related terms unless they appear in similar documents within the corpus.\n\n\n\n\n\nBias Towards Rare Terms\n\n\n\n\n  \nWhile the IDF component of TF-IDF aims to balance the frequency of terms, it can sometimes overly emphasize rare terms. This might lead to overvaluing words that appear infrequently but are not necessarily more relevant or important in the context of the document.\n\n\n\n\n\nVocabulary Limitation\n\n\n\n\n  \nThe TF-IDF model is limited to the vocabulary of the corpus it was trained on. It cannot handle new words that were not in the training corpus, making it less effective for dynamic content or languages that evolve rapidly.\n\n\n\n\n\nNormalization Issues\n\n\n\n\n  \nThe normalization process in TF-IDF (e.g., dividing by the total number of words in a document) may not always be effective in balancing document lengths and word frequencies, potentially leading to skewed results.\n\n\n\n\n\nRequires a Large and Representative Corpus\n\n\n\n\n  \nFor the IDF part of TF-IDF to be effective, it needs a large and representative corpus. If the corpus is not representative of the language or the domain of interest, the IDF scores may not accurately reflect the importance of the words.\n\n\n\n\n\nNo Distinction Between Different Types of Documents\n\n\n\n\n  \nTF-IDF treats all documents in the corpus equally, without considering the type or quality of the documents. This means that all sources are considered equally authoritative, which may not be the case.\n\n\n\n\n\nPoor Performance with Short Texts\n\n\n\n\n  \nIn very short documents, like tweets or SMS messages, the TF-IDF scores can be less meaningful because of the limited word occurrence and context.\n\n\n\n\n\nBest Match 25 (BM25)\n\n\n\n\n  \nBM25 is a ranking function used in information retrieval systems, particularly in search engines, to rank documents based on their relevance to a given search query. It’s a part of the family of probabilistic information retrieval models and is an extension of the TF-IDF (Term Frequency-Inverse Document Frequency) approach, though it introduces several improvements and modifications.\n\n\n\n\n\nKey Components of BM25\n\n\n\n\n  \n\n    \nTerm Frequency (TF)\n: BM25 modifies the term frequency component of TF-IDF to address the issue of term saturation. In TF-IDF, the more frequently a term appears in a document, the more it is considered relevant. However, this can lead to a problem where beyond a certain point, additional occurrences of a term don’t really indicate more relevance. BM25 addresses this by using a logarithmic scale for term frequency, which allows for a point of diminishing returns, preventing a term’s frequency from having an unbounded impact on the document’s relevance.\n\n  \n\n  \n\n    \nInverse Document Frequency (IDF)\n: Like TF-IDF, BM25 includes an IDF component, which helps to weight a term’s importance based on how rare or common it is across all documents. The idea is that terms that appear in many documents are less informative than those that appear in fewer documents.\n\n  \n\n  \n\n    \nDocument Length Normalization\n: BM25 introduces a sophisticated way of handling document length. Unlike TF-IDF, which may unfairly penalize longer documents, BM25 normalizes for length in a more balanced manner, reducing the impact of document length on the calculation of relevance.\n\n  \n\n  \n\n    \nTunable Parameters\n: BM25 includes parameters like \nk1\n and \nb\n, which can be adjusted to optimize performance for specific datasets and needs. \nk1\n controls how quickly an increase in term frequency leads to term saturation, and \nb\n controls the degree of length normalization.\n\n  \n\n\n\n\n\nExample\n\n\n\n\n  \n\n    \nImagine you have a collection of documents and a user searches for “solar energy advantages”.\n\n\n    \n\n      \nDocument A\n is 300 words long and mentions “solar energy” 4 times and “advantages” 3 times.\n\n      \nDocument B\n is 1000 words long and mentions “solar energy” 10 times and “advantages” 1 time.\n\n    \n\n  \n\n  \nUsing BM25:\n    \n\n      \nTerm Frequency\n: The term “solar energy” appears more times in Document B, but due to term saturation, the additional occurrences don’t contribute as much to its relevance score as the first few mentions.\n\n      \nInverse Document Frequency\n: If “solar energy” and “advantages” are relatively rare in the overall document set, their appearances in these documents increase the relevance score more significantly.\n\n      \nDocument Length Normalization\n: Although Document B is longer, BM25’s length normalization ensures that it’s not unduly penalized simply for having more words. The relevance of the terms is balanced against the length of the document.\n\n    \n\n  \n\n  \nSo, despite Document B having more mentions of “solar energy”, BM25 will calculate the relevance of both documents in a way that balances term frequency, term rarity, and document length, potentially ranking them differently based on how these factors interplay. The final relevance scores would then determine their ranking in the search results for the query “solar energy advantages”.\n\n\n\n\n\nBM25: Evolution of TF-IDF\n\n\n\n\n  \nBM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query. It’s part of the probabilistic information retrieval model and is considered an evolution of the TF-IDF (Term Frequency-Inverse Document Frequency) model. Both are used to rank documents based on their relevance to a query, but they differ in how they calculate this relevance.\n\n\n\n\n\nBM25\n\n\n\n  \nTerm Frequency Component\n: Like TF-IDF, BM25 considers the frequency of the query term in a document. However, it adds a saturation point to prevent a term’s frequency from disproportionately influencing the document’s relevance.\n\n  \nLength Normalization\n: BM25 adjusts for the length of the document, penalizing longer documents less harshly than TF-IDF.\n\n  \nTuning Parameters\n: It includes two parameters, \nk1\n and \nb\n, which control term saturation and length normalization, respectively. These can be tuned to suit specific types of documents or queries.\n\n\n\n\n\nTF-IDF\n\n\n\n  \nTerm Frequency\n: TF-IDF measures the frequency of a term in a document. The more times the term appears, the higher the score.\n\n  \nInverse Document Frequency\n: This component reduces the weight of terms that appear in many documents across the corpus, assuming they are less informative.\n\n  \nSimpler Model\n: TF-IDF is generally simpler than BM25 and doesn’t involve parameters like \nk1\n or \nb\n.\n\n\n\n\n\nExample\n\n\n\n  \n\n    \nImagine a search query “chocolate cake recipe” and two documents:\n\n\n    \n\n      \nDocument A\n: 100 words, “chocolate cake recipe” appears 10 times.\n\n      \nDocument B\n: 1000 words, “chocolate cake recipe” appears 15 times.\n\n    \n\n\n    \nUsing TF-IDF\n:\n\n    \n\n      \nThe term frequency for “chocolate cake recipe” would be higher in Document A.\n\n      \nDocument B, being longer, might get a lower relevance score due to less frequency of the term.\n\n    \n\n\n    \nUsing BM25\n:\n\n    \n\n      \nThe term frequency component would reach a saturation point, meaning after a certain frequency, additional occurrences of “chocolate cake recipe” contribute less to the score.\n\n      \nLength normalization in BM25 would not penalize Document B as heavily as TF-IDF, considering its length.\n\n      \nThe tuning parameters \nk1\n and \nb\n could be adjusted to optimize the balance between term frequency and document length.\n\n    \n\n  \n\n  \n\n    \nIn essence, while both models aim to determine the relevance of documents to a query, BM25 offers a more nuanced and adjustable approach, especially beneficial in handling longer documents and ensuring that term frequency doesn’t disproportionately affect relevance.\n\n  \n\n\n\n\n\nLimitations of BM25\n\n\n\n\n  \nUnderstanding the limitations below is crucial when implementing BM25 in a search engine or information retrieval system, as it helps in identifying cases where BM25 might need to be supplemented with other techniques or algorithms for better performance.\n\n\n\n\n\nParameter Sensitivity\n\n\n\n\n  \nBM25 includes parameters like \nk1\n and \nb\n, which need to be fine-tuned for optimal performance. This tuning process can be complex and is highly dependent on the specific nature of the document collection and queries. Inappropriate parameter settings can lead to suboptimal results.\n\n\n\n\n\nNon-Handling of Semantic Similarities\n\n\n\n\n  \nBM25 primarily relies on exact keyword matching. It does not account for the semantic relationships between words. For instance, it would not recognize “automobile” and “car” as related terms unless explicitly programmed to do so. This limitation makes BM25 less effective in understanding the context or capturing the nuances of language.\n\n\n\n\n\nIneffectiveness with Short Queries or Documents\n\n\n\n\n  \nBM25’s effectiveness can decrease with very short queries or documents, as there are fewer words to analyze, making it harder to distinguish relevant documents from irrelevant ones.\n\n\n\n\n\nLength Normalization Challenges\n\n\n\n\n  \nWhile BM25’s length normalization aims to prevent longer documents from being unfairly penalized, it can sometimes lead to the opposite problem, where shorter documents are unduly favored. The balance is not always perfect, and the effectiveness of the normalization can vary based on the dataset.\n\n\n\n\n\nQuery Term Independence\n\n\n\n\n  \nBM25 assumes independence between query terms. It doesn’t consider the possibility that the presence of certain terms together might change the relevance of a document compared to the presence of those terms individually.\n\n\n\n\n\nDifficulty with Rare Terms\n\n\n\n\n  \nLike TF-IDF, BM25 can struggle with very rare terms. If a term appears in very few documents, its IDF (Inverse Document Frequency) component can become disproportionately high, skewing results.\n\n\n\n\n\nPerformance in Specialized Domains\n\n\n\n\n  \nIn specialized domains with unique linguistic features (like legal, medical, or technical fields), BM25 might require significant customization to perform well. This is because standard parameter settings and term-weighting mechanisms may not align well with the unique characteristics of these specialized texts.\n\n\n\n\n\nIgnoring Document Quality\n\n\n\n\n  \nBM25 focuses on term frequency and document length but doesn’t consider other aspects that might indicate document quality, such as authoritativeness, readability, or the freshness of information.\n\n\n\n\n\nVulnerability to Keyword Stuffing\n\n\n\n\n  \nLike many other keyword-based algorithms, BM25 can be susceptible to keyword stuffing, where documents are artificially loaded with keywords to boost relevance.\n\n\n\n\n\nIncompatibility with Complex Queries\n\n\n\n\n  \nBM25 is less effective for complex queries, such as those involving natural language questions or multi-faceted information needs. It is designed for keyword-based queries and may not perform well with queries that require understanding of context or intent.\n\n\n\n\n\nWord2Vec\n\n\n\n\n  \nProposed in \nEfficient Estimation of Word Representations in Vector Space\n by Mikolov et al. (2013), the Word2Vec algorithm marked a significant advancement in the field of NLP as a notable example of a word embedding technique.\n\n  \nWord2Vec is renowned for its effectiveness in learning word vectors, which are then used to decode the semantic relationships between words. It utilizes a vector space model to encapsulate words in a manner that captures both semantic and syntactic relationships. This method enables the algorithm to discern similarities and differences between words, as well as to identify analogous relationships, such as the parallel between “Stockholm” and “Sweden” and “Cairo” and “Egypt.”\n\n  \nWord2Vec’s methodology of representing words as vectors in a semantic and syntactic space has profoundly impacted the field of NLP, offering a robust framework for capturing the intricacies of language and its usage.\n\n\n\n\n\nMotivation behind Word2Vec: The Need for Context-based Semantic Understanding\n\n\n\n\n  \n\n    \nTF-IDF and BM25 are methods used in information retrieval to rank documents based on their relevance to a query. While they provide useful measures for text analysis, they do not offer context-based “semantic” embeddings (in the same way that \nWord2Vec\n or \nBERT\n embeddings do). Here’s why:\n\n\n    \n\n      \n\n        \nTF-IDF\n: This method calculates a weight for each word in a document, which increases with the number of times the word appears in the document but decreases based on the frequency of the word across all documents. TF-IDF is good at identifying important words in a document but doesn’t capture the meaning of the words or their relationships with each other. It’s more about word importance than word meaning.\n\n      \n\n      \n\n        \nBM25\n: An extension of TF-IDF, BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query. While it improves upon TF-IDF by incorporating probabilistic understanding of term occurrence and handling of term saturation, it still fundamentally operates on term frequency and inverse document frequency. Like TF-IDF, BM25 doesn’t inherently capture semantic relationships between words.\n\n      \n\n    \n\n  \n\n  \nIn contrast, semantic embeddings (like those from \nWord2Vec\n, \nBERT\n, etc.) are designed to capture the meanings of words and their relationships to each other. These embeddings represent words as vectors in a way that words with similar meanings are located close to each other in the vector space, enabling the capture of semantic relationships and nuances in language.\n\n  \nTherefore, while TF-IDF and BM25 are valuable tools for information retrieval and determining document relevance, they do not provide semantic embeddings of words or phrases. They are more focused on word occurrence and frequency rather than on capturing the underlying meanings and relationships of words.\n\n\n\n\n\nCore Idea\n\n\n\n\n  \nWord2Vec employs a shallow neural network, trained on a large textual corpus, to predict the context surrounding a given word. The essence of Word2Vec lies in its ability to convert words into high-dimensional vectors. This representation allows the algorithm to capture the meaning, semantic similarity, and relationships with surrounding text. A notable feature of Word2Vec is its capacity to perform arithmetic operations with these vectors to reveal linguistic patterns, such as the famous analogy \nking - man + woman = queen\n.\n\n\n\n\n\nWord2Vec Architectures\n\n\n\n\n  \n\n    \nWord2Vec offers two distinct architectures for training:\n\n\n    \n\n      \nContinuous Bag-of-Words (CBOW)\n:\n        \n\n          \nThis model predicts a target word based on its context words. CBOW computes the conditional probability of a target word given the context words surrounding it across a window of size k. The input is a summation of the word vectors of the surrounding context words, with the output being the current word. \n  \n\n          \nReferring to the figure above (\nsource\n), the CBOW model is a simple fully connected neural network with one hidden layer. The input layer, which takes the one-hot vector of context words, has \\(\\mathbf{V}\\) neurons, while the hidden layer has \\(\\mathbf{N}\\) neurons. The output layer is a softmax of all words in the vocabulary. The layers are connected by weight matrices \\(\\mathbf{W} \\in \\mathcal{R}^{V \\times N}\\) and \\(\\mathbf{W}^{\\prime} \\in \\mathcal{R}^{N \\times V}\\). Each word from the vocabulary is finally represented as two learned vectors, \\(\\mathbf{v}_{\\mathbf{c}}\\) and \\(\\mathbf{v}_{\\mathbf{w}}\\), corresponding to context and target word representations, respectively. Thus, the \\(k^{\\text{th}}\\) word in the vocabulary will have:\n\n        \n\n\n\\[\\mathbf{v}_{\\mathbf{c}}=\\mathbf{W}_{(\\mathbf{k}, .)} \\quad \\text { and } \\quad \\mathbf{v}_{\\mathbf{w}}=\\mathbf{W}_{(., \\mathbf{k})}^{\\prime}\\]\n\n        \n\n          \nOverall, for any word \\(w_i\\) with a given context word \\(c\\) as input:\n\n        \n\n\n\\[\\mathbf{P}\\left(\\frac{\\mathbf{w}_{\\mathbf{i}}}{\\mathbf{c}}\\right)=\\mathbf{y}_{\\mathbf{i}}=\\frac{e^{u_i}}{\\sum_{i=1}^V e^{u_i}} \\quad \\text { where }, \\mathbf{u}_{\\mathbf{i}}=\\mathbf{v}_{\\mathbf{w}_{\\mathbf{i}}}^{\\mathbf{T}} \\cdot \\mathbf{v}_{\\mathbf{c}}\\]\n\n        \n\n          \nThe parameters \\(\\theta=\\left\\{\\mathbf{V}_{\\mathbf{w}}, \\mathbf{V}_{\\mathbf{c}}\\right\\}\\) are learned by defining the objective function as the log-likelihood and finding its gradient as:\n\n        \n\n\n\\[\\begin{gathered}\n \\mathbf{l}(\\theta)=\\sum_{\\mathbf{w} \\in \\text { Vocabulary }} \\log \\left(\\mathbf{P}\\left(\\frac{\\mathbf{w}}{\\mathbf{c}}\\right)\\right) \\\\\n \\frac{\\partial \\mathbf{l}(\\theta)}{\\partial \\mathbf{V}_{\\mathbf{w}}}=\\mathbf{V}_{\\mathbf{c}}\\left(\\mathbf{1}-\\mathbf{P}\\left(\\frac{\\mathbf{w}}{\\mathbf{c}}\\right)\\right)\n \\end{gathered}\\]\n\n        \n\n          \nIn the general CBOW model, all the one-hot vectors of context words are taken as input simultaneously, i.e.,\n\n        \n\n\n\\[\\mathbf{h}=\\mathbf{W}^{\\mathbf{T}}\\left(\\mathbf{x}_1+\\mathbf{x}_2+\\ldots+\\mathbf{x}_{\\mathbf{c}}\\right)\\]\n      \n\n      \nContinuous Skip-gram\n: Conversely, the Skip-gram model predicts the surrounding context words from a given target word. The input is the target word, and the output is a softmax classification over the entire vocabulary to predict the context words. Skip-gram does the exact opposite of the CBOW model by predicting the surrounding context words given the central target word. The context words are assumed to be located symmetrically to the target word within a distance equal to the window size in both directions.\n\n    \n\n  \n\n  \n\n    \nThe following figure from the paper shows the two Word2Vec model architectures. The CBOW architecture predicts the current word based on the context, and the skip-gram predicts surrounding words given the current word.\n\n  \n\n\n\n\n\n\n\n\nTraining and Optimization\n\n\n\n\n  \nThe training of Word2Vec involves representing every word in a fixed vocabulary by a vector and then optimizing these vectors to predict surrounding words accurately. This is achieved through stochastic gradient descent, minimizing a loss function that indicates the discrepancy between predicted and actual context words. The algorithm uses a sliding window approach to maximize the probability of context words given a center word, as illustrated in the accompanying diagram:\n\n\n\n\n\n\n\n\nEmbedding and Semantic Relationships\n\n\n\n\n  \nThrough the training process, Word2Vec places words with similar meanings in proximity within the high-dimensional vector space. For example, ‘bread’ and ‘croissant’ would have closely aligned vectors, just as ‘woman’, ‘king’, and ‘man’ would demonstrate meaningful relationships through vector arithmetic:\n\n\n\n\n\n\n\n\nDistinction from Traditional Models\n\n\n\n\n  \nA key differentiation between Word2Vec and traditional count-based language models is its reliance on embeddings. Deep learning-based NLP models, including Word2Vec, represent words, phrases, and even sentences using these embeddings, which encode much richer contextual and semantic information.\n\n\n\n\n\nSemantic Nature of Word2Vec Embeddings\n\n\n\n\n  \nWord2Vec embeddings are considered semantic in the sense that they capture semantic relationships between words based on their usage in the text. The key idea behind Word2Vec is that words used in similar contexts tend to have similar meanings. This is often summarized by the phrase “a word is characterized by the company it keeps.”\n\n  \nWhen Word2Vec is trained on a large corpus of text, it learns vector representations (embeddings) for words such that words with similar meanings have similar embeddings. This is achieved through either of two model architectures: Continuous Bag-of-Words (CBOW) or Skip-Gram.\n    \n\n      \nCBOW Model\n: This model predicts a target word based on context words. For example, in the sentence “The cat sat on the ___”, the model tries to predict the word ‘mat’ based on the context provided by the other words.\n\n      \nSkip-Gram Model\n: This model works the other way around, where it uses a target word to predict context words. For instance, given the word ‘cat’, it tries to predict ‘the’, ‘sat’, ‘on’, and ‘mat’.\n\n    \n\n  \n\n  \nThese embeddings capture various semantic relationships, such as:\n    \n\n      \nSimilarity\n: Words with similar meanings have embeddings that are close in the vector space.\n\n      \nAnalogy\n: Relationships like “man is to woman as king is to queen” can often be captured through vector arithmetic (e.g., \nvector('king') - vector('man') + vector('woman')\n is close to \nvector('queen')\n).\n\n      \nClustering\n: Words with similar meanings tend to cluster together in the vector space.\n\n    \n\n  \n\n  \nHowever, it’s important to note that while Word2Vec captures many semantic relationships, it also has limitations. For example, it doesn’t capture polysemy well (the same word having different meanings in different contexts) and sometimes the relationships it learns are more syntactic than semantic. More advanced models like BERT and GPT have since been developed to address some of these limitations.\n\n\n\n\n\nKey Limitations and Advances in Word2Vec and Word Embeddings\n\n\n\n\n  \n\n    \nWord2Vec, a pivotal development in natural language processing, has significantly advanced our understanding of semantic relationships between words through vector embeddings. However, despite its breakthrough status, Word2Vec and traditional word embeddings are not without limitations, many of which have been addressed in subsequent developments within the field.\n\n  \n\n  \nStatic, Non-Contextualized Nature\n:\n    \n\n      \nSingle Vector Per Word\n: Word2Vec assigns a unique vector to each word, which remains static regardless of the word’s varying context in different sentences. This results in a representation that cannot dynamically adapt to different usages of the same word.\n\n      \nCombination of Contexts\n: In cases where a word like “bank” appears in multiple contexts (“river bank” vs. “financial bank”), Word2Vec does not generate distinct embeddings for each scenario. Instead, it creates a singular, averaged representation that amalgamates all the contexts in which the word appears, leading to a generalized semantic representation.\n\n      \nLack of Disambiguation\n: The model’s inability to differentiate between the multiple meanings of polysemous words means that words like “bank” are represented by a single vector, irrespective of the specific meaning in a given context.\n\n      \nContext Window Limitation\n: Word2Vec employs a fixed-size context window, capturing only local co-occurrence patterns without a deeper understanding of the word’s role in the broader sentence or paragraph.\n\n    \n\n  \n\n  \nTraining Process and Computational Intensity\n:\n    \n\n      \nAdjustments During Training\n: Throughout the training process, the word vectors are continually adjusted, not to switch between meanings but to refine the word’s placement in the semantic space based on an aggregate of its various uses.\n\n      \nResource Demands\n: Training Word2Vec, particularly for large vocabularies, requires significant computational resources and time. Techniques like negative sampling were introduced to alleviate some of these demands, but computational intensity remains a challenge.\n\n    \n\n  \n\n  \nHandling of Special Cases\n:\n    \n\n      \nPhrase Representation\n: Word2Vec struggles with representing phrases or idioms where the meaning is not simply an aggregation of the meanings of individual words. For instance, idioms like “hot potato” or named entities such as “Boston Globe” cannot be accurately represented by the combination of individual word embeddings. One solution, as explored by \nMikolov et al. (2013)\n, is to identify such phrases based on word co-occurrence and train embeddings for them separately. More recent methods have explored directly learning n-gram embeddings from unlabeled data.\n\n      \nOut-of-Vocabulary Words\n: The model faces challenges with unknown or out-of-vocabulary (OOV) words. This issue is better addressed in models that treat words as compositions of characters, such as character embeddings, which are especially beneficial for languages with non-segmented scripts.\n\n    \n\n  \n\n  \nGlobal Vector Representation Limitations\n:\n    \n\n      \nUniform Representation Across Contexts\n: Word2Vec, like other traditional methods, generates a global vector representation for each word, which does not account for the various meanings a word can have in different contexts. For example, the different senses of “bank” in diverse sentences are not captured distinctively. This results in embeddings that are less effective in tasks requiring precise contextual understanding, such as sentiment analysis.\n\n      \nSentiment Polarity Issues\n: Learning embeddings based only on a small window of surrounding words can lead to words with opposing sentiment polarities (e.g., “good” and “bad”) sharing almost the same embedding. This is problematic in tasks such as sentiment analysis, where it is crucial to distinguish between these sentiments. \nTang et al. (2014)\n addressed this issue by proposing sentiment-specific word embeddings (SSWE), which incorporate the supervised sentiment polarity of text in their loss functions while learning the embeddings.\n\n    \n\n  \n\n  \nResulting Embedding Compromises\n:\n    \n\n      \nThe resulting vector for words with distinct meanings is a compromise that reflects its diverse uses, leading to less precise representations for tasks requiring accurate contextual understanding.\n\n    \n\n  \n\n  \nThese limitations of Word2Vec and traditional word embeddings have spurred advancements in the field, leading to the development of more sophisticated language models like BERT and ELMo. These newer models address issues of context sensitivity, polysemy, computational efficiency, and handling of OOV words, marking a significant progression in natural language processing.\n\n\n\n\n\nFurther Learning Resources\n\n\n\n\n  \nFor those interested in a deeper exploration of Word2Vec, the following resources provide comprehensive insights into the foundational aspects of Word2Vec:\n    \n\n      \nChris McCormick’s \nWord2Vec Tutorial - The Skip-Gram Model\n\n      \nWord2Vec Tutorial Part 2 - Negative Sampling\n\n      \nApplying word2vec to Recommenders and Advertising\n\n      \nJay Alammar’s \nThe Illustrated Word2vec\n; \nVideo\n\n    \n\n  \n\n\n\n\n\nGlobal Vectors for Word Representation (GloVe)\n\n\n\nOverview\n\n\n\n\n  \nProposed in \nGloVe: Global Vectors for Word Representation\n by Pennington et al. (2014), Global Vectors for Word Representation (GloVe) embeddings are a type of word representation used in NLP. They are designed to capture not just the local context of words but also their global co-occurrence statistics in a corpus, thus providing a rich and nuanced word representation.\n\n  \nBy blending these approaches, GloVe captures a fuller picture of word meaning and usage, making it a valuable tool for various NLP tasks, such as sentiment analysis, machine translation, and information retrieval.\n\n  \nHere’s a detailed explanation along with an example:\n\n\n\n\n\nHow GloVe Works\n\n\n\n\n  \n\n    \nCo-Occurrence Matrix:\n GloVe starts by constructing a large matrix that represents the co-occurrence statistics of words in a given corpus. This matrix has dimensions of \n[vocabulary size] x [vocabulary size]\n, where each entry \\((i, j)\\) in the matrix represents how often word i occurs in the context of word j.\n\n  \n\n  \n\n    \nMatrix Factorization:\n The algorithm then applies matrix factorization techniques to this co-occurrence matrix. The goal is to reduce the dimensions of each word into a lower-dimensional space (the embedding space), while preserving the co-occurrence information.\n\n  \n\n  \n\n    \nWord Vectors:\n The end result is that each word in the corpus is represented by a vector in this embedding space. Words with similar meanings or that often appear in similar contexts will have similar vectors.\n\n  \n\n  \n\n    \nRelationships and Analogies:\n These vectors capture complex patterns and relationships between words. For example, they can capture analogies like “man is to king as woman is to queen” by showing that the vector ‘king’ - ‘man’ + ‘woman’ is close to ‘queen’.\n\n  \n\n\n\n\n\nExample\n\n\n\n\n  \nImagine a simple corpus with the following sentences:\n    \n\n      \n“The cat sat on the mat.”\n\n      \n“The dog sat on the log.”\n\n    \n\n  \n\n  \nFrom this corpus, a co-occurrence matrix is constructed. For instance, ‘cat’ and ‘mat’ will have a higher co-occurrence score because they appear close to each other in the sentences. Similarly, ‘dog’ and ‘log’ will be close in the embedding space.\n\n  \nAfter applying GloVe, each word (like ‘cat’, ‘dog’, ‘mat’, ‘log’) will be represented as a vector. The vector representation captures the essence of each word, not just based on the context within its immediate sentence, but also based on how these words co-occur in the entire corpus.\n\n  \nIn a large and diverse corpus, GloVe can capture complex relationships. For example, it might learn that ‘cat’ and ‘dog’ are both pets, and this will be reflected in how their vectors are positioned relative to each other and to other words like ‘pet’, ‘animal’, etc.\n\n\n\n\n\nSignificance of GloVe\n\n\n\n\n  \nGloVe is powerful because it combines the benefits of two major approaches in word representation:\n    \n\n      \nLocal Context Window Methods (like Word2Vec):\n These methods look at the local context, but might miss the broader context of word usage across the entire corpus.\n\n      \nGlobal Matrix Factorization Methods:\n These methods, like Latent Semantic Analysis (LSA), consider global word co-occurrence but might miss the nuances of local word usage.\n\n    \n\n  \n\n\n\n\n\nLimitations of GloVe\n\n\n\n\n  \nWhile GloVe has been widely used and offers several rich word representations, it may not be the optimal choice for every NLP application, especially those requiring context sensitivity, handling of rare words, or efficient handling of computational resources as detailed below.\n\n\n\n\n\nLack of Context-Sensitivity\n\n\n\n\n  \nIssue:\n GloVe generates a single, static vector for each word, regardless of the specific context in which the word is used. This can be a significant limitation, especially for words with multiple meanings (polysemy).\n\n  \nExample:\n The word “bank” will have the same vector representation whether it refers to the side of a river or a financial institution, potentially leading to confusion in downstream tasks where context matters.\n\n  \nComparison:\n Modern models like BERT and GPT address this limitation by creating context-sensitive embeddings, where the meaning of a word can change based on the sentence or context in which it appears.\n\n\n\n\n\nInefficient for Rare Words\n\n\n\n\n  \nIssue:\n GloVe relies on word co-occurrence statistics from large corpora, which means it may not generate meaningful vectors for rare words or words that don’t appear frequently enough in the training data.\n\n  \nExample:\n Words that occur infrequently in a corpus will have less reliable vector representations, potentially leading to poor performance on tasks that involve rare or domain-specific vocabulary.\n\n  \nComparison:\n Subword-based models like FastText handle this limitation more effectively by creating word representations based on character n-grams, allowing even rare words to have meaningful embeddings.\n\n\n\n\n\nCorpus Dependence\n\n\n\n\n  \nIssue:\n The quality of the GloVe embeddings is highly dependent on the quality and size of the training corpus. If the corpus lacks diversity or is biased, the resulting word vectors will reflect these limitations.\n\n  \nExample:\n A GloVe model trained on a narrow or biased dataset may fail to capture the full range of meanings or relationships between words, especially in domains or languages not well-represented in the corpus.\n\n  \nComparison:\n This issue is less pronounced in models like transformer-based architectures, where transfer learning allows fine-tuning on specific tasks or domains, reducing the dependence on a single corpus.\n\n\n\n\n\nComputational Cost\n\n\n\n\n  \nIssue:\n Training GloVe embeddings on large corpora involves computing and factorizing large co-occurrence matrices, which can be computationally expensive and memory-intensive.\n\n  \nExample:\n The memory requirement for storing the full co-occurrence matrix grows quadratically with the size of the vocabulary, which can be prohibitive for very large datasets.\n\n  \nComparison:\n While Word2Vec also has computational challenges, GloVe’s matrix factorization step tends to be more resource-intensive than the shallow neural networks used by Word2Vec.\n\n\n\n\n\nLimited to Word-Level Representation\n\n\n\n\n  \nIssue:\n GloVe embeddings operate at the word level and do not directly handle subword information such as prefixes, suffixes, or character-level nuances.\n\n  \nExample:\n Morphologically rich languages, where words can take many forms based on tense, gender, or plurality, may not be well-represented in GloVe embeddings.\n\n  \nComparison:\n FastText, in contrast, incorporates subword information into its word vectors, allowing it to better represent words in languages with complex morphology or in cases where a word is rare but its root form is common.\n\n\n\n\n\nInability to Handle OOV (Out-of-Vocabulary) Words\n\n\n\n\n  \nIssue:\n Since GloVe produces fixed embeddings for words during the training phase, it cannot generate embeddings for words that were not present in the training corpus, known as Out-of-Vocabulary (OOV) words.\n\n  \nExample:\n If a new or domain-specific word is encountered during testing or inference, GloVe cannot generate a meaningful vector for it.\n\n  \nComparison:\n Subword-based models like FastText or context-based models like BERT can mitigate this problem by creating embeddings dynamically, even for unseen words.\n\n\n\n\n\nfastText\n\n\n\nOverview\n\n\n\n\n  \nProposed in \nEnriching Word Vectors with Subword Information\n by Bojanowski et al. (2017), fastText is an advanced word representation and sentence classification library developed by Facebook AI Research (FAIR). It’s primarily used for text classification and word embeddings in NLP. fastText differs from traditional word embedding techniques through its unique approach to representing words, which is particularly beneficial for understanding morphologically complex languages or handling rare words.\n\n  \nSpecifically, fastText’s innovative approach of using subword information makes it a powerful tool for a variety of NLP tasks, especially in dealing with languages that have extensive word forms and in situations where the dataset contains many rare words. By learning embeddings that incorporate subword information, fastText provides a more nuanced and comprehensive understanding of language semantics compared to traditional word embedding methods.\n\n  \nHere’s a detailed look at fastText with an example.\n\n\n\n\n\nCore Features of fastText\n\n\n\n\n  \n\n    \nSubword Information\n: Unlike traditional models that treat words as the smallest unit for training, fastText breaks down words into smaller units - subwords or character n-grams. For instance, for the word “fast”, with a chosen n-gram range of 3 to 6, some of the subwords would be “fas”, “fast”, “ast”, etc. This technique helps in capturing the morphology of words.\n\n  \n\n  \n\n    \nHandling of Rare Words\n: Due to its subword approach, fastText can effectively handle rare words or even words not seen during training. It generates embeddings for these words based on their subword units, allowing it to infer some meaning from these subcomponents.\n\n  \n\n  \n\n    \nEfficiency in Learning Word Representations\n: fastText is efficient in learning representations for words that appear infrequently in the corpus, which is a significant limitation in many other word embedding techniques.\n\n  \n\n  \n\n    \nApplicability to Various Languages\n: Its subword feature makes it particularly suitable for languages with rich word formations and complex morphology, like Turkish or Finnish.\n\n  \n\n  \n\n    \nWord Embedding and Text Classification\n: fastText can be used both for generating word embeddings and for text classification purposes, providing versatile applications in NLP tasks.\n\n  \n\n\n\n\n\nExample\n\n\n\n\n  \nConsider the task of building a sentiment analysis model using word embeddings for an input sentence like “The movie was breathtakingly beautiful”. In traditional models like Word2Vec, each word is treated as a distinct unit, and if words like “breathtakingly” are rare in the training dataset, the model may not have a meaningful representation for them.\n\n  \nWith fastText, “breathtakingly” is broken down into subwords (e.g., “breat”, “eathtaking”, “htakingly”, etc.). fastText then learns vectors for these subwords. When computing the vector for “breathtakingly”, it aggregates the vectors of its subwords. This approach allows fastText to handle rare words more effectively, as it can utilize the information from common subwords to understand less common or even out-of-vocabulary words.\n\n\n\n\n\nLimitations of fastText\n\n\n\n\n  \nDespite its many strengths, fastText has several limitations that users should be aware of. These limitations can influence the effectiveness and appropriateness of fastText for certain NLP tasks, and understanding them can help users make more informed decisions when choosing word embedding models.\n\n\n\n\n\nLimited Contextual Awareness\n\n\n\nfastText operates on the principle of learning word embeddings by breaking down words into subwords. However, it does not consider the broader context in which a word appears within a sentence. This is because fastText, like Word2Vec, generates static embeddings, meaning that each word or subword is represented by the same vector regardless of its surrounding context.\n\n\n\nFor instance, the word “bank” in the sentences “He went to the bank to withdraw money” and “He sat by the river bank” will have the same embedding, even though the meanings are different in each case. More advanced models like BERT or GPT address this limitation by generating dynamic, context-sensitive embeddings.\n\n\n\nSensitivity to Subword Granularity\n\n\n\nWhile fastText’s subword approach is one of its key strengths, it can also be a limitation depending on the language and task. The choice of n-grams (i.e., the length of subwords) can have a significant impact on the quality of embeddings. Selecting the wrong subword granularity may lead to suboptimal performance, as shorter n-grams might capture too much noise, while longer n-grams may fail to generalize effectively.\n\n\n\nFurthermore, fastText might overemphasize certain subwords, leading to biases in word embeddings. For example, frequent subword combinations (e.g., prefixes and suffixes) might dominate the representation, overshadowing the contributions of other meaningful subword units.\n\n\n\nInability to Model Long-Distance Dependencies\n\n\n\nfastText’s reliance on local subword features means it struggles to capture long-distance dependencies between words in a sentence. For instance, in sentences where key information is spread out over several words (e.g., “The man, who was wearing a red jacket, crossed the street”), fastText cannot effectively model relationships between the subject and the predicate when they are far apart. Models like LSTMs or transformers are more suited for handling such dependencies.\n\n\n\nScalability and Resource Requirements\n\n\n\nWhile fastText is designed to be efficient, it still requires significant computational resources, especially when dealing with large corpora or many languages. Training models with large n-grams can increase both the memory and time required for training. In addition, the storage requirements for embeddings can grow substantially, particularly when generating embeddings for extensive vocabularies with numerous subwords.\n\n\n\nLack of Language-Specific Optimizations\n\n\n\nAlthough fastText is well-suited for morphologically rich languages, it lacks the language-specific optimizations that some newer NLP models (like multilingual BERT) offer. fastText treats all languages uniformly, which can be a limitation for languages with unique syntactic or semantic characteristics that require specialized treatment. For example, languages with complex agreement systems or non-concatenative morphology might benefit from more tailored approaches than fastText provides.\n\n\n\nLimited Performance in Highly Context-Dependent Tasks\n\n\n\nfastText performs well in tasks where morphology and subword information play a key role, such as text classification or simple sentiment analysis. However, for highly context-dependent tasks such as machine translation, nuanced sentiment detection, or question-answering systems, fastText may not provide enough context sensitivity. More sophisticated models like transformers, which are designed to capture nuanced semantic and syntactic relationships, generally perform better in such scenarios.\n\n\n\nBERT Embeddings\n\n\n\n\n  \nFor more details about BERT embeddings, please refer the \nBERT\n primer.\n\n\n\n\n\nHandling Polysemous Words – Key Limitation of BoW, TF-IDF, BM25, Word2Vec, GloVe, and fastText\n\n\n\n\n  \nBoW, TF-IDF, BM25, Word2Vec, GloVe, and fastText each have distinct ways of representing words and their meanings. However, all of these methods generate a single embedding per word, leading to a blended representation of different senses for polysemous words. This approach averages the contexts, which can dilute the specific meanings of polysemous words. Put simply, a major challenge across several of these methods is their inability to handle polysemous words (words with multiple meanings) effectively, often resulting in a single representation that blends different senses of the word. While later methods such as fastText provide some improvements by leveraging subword information, none fully resolves the issue of distinguishing between different senses of a word based on its context.\n\n  \nBERT, on the other hand, overcomes this limitation by generating contextualized embeddings that adapt to the specific meaning of a word based on its surrounding context. This allows BERT to differentiate between multiple senses of a polysemous word, providing a more accurate representation.\n\n  \nBelow is a detailed examination of how each method deals with polysemy.\n\n\n\n\n\nBag of Words (BoW)\n\n\n\n\n  \nDescription:\n\n    \n\n      \nBoW is a simple method that represents text as a collection of words without considering grammar or word order. It counts the frequency of each word in a document.\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nWord Frequency:\n\n        \n\n          \nBoW does not create embeddings; instead, it treats each word as an individual token. Therefore, it cannot distinguish between different meanings of a word in different contexts.\n\n        \n\n      \n\n      \nContext Insensitivity:\n\n        \n\n          \nThe method cannot differentiate between polysemous meanings, as each occurrence of a word contributes equally to its frequency count, regardless of its meaning in context.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nSince BoW lacks context sensitivity, polysemous words are treated as if they have only one meaning, which limits its effectiveness in capturing semantic nuances.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nTF-IDF (Term Frequency-Inverse Document Frequency)\n\n\n\n\n  \nDescription:\n\n    \n\n      \nTF-IDF refines BoW by considering how important a word is in a document relative to the entire corpus. It assigns higher weights to words that appear frequently in a document but less often in the corpus.\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nTerm Weighting:\n\n        \n\n          \nTF-IDF improves over BoW by emphasizing less common but important words. However, it still treats each word as a unique token without considering its multiple meanings in different contexts.\n\n        \n\n      \n\n      \nContext-Agnostic:\n\n        \n\n          \nLike BoW, TF-IDF does not distinguish between the different senses of polysemous words, as it focuses on term frequency without leveraging context.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nWhile TF-IDF addresses term relevance, it remains unable to handle polysemous words accurately due to its single-representation approach.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nBM25\n\n\n\n\n  \nDescription:\n\n    \n\n      \nBM25 is an extension of TF-IDF, often used in information retrieval, which ranks documents based on the frequency of query terms but also considers document length and term saturation.\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nRank-based Approach:\n\n        \n\n          \nBM25 assigns relevance scores to documents based on keyword matches, but like BoW and TF-IDF, it does not account for polysemy since it treats each occurrence of a word the same way.\n\n        \n\n      \n\n      \nContext-Agnostic:\n\n        \n\n          \nWhile BM25 improves retrieval effectiveness through sophisticated term weighting, it still represents polysemous words as a single entity.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nBM25 struggles with polysemy as it relies on exact word matches rather than distinguishing between different meanings of a word in different contexts.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nWord2Vec\n\n\n\n\n  \nDescription:\n\n    \n\n      \nWord2Vec includes two model architectures: Continuous Bag of Words (CBOW) and Skip-gram. Both learn word embeddings by predicting target words from context words (CBOW) or context words from a target word (Skip-gram).\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nSingle Vector Representation:\n\n        \n\n          \nWord2Vec generates a single embedding for each word in the vocabulary, regardless of its context. This means that all senses of a polysemous word are represented by the same vector.\n\n        \n\n      \n\n      \nContext Averaging:\n\n        \n\n          \nThe embedding of a polysemous word is an average representation of all the contexts in which the word appears. For example, the word “bank” will have a single vector that averages contexts from both financial institutions and river banks.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nThis single-vector approach fails to capture distinct meanings accurately, leading to less precise embeddings for polysemous words.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nGloVe\n\n\n\n\n  \nDescription:\n\n    \n\n      \nGloVe is a count-based model that constructs word embeddings using global word-word co-occurrence statistics from a corpus. It learns embeddings by factorizing the co-occurrence matrix.\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nSingle Vector Representation:\n\n        \n\n          \nLike Word2Vec, GloVe assigns a single embedding to each word in the vocabulary.\n\n        \n\n      \n\n      \nGlobal Context:\n\n        \n\n          \nThe embedding captures the word’s overall statistical context within the corpus. Thus, the different senses of polysemous words are combined into one vector.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nSimilar to Word2Vec, this blending of senses can dilute the quality of embeddings for polysemous words.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nfastText\n\n\n\n\n  \nDescription:\n\n    \n\n      \nfastText, developed by Facebook, extends Word2Vec by incorporating subword information. It represents words as bags of character n-grams, which allows it to generate embeddings for words based on their subword units.\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nSingle Vector Representation:\n\n        \n\n          \nAlthough fastText incorporates subword information and can better handle rare words and morphologically rich languages, it still produces a single vector for each word.\n\n        \n\n      \n\n      \nSubword Information:\n\n        \n\n          \nThe inclusion of character n-grams can capture some nuances of polysemy, especially when different meanings have distinct morphological patterns. However, this is not a complete solution for polysemy.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nWhile slightly better at representing polysemous words than Word2Vec and GloVe due to subword information, fastText still merges multiple senses into a single embedding.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nBERT\n\n\n\n\n  \nDescription:\n\n    \n\n      \nBERT is a transformer-based model that generates contextual embeddings by considering both the left and right context of a word in a sentence. Unlike Word2Vec and GloVe, BERT produces different embeddings for the same word depending on the surrounding context.\n\n    \n\n  \n\n  \nHandling Polysemy:\n\n    \n\n      \nContextualized Embeddings:\n\n        \n\n          \nBERT addresses the limitations of previous models by creating unique embeddings for polysemous words based on their specific usage within a sentence. For example, the word “bank” in the sentence “I went to the river bank” will have a different embedding than “I deposited money at the bank.”\n\n        \n\n      \n\n      \nDynamic Representation:\n\n        \n\n          \nBERT captures the different meanings of polysemous words by analyzing the entire sentence, thereby generating representations that are highly sensitive to context.\n\n        \n\n      \n\n      \nAdvancements Over Single-Vectors:\n\n        \n\n          \nUnlike Word2Vec, GloVe, or fastText, BERT is not constrained to a single-vector representation for polysemous words. It dynamically adapts to the specific sense of a word in each context, offering a significant improvement in handling polysemy.\n\n        \n\n      \n\n      \nLimitations:\n\n        \n\n          \nAlthough BERT excels in handling polysemy, its computational complexity is higher, requiring more resources for both training and inference. Additionally, it requires large amounts of data to fine-tune effectively for domain-specific applications.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nExample: BoW, TF-IDF, BM25, Word2Vec, GloVe, fastText, and BERT Embeddings\n\n\n\n\n  \nLet’s expand on the example involving the word “cat” to illustrate how different embedding techniques (BoW, TF-IDF, BM25, Word2Vec, GloVe, fastText, and BERT) might represent it. We’ll consider the same documents as before:\n    \n\n      \nDocument 1: “Cat sat on the mat.”\n\n      \nDocument 2: “Dog sat on the log.”\n\n      \nDocument 3: “Cat chased the dog.”\n\n    \n\n  \n\n\n\n\n\nBag of Words (BoW) Representation for “Cat”\n\n\n\n\n  \nBag of Words is one of the simplest forms of word representation. In this method, each document is represented as a vector of word counts. The position of each word in the vector corresponds to the presence or absence (or count) of the word in the document, regardless of the word order.\n\n  \nFor example, consider a vocabulary consisting of the words {cat, sat, on, the, mat, dog, log, chased}. The BoW vectors for each document would be:\n    \n\n      \nDocument 1: \n[1, 1, 1, 1, 1, 0, 0, 0]\n (because the words “cat”, “sat”, “on”, “the”, and “mat” each appear once).\n\n      \nDocument 2: \n[0, 1, 1, 1, 0, 1, 1, 0]\n (because “dog”, “sat”, “on”, “the”, and “log” appear).\n\n      \nDocument 3: \n[1, 0, 0, 1, 0, 1, 0, 1]\n (because “cat”, “the”, “dog”, and “chased” appear).\n\n    \n\n  \n\n  \nBoW Representation for “Cat”: \n[1, 0, 1]\n (the word “cat” appears once in Document 1 and once in Document 3, but not in Document 2).\n\n\n\n\n\nTF-IDF Embedding for “Cat”\n\n\n\n\n  \nIn TF-IDF, each word in a document is assigned a weight. This weight increases with the number of times the word appears in the document but is offset by the frequency of the word in the corpus.\n\n  \nTF-IDF assigns a weight to a word in each document, reflecting its importance. The steps are:\n    \n\n      \nCalculate Term Frequency (TF): Count of “cat” in each document divided by the total number of words in that document.\n\n      \nCalculate Inverse Document Frequency (IDF): Logarithm of the total number of documents divided by the number of documents containing “cat”.\n\n      \nMultiply TF by IDF for each document.\n\n    \n\n  \n\n  \nFor instance, the TF-IDF weight for the word “cat” in Document 1 would be calculated as follows (simplified calculation):\n    \n\n      \nTerm Frequency (TF) of “cat” in Document 1 = 1/5 (it appears once out of five words).\n\n      \nInverse Document Frequency (IDF) of “cat” = log(3/2) (it appears in 2 out of 3 documents, and we use the logarithm to dampen the effect).\n\n      \nTF-IDF for “cat” in Document 1 = TF * IDF = (1/5) * log(3/2).\n\n    \n\n  \n\n  \nFinal TF-IDF Embedding for “Cat”: \n[0.18, 0, 0.18]\n (assuming normalized values for simplicity).\n\n\n\n\n\nBM25 Embedding for “Cat”\n\n\n\n\n  \nBM25 builds on top of TF-IDF and thus is more complex than TF-IDF. It considers term frequency, document frequency, document length, and two parameters: k1 and b. The final BM25 score for “cat” in each document might look like this (assuming certain values for \\(k1\\) and \\(b\\)):\n\n  \nFinal BM25 Score for “Cat”: \n[2.5, 0, 2.3]\n (hypothetical values).\n\n\n\n\n\nWord2Vec Embedding for “Cat”\n\n\n\n\n  \nWord2Vec provides a dense vector for each word. This vector is learned based on the context in which the word appears across the entire corpus, not just our three documents as in the example above.\n\n  \nThe model might represent the word “cat” as a vector, such as \n[0.76, -0.21, 0.58, ...]\n (assuming a 3-dimensional space for simplicity, but in reality, these vectors often have hundreds of dimensions).\n\n\n\n\n\nGloVe Embedding for “Cat”\n\n\n\n\n  \nGloVe, like Word2Vec, provides a dense vector for each word based on the aggregate global word-word co-occurrence statistics from a corpus.\n\n  \nHypothetical GloVe Embedding for “Cat”: In a 3-dimensional space, \n[0.81, -0.45, 0.30]\n. As with Word2Vec, real-world GloVe embeddings would have a much higher dimensionality.\n\n\n\n\n\n\n  \nIn these examples, it’s important to note that the BoW, TF-IDF, and BM25 scores depend on the context of the specific documents, whereas the Word2Vec and GloVe embeddings are more general, trained on a larger corpus and representing the word’s meaning in a broader context. On the flip side, Word2Vec, GloVe, and fastText embeddings, lack contextualized representations (so they cannot represent polysemous works effectively), however, models such as ELMo and BERT overcome that limitation using contextualized embeddings. The specific values used here for TF-IDF, BM25, Word2Vec, and GloVe are illustrative and would vary based on the actual computation and dimensions used.\n\n\n\n\n\nfastText Embedding for “Cat”\n\n\n\n\n  \nfastText, like Word2Vec and GloVe, is a method for learning word embeddings, but it differs in its treatment of words. fastText treats each word as a bag of character n-grams, which allows it to better represent rare words or words not seen during training by breaking them down into smaller units.\n\n  \nHypothetical fastText Embedding for “Cat”: Assuming a 3-dimensional space, \n[0.72, -0.25, 0.63]\n. Like the others, real fastText embeddings typically have a much higher dimensionality.\n\n  \nIn this expanded example, the key addition of fastText is its ability to handle out-of-vocabulary words by breaking them down into n-grams, offering a more flexible representation, especially for languages with rich morphology or a lot of word forms. The specific values for fastText, like the others, are illustrative and depend on the actual corpus and training setup.\n\n\n\n\n\nBERT Embedding for “Cat”\n\n\n\n\n  \n\n    \nBERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that generates contextualized word embeddings, meaning the representation of a word depends on the surrounding words in the sentence. Unlike static embeddings (Word2Vec, GloVe, fastText), BERT captures the various meanings (polysemy) of a word based on its context. This makes BERT powerful for handling ambiguous or polysemous words like “cat,” whose meaning might change depending on how it’s used in a sentence.\n\n  \n\n  \nFor example, BERT would generate different embeddings for “cat” in the following contexts:\n    \n\n      \nDocument 1: “Cat sat on the mat.”\n\n      \nDocument 3: “Cat chased the dog.”\n\n    \n\n  \n\n  \n\n    \nHere, the word “cat” in Document 1 might be represented as a vector like \n[0.65, -0.34, 0.77, ...]\n, indicating a relaxed or neutral context, while in Document 3, where “cat” is involved in an action (“chased”), it might generate a different embedding like \n[0.78, -0.10, 0.89, ...]\n.\n\n  \n\n  \n\n    \nUnlike traditional word embeddings, BERT’s ability to incorporate both the left and right context enables a nuanced understanding of each occurrence of “cat.” These vectors would be different not only based on the sentence but also based on the larger document context in which the word appears.\n\n  \n\n  \nBERT Embedding for “Cat”: Instead of a static embedding like \n[0.76, -0.21, 0.58]\n (as in Word2Vec or GloVe), BERT might output \n[0.65, -0.34, 0.77]\n in one sentence and a different vector \n[0.78, -0.10, 0.89]\n for “cat” in another, demonstrating its strength in understanding word meaning based on context.\n\n\n\n\n\n\n  \nBERT embeddings are useful in tasks like question answering, text classification, and named entity recognition, where understanding the specific meaning of a word in its context is critical. By leveraging bidirectional attention, BERT improves significantly over previous models that treat words in isolation or with limited context.\n\n\n\n\n\nSummary: Types of Embeddings\n\n\n\n\n  \nIn the field of NLP, a variety of embedding techniques have been developed, each suited to specific applications and use cases. This article categorizes and delves into different types of word embeddings and their functionalities.\n\n\n\n\n\nBag-of-Words-based Embeddings\n\n\n\n\n  \nThese embeddings do not consider the order of words.\n    \n\n      \nBag of Words (BoW)\n: The simplest text representation method, BoW is a count-based approach that tallies the occurrences of each word in a document. However, it disregards any information about the order or structure of words, treating the text as a mere “bag” of words. It focuses only on the presence or absence of words, not their positioning within the document.\n\n      \nTF-IDF (Term Frequency-Inverse Document Frequency)\n: An advanced version of count vectors, TF-IDF considers the frequency of words in a document as well as their overall frequency in the corpus. Common words like “the” have lower TF-IDF scores, while unique or rare words have higher scores, reflecting their relative importance.\n\n    \n\n  \n\n\n\n\n\nPredictive Word Embeddings\n\n\n\n\n  \nThese models predict words based on their context.\n    \n\n      \nWord2Vec\n: A neural network-based model that learns to represent words as vectors in a high-dimensional space. Words with similar meanings are represented by proximate vectors. Word2Vec facilitates capturing meanings, semantic similarities, and relationships within text, exemplified by analogies like \nking - man + woman = queen\n.\n\n    \n\n  \n\n\n\n\n\nContextual and Sequential Data Embeddings\n\n\n\n\n  \nRepresenting order and context of words, and suited for sequential data like text.\n    \n\n      \nRecurrent Neural Networks (RNNs)\n: RNNs, and their advanced variants like LSTMs (Long Short-Term Memory), are adept at handling sequential data. They process inputs in a sequence, with each step’s output feeding into the next, capturing information from previous steps.\n\n      \nTransformer\n: A model that revolutionized NLP with its encoder-decoder architecture, leveraging self-attention mechanisms. Transformers excel in learning long-range dependencies, allowing them to focus on specific parts of the input sequence and better understand sentence meanings.\n\n    \n\n  \n\n\n\n\n\nContextual Embeddings\n\n\n\n\n  \n\n    \nThese consider the order and context of words.\n\n\n    \n\n      \nELMo (Embeddings from Language Models)\n: Generates contextual embeddings from the internal states of a bi-directional LSTM.\n\n      \nBERT (Bidirectional Encoder Representations from Transformers) Embeddings\n: Provides contextual embeddings based on the entire context of word usage.\n\n    \n\n  \n\n\n\n\n\nSentence/Document Embeddings\n\n\n\n\n  \n\n    \nFor broader textual units like sentences or documents.\n\n\n    \n\n      \nDoc2Vec\n: Extends Word2Vec to represent entire documents.\n\n      \nSentence-BERT\n: Adapts BERT for sentence-level embeddings.\n\n      \nUniversal Sentence Encoder\n: Encodes sentences into vectors for various tasks.\n\n    \n\n  \n\n\n\n\n\nPositional Embeddings\n\n\n\n\n  \n\n    \nEncodes the position of words within sequences.\n\n\n    \n\n      \nAbsolute Positional Embeddings\n: Used in Transformers to encode the absolute position of words.\n\n      \nRelative Positional Embeddings\n: Focuses on relative distances between words, beneficial in models like Transformer-XL and T5.\n\n      \nRotary Positional Embeddings/RoPE (Rotary Positional Encoding)\n: Employs rotational operations to encode relative positions.\n\n    \n\n  \n\n\n\n\n\nRelative Embeddings\n\n\n\n\n  \n\n    \nCapture relative positions between word pairs in sequences.\n\n\n    \n\n      \nRelative Positional Embeddings\n: Encodes the relative positioning of words, like in the sentence “Alice threw the ball to Bob,” where “ball” has a relative position to other words. In Transformer models, the difference between positions \\(i\\) and \\(j\\) in the input sequence is used to retrieve corresponding embedding vectors, enhancing the model’s ability to generalize to new sequence lengths.\n\n    \n\n  \n\n  \n\n    \nThis categorization of embedding techniques underscores the diversity and evolution of approaches in representing linguistic elements in NLP, each with distinct advantages and suited for specific applications.\n\n  \n\n\n\n\n\nComparative Analysis of BoW, TF-IDF, BM25, Word2Vec, GloVe, fastText, and BERT Embeddings\n\n\n\n\n  \nEach of these techniques represents a different approach to encoding text data for tasks like search, document classification, sentiment analysis, and more. Each method has its strengths and weaknesses, making them suitable for different tasks based on the trade-offs of simplicity vs. semantic richness and computational efficiency. Let’s break them down one by one, comparing their approaches, advantages, disadvantages, and use cases.\n\n\n\n\n\nBag of Words (BoW)\n\n\n\n\n  \nDescription:\n\n    \n\n      \nBoW is the simplest form of text representation where a document is represented as a vector of word counts, without any regard to word order or meaning.\n\n      \nIt creates a vocabulary of unique words from the corpus and then encodes each document based on the frequency of these words.\n\n    \n\n  \n\n  \nHow it works:\n\n    \n\n      \nCreate a vocabulary of all unique words.\n\n      \nFor each document, count the occurrence of each word in the vocabulary and represent the document as a vector of these counts.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nSimple and easy to implement.\n\n      \nWorks well when word order and semantics are not important.\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nIgnores semantics and word order (no understanding of the meaning or relationships between words).\n\n      \nHigh dimensionality (especially with large vocabularies).\n\n      \nSparsity of vectors (most values are zero since many words will not appear in every document).\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nText classification, simple document similarity tasks, and basic Natural Language Processing (NLP) problems.\n\n    \n\n  \n\n\n\n\n\nTerm Frequency-Inverse Document Frequency (TF-IDF)\n\n\n\n\n  \nDescription:\n\n    \n\n      \nTF-IDF improves upon BoW by weighting words based on their frequency in a document (Term Frequency, TF) and the rarity of the word across the corpus (Inverse Document Frequency, IDF). Words that are common in many documents receive lower weights.\n\n    \n\n  \n\n  \nHow it works:\n\n    \n\n      \nTF is the frequency of a term in a document.\n\n      \nIDF is calculated as the logarithm of the total number of documents divided by the number of documents that contain the term.\n\n      \nThe final TF-IDF score is the product of TF and IDF.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nReduces the importance of common words (like “the,” “and”) that provide little discriminative power.\n\n      \nMore informative than BoW by highlighting rare but significant words.\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nStill ignores the word order and contextual relationships.\n\n      \nSuffers from sparsity and high dimensionality like BoW.\n\n      \nDoes not capture deep semantics or relationships between words.\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nInformation retrieval, text ranking, document classification, and keyword extraction.\n\n    \n\n  \n\n\n\n\n\nBM25 (Best Matching 25)\n\n\n\n\n  \nDescription:\n\n    \n\n      \nBM25 is a ranking function used by search engines, building on the TF-IDF concept but incorporating additional factors like term saturation and document length normalization. It ranks documents based on how relevant they are to a given query.\n\n    \n\n  \n\n  \nHow it works:\n\n    \n\n      \nSimilar to TF-IDF, but uses a non-linear saturation function for term frequency and adjusts based on document length. It considers term frequency more naturally, as the contribution of a term to the score increases but saturates after a certain point.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nA more sophisticated version of TF-IDF that captures term importance more naturally.\n\n      \nHandles variations in document length and term frequency saturation better than TF-IDF.\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nStill based on a bag-of-words model, so it does not capture word semantics or order.\n\n      \nLimited to ranking tasks and cannot be directly used for other tasks like word embeddings.\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nSearch engines, information retrieval, query-document ranking.\n\n    \n\n  \n\n\n\n\n\nWord2Vec\n\n\n\nDescription:\n\n\n\n  \nWord2Vec is a neural network-based model that creates dense word embeddings. It maps words into a continuous vector space such that words with similar meanings are close to each other in the vector space.\n\n  \n\n    \nIt comes in two flavors: \nCBOW (Continuous Bag of Words)\n and \nSkip-gram\n.\n\n  \n\n  \nHow it works:\n\n    \n\n      \nCBOW predicts a word given its surrounding context.\n\n      \nSkip-gram predicts the surrounding context given a word.\n\n      \nBoth models learn vector representations for words based on co-occurrence patterns in large corpora.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nCaptures semantic relationships between words (e.g., “king” - “man” + “woman” ≈ “queen”).\n\n      \nDense vectors with much lower dimensionality compared to BoW or TF-IDF.\n\n      \nCaptures analogy relations (e.g., “Paris is to France as Berlin is to Germany”).\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nContext-independent: the same word always has the same vector regardless of its meaning in different contexts.\n\n      \nRequires large amounts of data to train good embeddings.\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nSemantic similarity, analogy reasoning, clustering, and as input for downstream NLP models.\n\n    \n\n  \n\n\n\n\n\nGloVe (Global Vectors for Word Representation)\n\n\n\n\n  \nDescription:\n\n    \n\n      \nGloVe is also a word embedding model, but it is based on matrix factorization of the co-occurrence matrix of words in a corpus. It attempts to capture both local context and global word statistics.\n\n    \n\n  \n\n  \nHow it works:\n\n    \n\n      \nFirst, a word co-occurrence matrix is built, which records how often words appear together in a corpus.\n\n      \nThen, this matrix is factorized to obtain word embeddings. The objective is to predict the ratio of probabilities of word co-occurrence.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nCombines the benefits of co-occurrence statistics (global context) and local context-based methods like Word2Vec.\n\n      \nDense embeddings, capturing semantic relationships.\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nLike Word2Vec, it generates context-independent embeddings.\n\n      \nRequires large corpora to learn good-quality embeddings.\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nSimilar to Word2Vec: used in semantic similarity tasks, text classification, and input embeddings for other NLP models.\n\n    \n\n  \n\n\n\n\n\nfastText\n\n\n\n\n  \nDescription:\n\n    \n\n      \nfastText\n is an extension of the Word2Vec model, developed by Facebook AI Research, that incorporates subword information into word embeddings. This allows fastText to create better representations of rare words and handle words with typos or variations. It represents each word as a bag of character n-grams, allowing it to capture morphology and word structure.\n\n    \n\n  \n\n  \nHow it works:\n\n    \n\n      \nfastText breaks down words into character-level n-grams and learns vector representations for both the words and their n-grams. The final word vector is the sum of its constituent n-grams. This approach captures word morphology and helps in generating meaningful embeddings even for unseen words or rare words.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nHandles rare words, typos, and out-of-vocabulary (OOV) words by leveraging subword information.\n\n      \nDense and low-dimensional embeddings, similar to Word2Vec, but with more robustness for morphologically rich languages.\n\n      \nFaster to train and can generate embeddings on-the-fly for unseen words.\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nLike Word2Vec and GloVe, fastText produces context-independent embeddings, meaning the same word has the same vector regardless of the sentence it appears in.\n\n      \nIt still operates primarily at the word level, so while it captures subword information, it does not account for the full sentence context like BERT.\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nUseful in tasks involving rare or morphologically complex words, such as text classification, named entity recognition, and search in languages with rich word forms or frequent misspellings. It is also used for tasks like language modeling, sentence classification, and similarity detection.\n\n    \n\n  \n\n\n\n\n\nBERT (Bidirectional Encoder Representations from Transformers)\n\n\n\nDescription:\n\n\n\n  \n\n    \nBERT is a transformer-based deep learning model that creates contextualized embeddings for words. It differs from Word2Vec and GloVe by considering the full context of a word (both left and right) rather than just local context.\n\n  \n\n  \nHow it works:\n\n    \n\n      \nUses a transformer architecture to learn embeddings from large-scale unsupervised text data (such as Wikipedia).\n\n      \nBERT is pre-trained on two tasks: \nMasked Language Modeling\n (MLM) and \nNext Sentence Prediction\n (NSP).\n\n      \nMLM trains BERT to predict missing words in a sentence, while NSP helps it understand sentence relationships.\n\n    \n\n  \n\n  \nAdvantages:\n\n    \n\n      \nContextual embeddings: the representation of a word changes depending on the context in which it appears.\n\n      \nHandles polysemy effectively (e.g., different meanings of the word “bank” are represented differently based on the sentence).\n\n      \nCaptures deep semantic and syntactic relationships.\n\n    \n\n  \n\n  \nDisadvantages:\n\n    \n\n      \nComputationally expensive and requires significant resources for training and inference.\n\n      \nLarge model size, making it harder to deploy in resource-constrained environments.\n\n    \n\n  \n\n  \nUse Cases:\n\n    \n\n      \nQuestion answering, text classification, named entity recognition, and other advanced NLP tasks.\n\n    \n\n  \n\n\n\n\n\nComparative Summary\n\n\n\nCount-Based Techniques (TF-IDF and BM25)\n\n\n\nPros\n\n\n\n\n  \nSimplicity and Efficiency\n: Easy to implement and computationally efficient, suitable for basic information retrieval tasks.\n\n  \nEffectiveness in Document Retrieval\n: Particularly good at identifying documents relevant to specific terms, thanks to their focus on term frequency.\n\n\n\n\n\nCons\n\n\n\n\n  \nLack of Semantic Understanding\n: They don’t capture deeper semantic relationships between words, leading to limited contextual interpretation.\n\n  \nSparse Representations\n: Can result in high-dimensional and sparse vectors, which are less efficient for complex NLP tasks.\n\n\n\n\n\nCo-occurrence Based/Static Embedding Techniques (Word2Vec, GloVe, fastText)\n\n\n\nPros\n\n\n\n\n  \nSemantic Relationship Modeling\n: Capable of capturing complex semantic relationships between words, offering richer representations.\n\n  \nSubword Information (fastText)\n: fastText’s consideration of subword elements aids in understanding morphology and handling out-of-vocabulary words.\n\n\n\n\n\nCons\n\n\n\n\n  \nFixed Context\n: Static embeddings assign a single, context-independent representation to each word, limiting their effectiveness in contextually varied scenarios.\n\n  \nComputational Intensity\n: Requires significant computational resources for training on large corpora.\n\n\n\n\n\nContextualized Representation Techniques (BERT, ELMo)\n\n\n\nPros\n\n\n\n\n  \nContext-Sensitive\n: They provide dynamic word representations based on context, leading to a more nuanced understanding of language.\n\n  \nState-of-the-Art Performance\n: Excel in a wide range of NLP tasks, offering superior performance compared to previous models.\n\n\n\n\n\nCons\n\n\n\n\n  \nComputational Requirements\n: Demand extensive computational power and larger datasets for training.\n\n  \nComplexity in Implementation\n: More complex to implement and integrate into applications compared to simpler models like TF-IDF.\n\n\n\n\n\n\n  \n\n    \n\n      \n\n        \nMethod\n\n        \nType\n\n        \nCaptures Semantics?\n\n        \nDimensionality\n\n        \nContextual?\n\n        \nComputational Complexity\n\n        \nPros\n\n        \nCons\n\n      \n\n    \n\n    \n\n      \n\n        \nBoW\n\n        \nFrequency-based\n\n        \nNo\n\n        \nHigh\n\n        \nNo\n\n        \nLow\n\n        \n- Simple and easy to implement. \n- Fast for small datasets.\n\n        \n- Ignores word order and semantics.\n- Very high-dimensional and sparse.\n\n      \n\n      \n\n        \nTF-IDF\n\n        \nFrequency-based\n\n        \nLimited\n\n        \nHigh\n\n        \nNo\n\n        \nLow\n\n        \n- Reduces impact of common words.\n- Simple and interpretable.\n\n        \n- Still high-dimensional and sparse.\n- No deep semantic understanding.\n\n      \n\n      \n\n        \nBM25\n\n        \nFrequency-based\n\n        \nLimited\n\n        \nHigh\n\n        \nNo\n\n        \nModerate\n\n        \n- More effective ranking than TF-IDF.\n- Considers document length and term saturation.\n\n        \n- Complex for non-ranking tasks.\n- Still a bag-of-words model with no semantics.\n\n      \n\n      \n\n        \nWord2Vec\n\n        \nEmbedding-based\n\n        \nYes (basic)\n\n        \nLow\n\n        \nNo\n\n        \nModerate\n\n        \n- Dense, low-dimensional vectors.\n- Captures basic semantic relationships.\n\n        \n- Context-independent embeddings.\n- Requires large corpora for good results.\n\n      \n\n      \n\n        \nGloVe\n\n        \nEmbedding-based\n\n        \nYes (basic)\n\n        \nLow\n\n        \nNo\n\n        \nModerate\n\n        \n- Combines local and global context.\n- Dense vectors and efficient embeddings.\n\n        \n- Context-independent like Word2Vec.\n- Requires pre-training on large datasets.\n\n      \n\n      \n\n        \nfastText\n\n        \nEmbedding-based\n\n        \nYes (basic)\n\n        \nLow\n\n        \nNo\n\n        \nModerate\n\n        \n- Captures subword information.\n- Works well with rare words or misspellings.\n\n        \n- Context-independent embeddings.\n- Limited to word-level representations.\n\n      \n\n      \n\n        \nBERT\n\n        \nEmbedding-based\n\n        \nYes (rich)\n\n        \nLow\n\n        \nYes\n\n        \nHigh\n\n        \n- Contextualized embeddings capture deep meaning.\n- Handles polysemy effectively.\n\n        \n- Computationally expensive and slow.\n- Large model size, hard to deploy on scale.\n\n      \n\n    \n\n  \n\n\n\n\n\nKey Takeaways\n\n\n\n\n  \nBoW and TF-IDF are simple, interpretable, and fast but fail to capture meaning and relationships between words.\n\n  \nBM25 refines TF-IDF for better ranking performance but shares similar limitations.\n\n  \nWord2Vec and GloVe generate dense embeddings that capture semantic relationships but are context-independent.\n\n  \nfastText builds on Word2Vec by incorporating subword information, making it more robust to rare words, misspellings, and morphologically rich languages, though it remains context-independent.\n\n  \nBERT is a state-of-the-art model that generates contextualized embeddings, making it the most powerful for complex NLP tasks but at the cost of high computational resources.\n\n\n\n\n\nFAQs\n\n\n\nWhat does the “Continuous” in Word2Vec’s Continuous Bag of Words and Continuous Skipgram refer to?\n\n\n\n\n  \nThe term “continuous” in Word2Vec’s Continuous Bag of Words (CBOW) and Continuous Skipgram models refers to the continuous and distributed representation of words in the vector space. This is in contrast to traditional bag-of-words models, which represent words using discrete and sparse vectors.\n\n\n\n\n\nTraditional Bag of Words\n\n\n\n\n  \nDiscrete Representation:\n\n    \n\n      \nIn a traditional Bag of Words model, each word is represented as a unique index in a vocabulary, creating a sparse and high-dimensional vector. For example, in a vocabulary of 10,000 words, “cat” might be represented as a vector with a 1 in the position corresponding to “cat” and 0s elsewhere.\n\n    \n\n  \n\n  \nSparse Vectors:\n\n    \n\n      \nThese vectors are sparse because most elements are zero. Each word vector is orthogonal to every other word vector, meaning there is no inherent similarity between words represented in this way.\n\n    \n\n  \n\n  \nNo Context:\n\n    \n\n      \nBoW models do not capture the context in which words appear. They only consider word frequencies within documents, ignoring word order and contextual relationships.\n\n    \n\n  \n\n\n\n\n\nContinuous Bag of Words\n\n\n\n\n  \nContinuous and Distributed Representation:\n\n    \n\n      \nThe “continuous” in CBOW refers to the use of continuous and dense vectors to represent words. Instead of sparse vectors, each word is mapped to a dense vector of real numbers. These vectors are typically of much lower dimensionality (e.g., 100 or 300 dimensions) and are learned through training on a large corpus.\n\n    \n\n  \n\n  \nContextual Embeddings:\n\n    \n\n      \nCBOW captures the context of a word by considering its surrounding words. Given a context (the words surrounding a target word), CBOW predicts the target word. For example, in the sentence “The cat sat on the mat,” the context for “sat” might be \n[\"The\", \"cat\", \"on\", \"the\", \"mat\"]\n.\n\n    \n\n  \n\n  \nTraining Process:\n\n    \n\n      \nThe model learns to maximize the probability of the target word given its context. This is done using a neural network that adjusts the word vectors to make similar words (words that appear in similar contexts) have similar vector representations.\n\n    \n\n  \n\n  \nDense Vectors:\n\n    \n\n      \nEach word is associated with a dense vector that captures various syntactic and semantic properties. These vectors are “continuous” in that they can take on any value in the real-number space, unlike the discrete indices used in traditional BoW models.\n\n    \n\n  \n\n  \nExample:\n\n    \n\n      \nSuppose “cat” is represented by a 100-dimensional vector like \n[0.25, -0.1, 0.75, ...]\n. This vector is learned from the contexts in which “cat” appears, and words that appear in similar contexts (like “dog”) will have similar vectors.\n\n    \n\n  \n\n\n\n\n\nContinuous Skipgram\n\n\n\n\n  \nContinuous and Distributed Representation:\n\n    \n\n      \nSimilar to CBOW, the “continuous” in the Continuous Skipgram model refers to the continuous and dense word vectors used to represent words. These vectors are also of lower dimensionality and are learned from large corpora.\n\n    \n\n  \n\n  \nReverse Prediction:\n\n    \n\n      \nUnlike CBOW, which predicts the target word from its context, Skipgram does the reverse: it predicts the context words given a target word. For example, in the sentence “The cat sat on the mat,” Skipgram would take “sat” as the input and try to predict words like “cat,” “on,” “the,” and “mat.”\n\n    \n\n  \n\n  \nTraining Process:\n\n    \n\n      \nThe Skipgram model learns to predict the surrounding words of a given target word by adjusting the word vectors during training. The objective is to maximize the probability of context words given the target word, ensuring that words appearing in similar contexts end up with similar vector representations.\n\n    \n\n  \n\n  \nDense Vectors:\n\n    \n\n      \nLike CBOW, Skipgram also uses dense vectors to represent words. These vectors are continuous and can capture intricate relationships between words based on their context in a corpus.\n\n    \n\n  \n\n  \nExample:\n\n    \n\n      \nIf “sat” is represented by a 100-dimensional vector \n[0.12, -0.3, 0.58, ...]\n, the Skipgram model would adjust this vector during training to maximize the likelihood of predicting context words like “cat” and “on” when “sat” is the input.\n\n    \n\n  \n\n\n\n\n\nKey Advantages of CBOW and Skipgram\n\n\n\n\n  \nCaptures Contextual Information:\n\n    \n\n      \nBoth models capture contextual relationships between words, leading to more meaningful word representations. CBOW does this by predicting the target word from its context, while Skipgram predicts the context from the target word.\n\n    \n\n  \n\n  \nDense and Low-Dimensional Vectors:\n\n    \n\n      \nThe use of dense, continuous vectors reduces the dimensionality of the word representation, making it computationally more efficient and enabling the model to generalize better.\n\n    \n\n  \n\n  \nSemantic Similarity:\n\n    \n\n      \nWords with similar meanings or that appear in similar contexts will have similar embeddings, allowing for better semantic understanding.\n\n    \n\n  \n\n  \nEfficient Training:\n\n    \n\n      \nCBOW is generally faster to train than Skipgram because it uses the entire context to predict the target word, while Skipgram can better capture rare word associations by focusing on predicting the context from the target word.\n\n    \n\n  \n\n\n\n\n\nSummary\n\n\n\n\n  \nThe “continuous” in both CBOW and Skipgram highlights the transition from discrete, sparse word representations to continuous, dense vector representations. This shift allows these models to capture contextual information and semantic relationships between words more effectively, leading to more powerful and meaningful word embeddings. CBOW excels in efficiency, while Skipgram often provides richer representations for rare words due to its reverse prediction approach.\n\n\n\n\n\nHow are Word2Vec, GloVe, and fastText Co-occurrence-based Embedding Techniques?\n\n\n\n\n  \nWord2Vec, GloVe, and FastText are all co-occurrence-based embedding techniques, but they differ in their approaches to leveraging co-occurrence information to learn word embeddings. Here’s a detailed explanation of each method and how they utilize co-occurrence information:\n\n\n\n\n\nWord2Vec\n\n\n\n\n  \nDescription:\n\n    \n\n      \nWord2Vec, developed by Google, includes two model architectures: Continuous Bag of Words (CBOW) and Skip-gram.\n\n    \n\n  \n\n  \nCo-occurrence Information:\n\n    \n\n      \nCBOW:\n Predicts a target word based on the context words (words surrounding the target word within a fixed window size). This approach implicitly leverages word co-occurrence within the context window to learn embeddings.\n\n      \nSkip-gram:\n Predicts context words given a target word. This method also relies on co-occurrence information within a fixed window around the target word.\n\n      \nTraining Objective:\n Both CBOW and Skip-gram use neural networks to optimize the embeddings so that words appearing in similar contexts have similar vectors.\n\n    \n\n  \n\n  \nThe models learn embeddings by maximizing the probability of predicting context words given a target word (Skip-gram) or predicting a target word given context words (CBOW).\n\n\n\n\n\nGloVe\n\n\n\n\n  \nDescription:\n\n    \n\n      \nGloVe, developed by researchers at Stanford, is explicitly designed to capture global statistical information from a corpus by factorizing the word-word co-occurrence matrix.\n\n    \n\n  \n\n  \nCo-occurrence Information:\n\n    \n\n      \nCo-occurrence Matrix:\n GloVe constructs a large sparse matrix where each cell represents the co-occurrence frequency of a pair of words within a specific context window.\n\n      \nObjective Function:\n GloVe’s training objective is to factorize this co-occurrence matrix to produce word vectors. It aims to ensure that the dot product of word vectors approximates the logarithm of the words’ co-occurrence probabilities.\n\n      \nGlobal Context:\n Unlike Word2Vec, which focuses on local context within a sliding window, GloVe captures global co-occurrence statistics across the entire corpus.\n\n    \n\n  \n\n\n\n\n\nFastText\n\n\n\n\n  \nDescription:\n\n    \n\n      \nFastText, developed by Facebook, extends Word2Vec by incorporating subword information, representing words as bags of character n-grams.\n\n    \n\n  \n\n  \nCo-occurrence Information:\n\n    \n\n      \nSubword Level Co-occurrence:\n FastText builds on the Skip-gram model of Word2Vec but adds a layer of granularity by considering subwords (character n-grams). This means that it leverages co-occurrence information at both the word level and the subword level.\n\n      \nTraining Objective:\n Similar to Skip-gram, FastText predicts context words from a target word, but it enriches the embeddings with subword information, allowing it to better handle rare and morphologically rich words.\n\n      \nEnhanced Co-occurrence Handling:\n By incorporating subword information, FastText captures more detailed co-occurrence patterns, especially beneficial for languages with rich morphology or for handling out-of-vocabulary words.\n\n    \n\n  \n\n\n\n\n\nSummary of Co-occurrence Based Techniques\n\n\n\n\n  \nWord2Vec:\n Uses local co-occurrence information within a context window around each word. It learns embeddings by optimizing the prediction of target-context word pairs through neural networks (CBOW and Skip-gram models).\n\n  \nGloVe:\n Utilizes global co-occurrence statistics from the entire corpus by factorizing a co-occurrence matrix. It explicitly captures how frequently words co-occur across the corpus, aiming to directly model the co-occurrence probabilities.\n\n  \n\n    \nFastText:\n Extends the Skip-gram model to include subword information, leveraging both word-level and subword-level co-occurrence information. This approach helps to capture more fine-grained co-occurrence patterns and improves handling of rare or complex words.\n\n  \n\n  \nEach of these methods leverages co-occurrence information to learn word embeddings, but they do so in different ways and with varying levels of granularity, ranging from local context windows (Word2Vec) to global co-occurrence matrices (GloVe) and subword-level details (FastText).\n\n\n\n\n\nDoes Word2Vec use word-level or sub-word-level tokenization?\n\n\n\n\n  \nWord2Vec uses word-level tokenization, meaning that it treats each word as a distinct unit or token, while other models like FastText use subword tokenization to overcome some of the limitations of Word2Vec. The Word2Vec model is designed to create embeddings for individual words based on their co-occurrence with other words in a large corpus. It does not natively break words down into smaller units, such as subwords, prefixes, or suffixes.\n\n\n\n\n\nKey Points\n\n\n\n\n  \nWord-level tokenization\n: In Word2Vec, each word is treated as an atomic entity, and embeddings are learned for each unique word in the vocabulary. This means words like “run” and “running” would have separate embeddings, without any explicit shared representation of the “run” root.\n\n  \nVocabulary limitations\n: One drawback of word-level tokenization in Word2Vec is that it doesn’t handle out-of-vocabulary (OOV) words well. If a word is not in the training corpus, the model won’t have an embedding for it. Similarly, it cannot generalize across words with similar morphological structures.\n\n\n\n\n\nSub-word-level Tokenization\n\n\n\n\n  \nSub-word tokenization\n (breaking words into smaller units like character n-grams, morphemes, or subwords) is handled by models like FastText or Byte Pair Encoding (BPE) in models such as BERT. For example, FastText builds on Word2Vec by learning embeddings not just for words but for character n-grams, enabling it to generalize better to unseen words and handle morphological variations.\n\n\n\n\n\nRelated: \nMatryoshka Representation Learning\n\n\n\n\n  \nProposed in \nMatryoshka Representation Learning\n by Kusupati et al. from UW, Matryoshka Representation Learning (MRL) is a novel approach for adaptive and efficient representation learning.  This technique, adopted in OpenAI’s latest embedding update, text-embedding-3-large, is characterized by its ability to encode information at multiple granularities within a single high-dimensional vector. Drawing an analogy from the Russian Matryoshka dolls, MRL encapsulates details at various levels within a single embedding structure, allowing for adaptability to the computational and statistical needs of different tasks.\n\n  \nThe essence of MRL lies in its ability to create coarse-to-fine representations, where earlier dimensions in the embedding vector store more crucial information, and subsequent dimensions add finer details. You can understand how this works by the analogy of trying to classify an image at multiple resolutions – the lower resolutions give high-level info and the higher resolutions add finer details – human perception of the natural world also has a naturally coarse-to-fine granularity, as shown in the animation below.\n\n\n\n\n\n\n\n\n\n  \nMRL achieves this by modifying the loss function in the model, where the total loss is the sum of losses over individual vector dimension ranges: \\(Loss_{Total} =  L(\\text{upto 8d}) + L(\\text{upto 16d}) + L(\\text{upto 32d}) + \\ldots + L(\\text{upto 2048d})\\). As a result, MRL incentivizes the model to capture essential information in each subsection of the vector. Notably, this technique allows for the use of any subset of the embedding dimensions, offering flexibility beyond fixed dimension slices like 8, 16, 32, etc.\n\n  \nThe figure below from the paper shows that MRL is adaptable to any representation learning setup and begets a Matryoshka Representation \\(z\\) by optimizing the original loss \\(L(.)\\) at \\(O(\\log(d))\\) chosen representation sizes. Matryoshka Representation can be utilized effectively for adaptive deployment across environments and downstream tasks.\n\n\n\n\n\n\n\n\n\n  \nMRL’s adaptability extends to a wide range of modalities, including vision, vision+language, and language models (such as ViT, ResNet, ALIGN, and BERT). The method has shown remarkable results in various applications, such as adaptive classification and retrieval, robustness evaluations, few-shot and long-tail learning, and analyses of model disagreement. In practical terms, MRL facilitates up to 14x smaller embedding sizes for tasks like ImageNet-1K classification without compromising accuracy, up to 14x real-world speed-ups for large-scale retrieval, and up to 2% accuracy improvements in long-tail few-shot classification.\n\n  \nOne of the striking outcomes of using MRL is demonstrated in OpenAI’s text-embedding-3-large model, which, when trimmed to 256 dimensions, outperforms the full-sized text-embedding-ada-002 with 1536 dimensions on the MTEB benchmark. This indicates a significant reduction in size (to about 1/6th) while maintaining or even enhancing performance.\n\n  \nImportantly, MRL integrates seamlessly with existing representation learning pipelines, requiring minimal modifications and imposing no additional costs during inference and deployment. Its flexibility and efficiency make it a promising technique for handling web-scale datasets and tasks. OpenAI has made the pretrained models and code for MRL publicly available, underlining the method’s potential as a game-changer in the field of representation learning.\n\n  \nCode\n; \nOpenAI Blog\n\n\n\n\n\nReferences\n\n\n\n\n  \nNLPOverview: Word2Vec\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2021DistilledWordVectors,\n  title   = {Word Vectors},\n  author  = {Jain, Vinija and Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2021},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/word-vectors/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Vision Transformer (ViT)\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nHow the Vision Transformer (ViT) works in a nutshell\n\n  \nViT v/s CNNs: Data Efficiency and Fine-Tuning\n\n  \nRepresenting an image as a sequence of patches\n\n  \nPositional embeddings\n\n  \nKey findings\n\n  \nHow far away are the learned non-local interactions?\n\n  \nAttention distance and visualization\n\n  \nFAQs\n    \n\n      \nWhy does ViT use linear projections of flattened patches at the input?\n\n      \nHow are linear projections of flattened patches at the input calculated in ViT? What are the inputs and outputs?\n\n      \nWhy does ViT rely on \\(16 \\times 16\\) pixels for its input patches?\n\n      \nWhy does ViT not use a tokenizer at the input (akin to Transformers for text processing tasks)?\n\n      \nWhat are “tokens” when embedding an image using an encoder? How are they different compared to word/sub-word tokens in NLP?\n\n    \n\n  \n\n  \nImplementation\n\n  \nConclusion\n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \n\n    \nThis article investigates over how the Vision Transformer (ViT) works by going over the minor modifications of the transformer architecture for image classification.\n\n  \n\n  \n\n    \nWe recommend checking out the primers on \nTransformer\n and \nattention\n prior to exploring ViT.\n\n  \n\n  \nTransformers lack the inductive biases of Convolutional Neural Networks (CNNs), such as translation invariance and a locally restricted \nreceptive field\n. To clarify what this implies: invariance refers to the fact that you can recognize an entity (i.e. object) in an image, even when its appearance or position varies. Translation in computer vision implies that each image pixel has been moved by a fixed amount in a particular direction.\n\n  \nMoreover, remember that convolution is a linear local operator. We see only the neighbor values as indicated by the kernel.\n\n  \nOn the other hand, the transformer is by design permutation invariant. The bad news is that it cannot process grid-structured data. We need sequences! To this end, we will convert a spatial non-sequential signal to a sequence! Let’s see how.\n\n\n\n\n\nHow the Vision Transformer (ViT) works in a nutshell\n\n\n\n\n  \n\n    \nThe total architecture is called Vision Transformer (ViT), proposed by Alexey Dosovitskiy et al. (2020) in \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n. Let’s examine it step by step.\n\n\n    \n\n      \nSplit an image into patches\n\n      \nFlatten the patches\n\n      \nProduce lower-dimensional linear embeddings from the flattened patches\n\n      \nAdd positional embeddings\n\n      \nFeed the sequence as an input to a standard transformer encoder\n\n      \nPretrain the model with image labels (fully supervised on a huge dataset)\n\n      \nFinetune on the downstream dataset for image classification\n\n    \n\n  \n\n  \n\n    \nThe following image from Google’s AI blog shows the inner workings of ViT:\n\n  \n\n\n\n\n\n\n\n\n\n  \nImage patches are basically the sequence tokens (like words). In fact, the encoder block is identical to the original transformer proposed by Vaswani et al. (2017) as we have extensively \ndescribed\n. The following image shows the well-known transformer block (source: \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n):\n\n\n\n\n\n\n\n\n\n  \nThe only thing that changes is the number of those blocks. To this end, and to further prove that with more data they can train larger ViT variants, 3 models were proposed (source: \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n):\n\n\n\n\n\n\n\n\n\n  \n\n    \nHeads refer to \nmulti-head attention\n, while the MLP size refers to the blue module in the figure. MLP stands for multi-layer perceptron but it’s actually a bunch of linear transformation layers.\n\n  \n\n  \n\n    \nHidden size DD is the embedding size, which is kept fixed throughout the layers. Why keep it fixed? So that we can use short residual \nskip connections\n.\n\n  \n\n  \n\n    \nIn case you missed it, there is no decoder in the game. Just an extra linear layer for the final classification called MLP head. But is this enough? Yes and no. Actually, we need a massive amount of data and as a result computational resources.\n\n  \n\n\n\n\n\nViT v/s CNNs: Data Efficiency and Fine-Tuning\n\n\n\n\n  \n\n    \nSpecifically, if ViT is trained on datasets with more than 14M images it can approach or beat state-of-the-art CNNs. If not, you better stick with ResNets or EfficientNets.\n\n  \n\n  \n\n    \nViT is pretrained on the large dataset and then fine-tuned to small ones. The only modification is to discard the prediction head (MLP head) and attach a new \\(D \\times K\\)\\(linear layer, where\\)K$$ is the number of classes of the small dataset.\n\n  \n\n\n\n\n\n\n  \nIt is interesting that the authors claim that it is better to fine-tune at higher resolutions than pre-training.\n\n\n\n\n\n\n  \nTo fine-tune in higher resolutions, 2D \ninterpolation\n of the pre-trained position embeddings is performed. The reason is that they model positional embeddings with trainable linear layers. Having that said, the key engineering part of this paper is all about feeding an image in the transformer.\n\n\n\n\n\nRepresenting an image as a sequence of patches\n\n\n\n\n  \nLet’s go over how you can reshape the image in patches. For an input image \\((x) \\in R^{H} \\times W \\times C\\) and patch size \\(p\\), we want to create \\(N\\) image patches denoted as \\((x) p \\in R^{N} \\times\\left(P^{2} C\\right)\\), where \\(N=\\frac{H W}{P} \\cdot N\\) is the sequence length similar to the words of a sentence.\n\n  \nThe image patch, i.e., \\([16, 16, 3]\\) is flattened to \\(16 \\times 16 \\times 3\\). The title of the paper should now make sense :)\n\n  \nLet’s use the \neinops\n library that works atop PyTorch. You can install it via \npip\n:\n\n\n\n\n\n$ pip install einops\n\n\n\n\n\n  \nAnd then some compact Pytorch code:\n\n\n\n\n\nfrom\n \neinops\n \nimport\n \nrearrange\n\n\n\np\n \n=\n \npatch_size\n \n# P in maths\n\n\n\nx_p\n \n=\n \nrearrange\n(\nimg\n,\n \n'b c (h p1) (w p2) -> b (h w) (p1 p2 c)'\n,\n \np1\n \n=\n \np\n,\n \np2\n \n=\n \np\n)\n\n\n\n\n\n\n  \nIn short, each symbol or each parenthesis indicates a dimension. For more information on einsum operations check out this \nblogpost\n on einsum operations.\n\n  \nNote that the image patches are always squares for simplicity.\n\n  \nAnd what about going from patch to embeddings? It’s just a linear transformation layer that takes a sequence of \\(P^{2} C\\) elements and outputs \\(D\\).\n\n\n\n\n\npatch_dim\n \n=\n \n(\npatch_size\n**\n2\n)\n \n*\n \nchannels\n \n# D in math\n\n\n\npatch_to_embedding\n \n=\n \nnn\n.\nLinear\n(\npatch_dim\n,\n \ndim\n)\n\n\n\n\n\n\n  \nWhat’s missing is that we need to provide some sort of order.\n\n\n\n\n\nPositional embeddings\n\n\n\n\n  \nEven though many positional embedding schemes were applied, no significant difference was found. This is probably due to the fact that the transformer encoder operates on a patch-level. Learning embeddings that capture the order relationships between patches (spatial information) is not so crucial. It is relatively easier to understand the relationships between patches of \\(P \\times P\\) than of a full image \\(Height \\times Width\\).\n\n\n\n\n\n\n  \nIntuitively, you can imagine solving a puzzle of 100 pieces (patches) compared to 5000 pieces (pixels).\n\n\n\n\n\n\n  \nHence, after the low-dimensional linear projection, a trainable position embedding is added to the patch representations. It is interesting to see what these position embeddings look like after training (source: \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n):\n\n\n\n\n\n\n\n\n\n  \nFirst, there is some kind of 2D structure. Second, patterns across rows (and columns) have similar representations. For high resolutions, a sinusoidal structure was used.\n\n\n\n\n\nKey findings\n\n\n\n\n  \nIn the early CNN days, we used to visualize the early layers. Why? Because we believe that well-trained networks often show nice and smooth filters. This following image visualizes AlexNet’s learned filters on the left (source: \nStandford’s Course CS231n\n and ViT’s learned filters on the right (source: \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n).\n\n\n\n\n\n\n\n\n\n  \nAs stated in CS231n:\n\n\n\n\n\n\n  \n“Notice that the first-layer weights are very nice and smooth, indicating a nicely converged network. The color/grayscale features are clustered because the AlexNet contains two separate streams of processing, and an apparent consequence of this architecture is that one stream develops high-frequency grayscale features and the other low-frequency color features.” ~ Stanford CS231 Course: Visualizing what ConvNets learn\n\n\n\n\n\n\n  \nFor such visualizations PCA is used. In this way, the author showed that early layer representations may share similar features.\n\n\n\n\n\nHow far away are the learned non-local interactions?\n\n\n\n\n  \n\n    \nShort answer: For patch size \\(P\\), maximum \\(P \\times P\\), which in our case is 128, even from the 1st layer!\n\n  \n\n  \n\n    \nWe don’t need successive Conv layers to get to 128-away pixels anymore. With convolutions without dilation, the receptive field is increased linearly. Using self-attention we have interaction between pixels representations in the 1st layer and pairs of representations in the 2nd layer and so on. The following image shows the mean attention distance v/s the network depth (source: \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n) the receptive field growth on the right (source: generated using \nFomoro AI calculator\n).\n\n  \n\n\n\n\n\n\n\n\n\n  \nBased on the diagram on the left from ViT, one can argue that:\n    \n\n      \nThere are indeed heads that attend to the whole patch already in the early layers.\n\n      \nOne can justify the performance gain based on the early access pixel interactions. It seems more critical for the early layers to have access to the whole patch (global info). In other words, the heads that belong to the upper left part of the image may be the core reason for superior performance.\n\n      \nInterestingly, the attention distance increases with network depth similar to the receptive field of local operations.\n\n      \nThere are also attention heads with consistently small attention distances in the low layers. On the right, a 24-layer with standard 3x3 convolutions has a receptive field of less than 50. We would approximately need 50 conv layers, to attend to a ~100 receptive field, without dilation or pooling layers.\n\n      \nTo enforce this idea of highly localized attention heads, the authors experimented with hybrid models that apply a ResNet before the Transformer. They found less highly localized heads, as expected. Along with filter visualization, it suggests that it may serve a similar function as early convolutional layers in CNNs.\n\n    \n\n  \n\n\n\n\n\nAttention distance and visualization\n\n\n\n\n  \nIt is critical to understand how they measured the mean attention distance. It’s analogous to the receptive field, but not exactly the same.\n\n  \nAttention distance was computed as the average distance between the query pixel and the rest of the patch, multiplied by the attention weight. They used 128 example images and averaged their results.\n\n  \nAn example: if a pixel is 20 pixels away and the attention weight is 0.5 the distance is 10.\n\n  \nFinally, the model attends to image regions that are semantically relevant for classification, as illustrated below (source: \nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n):\n\n\n\n\n\n\n\n\nFAQs\n\n\n\nWhy does ViT use linear projections of flattened patches at the input?\n\n\n\n\n  \n\n    \nThe ViT model uses linear projections of flattened patches at the input for several reasons:\n\n\n    \n\n      \n\n        \nReduction of Dimensionality\n: Images are inherently high-dimensional data. Flattening and projecting the patches into a lower-dimensional space makes the computation more manageable and efficient. This process is analogous to reducing the resolution of an image while retaining essential features.\n\n      \n\n      \n\n        \nUniform Data Representation\n: By flattening and projecting, the vision transformer treats the image patches similarly to how tokens (words) are treated in a language model. This uniformity allows the use of transformer architecture, originally designed for NLP tasks, in processing visual data.\n\n      \n\n      \n\n        \nCapturing Local Features\n: Each patch represents local features of the image. By projecting these patches, the model can capture and process these local features effectively. This is akin to how convolutional neural networks (CNNs) operate, but with a different approach.\n\n      \n\n      \n\n        \nScalability and Flexibility\n: The approach allows the model to be scalable to different image sizes and resolutions. It also offers flexibility in terms of the size of the patches and the depth of the network.\n\n      \n\n      \n\n        \nEnabling Positional Encoding\n: Flattening and projecting the patches allow for the addition of positional encodings. In transformers, positional encodings are crucial as they provide the model with information about the relative or absolute position of the patches in the image. Unlike CNNs, transformers do not inherently understand the order or position of the input data, so positional encodings are necessary.\n\n      \n\n      \n\n        \nFacilitating Self-Attention Mechanism\n: The transformer architecture relies heavily on the self-attention mechanism, which computes the response at a position in a sequence (in this case, a sequence of patches) by attending to all positions and computing a weighted sum of their features. Flattened and projected patches are conducive to this mechanism.\n\n      \n\n    \n\n  \n\n  \n\n    \nIn summary, the use of linear projections of flattened patches in ViT is a strategic design choice that leverages the strengths of transformer architecture while making it suitable for processing visual data. This method allows the model to handle high-dimensional image data efficiently, capture local features, and utilize the self-attention mechanism effectively.\n\n  \n\n\n\n\n\nHow are linear projections of flattened patches at the input calculated in ViT? What are the inputs and outputs?\n\n\n\n\n  \n\n    \nIn ViT, the linear projections of flattened patches at the input are calculated using a specific process. Let’s break down the steps, inputs, and outputs of this process:\n\n\n    \n\n      \nInput Image Processing\n:\n        \n\n          \nInput\n: The input is a raw image.\n\n          \nPatching\n: The image is divided into patches. For instance, if the patch size is 16x16 pixels, the image is split into non-overlapping 16x16 pixel patches.\n\n          \nFlattening\n: Each patch is flattened into a 1D vector. If a patch is 16x16 pixels and the image has 3 color channels (RGB), each flattened patch will have 16x16x3 = 768 elements.\n\n        \n\n      \n\n      \nLinear Projection\n:\n        \n\n          \nFlattened Patches\n: These are the flattened vectors from the above step.\n\n          \nProjection Matrix\n: A trainable matrix is used to project the flattened patches into a desired embedding dimension (say D). This matrix is part of the model’s learnable parameters.\n\n          \nCalculation\n: The linear projection is a matrix multiplication between the flattened patch vectors and the projection matrix. Mathematically, if ( P ) is a flattened patch vector and ( W ) is the projection matrix, then the projected patch embedding ( E ) is calculated as ( E = P \\times W ).\n\n          \nOutput (Patch Embeddings)\n: The output of this step is a series of D-dimensional vectors, where each vector represents a projected patch of the original image.\n\n        \n\n      \n\n      \nAddition of Positional Encodings\n:\n        \n\n          \nPositional Embeddings\n: To retain spatial information, positional embeddings are added to the patch embeddings. These embeddings are also learnable and represent the position of each patch in the image.\n\n          \nFinal Patch Representation\n: The final representation of each patch is the sum of its projected patch embedding and its positional embedding.\n\n        \n\n      \n\n      \nSequence Formation for Transformer\n:\n        \n\n          \nSequence of Patch Representations\n: The sequence of final patch representations (with positional information) is then fed into the subsequent layers of the transformer model for further processing.\n\n        \n\n      \n\n    \n\n  \n\n  \nIn summary:\n    \n\n      \nInputs\n: The input is a raw image, which is split into patches and then flattened.\n\n      \nProcess\n: Each flattened patch is linearly projected into a higher-dimensional space using a trainable projection matrix, and positional encodings are added.\n\n      \nOutputs\n: The outputs are the patch embeddings with added positional information, forming a sequence that is ready to be processed by the transformer’s layers.\n\n    \n\n  \n\n  \nThis process transforms the spatial image data into a sequence of embeddings, similar to how sentences are represented as sequences of word embeddings in NLP transformers. The linear projection and the addition of positional encodings are crucial for adapting the transformer architecture, originally designed for sequential data like text, to handle image data effectively.\n\n\n\n\n\nWhy does ViT rely on \\(16 \\times 16\\) pixels for its input patches?\n\n\n\n\n  \n\n    \nViT often uses 16x16 pixel patches for its input, but this choice is not a strict requirement; it’s more of a practical and empirical decision. Let’s explore why 16x16 is commonly used and whether other patch sizes could work:\n\n\n    \n\n      \n\n        \nBalance Between Granularity and Computational Efficiency\n: A 16x16 patch size is a compromise between capturing sufficient detail in each patch and keeping the number of patches (and thus the sequence length for the transformer) manageable. Smaller patches would provide more detailed information but would increase the sequence length and computational cost. Larger patches would reduce the sequence length but might miss finer details in the image.\n\n      \n\n      \n\n        \nEmpirical Performance\n: The choice of 16x16 has been empirically found to work well for a range of tasks and datasets in many studies and applications. This practical experience has led to its common adoption.\n\n      \n\n      \n\n        \nComparison with Convolutional Networks\n: The patch size somewhat mirrors the receptive field of filters in traditional convolutional neural networks (CNNs). In CNNs, filters capture local patterns, and their effective receptive field often aligns with the size of these patches in ViTs.\n\n      \n\n      \n\n        \nHardware and Memory Constraints\n: The patch size also needs to be chosen considering the hardware and memory constraints. Larger models with smaller patches might not be feasible for training and inference on available hardware.\n\n      \n\n      \n\n        \nOther Patch Sizes\n: ViTs can certainly work with other patch sizes. The choice of patch size is a hyperparameter that can be tuned based on the specific requirements of the task, the nature of the dataset, and the available computational resources. For instance, for higher resolution images or tasks requiring finer details, smaller patches might be more suitable. Conversely, for tasks where global features are more important, larger patches could be used.\n\n      \n\n    \n\n  \n\n  \n\n    \nIn summary, while \\(16 \\times 16\\) pixels is a common choice for input patches in ViTs due to a balance between detail capture and computational efficiency, it is not a fixed requirement. Other patch sizes can be used, and the optimal choice depends on the specific task, dataset characteristics, and available computational resources. Experimentation and empirical validation are key in determining the most suitable patch size for a given application.\n\n  \n\n\n\n\n\nWhy does ViT not use a tokenizer at the input (akin to Transformers for text processing tasks)?\n\n\n\n\n  \n\n    \nIn Vision Transformers (ViTs), the concept of tokenization is applied differently compared to how it’s used in traditional text-based transformers. ViTs do not use a tokenizer in the same sense as NLP models, and here’s why:\n\n\n    \n\n      \n\n        \nNature of Input Data\n: In NLP, tokenization is used to convert text into a series of tokens (words or subwords), which are then mapped to embeddings. This is necessary because text data is discrete and symbolic. In contrast, images are continuous and high-dimensional. The concept of ‘words’ or ‘characters’ does not directly apply to images.\n\n      \n\n      \n\n        \nImage Patching as a Form of Tokenization\n: In ViTs, the closest analog to tokenization is the division of an image into fixed-size patches. Each patch is treated as a ‘token’. These patches are then flattened and linearly projected into embeddings. This process can be thought of as a form of tokenization where each ‘token’ represents a part of the image rather than a word or character.\n\n      \n\n      \n\n        \nAbsence of Vocabulary\n: In text processing, tokenization is followed by mapping tokens to a vocabulary of known words or subwords. For images, there is no predefined vocabulary. Each patch is unique and is represented by its pixel values, which are then projected into a latent space.\n\n      \n\n      \n\n        \nDirect Processing of Raw Input\n: Unlike text, where tokenization is a necessary preprocessing step to handle the symbolic nature of language, ViTs can process raw image data directly (after patching and embedding). This direct processing of visual information is more analogous to how convolutional neural networks (CNNs) handle images, although the mechanisms are different.\n\n      \n\n    \n\n  \n\n  \n\n    \nIn summary, while ViTs don’t use a tokenizer in the traditional NLP sense, the process of dividing an image into patches and linearly projecting these patches serves a similar purpose. It converts the raw image data into a format that can be processed by the transformer architecture, ensuring that spatial information is retained and managed efficiently. The key difference lies in the nature of the input data and how it’s represented and processed within the model.\n\n  \n\n\n\n\n\nWhat are “tokens” when embedding an image using an encoder? How are they different compared to word/sub-word tokens in NLP?\n\n\n\n\n  \nWhen embedding an image using an encoder in the context of machine learning and neural networks, there are indeed “tokens,” although they might not be tokens in the traditional sense as understood in natural language processing (NLP).\n\n  \nIn NLP, tokens usually refer to words or subwords. However, in the context of image processing, an encoder, especially in models like Vision Transformers (ViT), converts an image into a series of numerical representations, which can be analogously thought of as “tokens.”\n\n  \n\n    \nHere’s a simplified breakdown of the process:\n\n\n    \n\n      \n\n        \nImage Segmentation\n: The image is divided into patches. Each patch can be seen as a visual “word.” This is analogous to tokenization in NLP, where a sentence is broken down into words or subwords.\n\n      \n\n      \n\n        \nFlattening and Linear Projection\n: Each patch is then flattened (turned into a 1D array) and passed through a linear projection to obtain a fixed-size vector. These vectors are the equivalent of word embeddings in NLP.\n\n      \n\n      \n\n        \nSequence of Embeddings\n: The sequence of these vectors (one for each patch) is what the Transformer encoder processes. Each vector represents the “token” corresponding to a part of the image.\n\n      \n\n      \n\n        \nPositional Encoding\n: Since Transformers don’t have a sense of order, positional encodings are added to these embeddings to provide information about the location of each patch in the image.\n\n      \n\n      \n\n        \nTransformer Encoder\n: The sequence of patch embeddings, now with positional information, is fed into the Transformer encoder. The encoder processes these embeddings (tokens) through its layers, enabling the model to understand and encode complex relationships and features within the image.\n\n      \n\n    \n\n  \n\n  \nIn summary, while the term “tokens” originates from text processing, its use in image processing with encoders like Vision Transformers is an analogy to how images are broken down into manageable, meaningful segments for the model to process, similar to how text is tokenized into words or subwords.\n\n\n\n\n\nImplementation\n\n\n\n\n  \nGiven an implementation of the vanilla Transformer encoder, ViT looks as simple as this:\n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nfrom\n \neinops\n \nimport\n \nrearrange\n\n\n\nfrom\n \nself_attention_cv\n \nimport\n \nTransformerEncoder\n\n\n\n\nclass\n \nViT\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \n*\n,\n\n                 \nimg_dim\n,\n\n                 \nin_channels\n=\n3\n,\n\n                 \npatch_dim\n=\n16\n,\n\n                 \nnum_classes\n=\n10\n,\n\n                 \ndim\n=\n512\n,\n\n                 \nblocks\n=\n6\n,\n\n                 \nheads\n=\n4\n,\n\n                 \ndim_linear_block\n=\n1024\n,\n\n                 \ndim_head\n=\nNone\n,\n\n                 \ndropout\n=\n0\n,\n \ntransformer\n=\nNone\n,\n \nclassification\n=\nTrue\n):\n\n        \n\"\"\"\n        Args:\n            img_dim: the spatial image size\n            in_channels: number of img channels\n            patch_dim: desired patch dim\n            num_classes: classification task classes\n            dim: the linear layer's dim to project the patches for MHSA\n            blocks: number of transformer blocks\n            heads: number of heads\n            dim_linear_block: inner dim of the transformer linear block\n            dim_head: dim head in case you want to define it. defaults to dim/heads\n            dropout: for pos emb and transformer\n            transformer: in case you want to provide another transformer implementation\n            classification: creates an extra CLS token\n        \"\"\"\n\n        \nsuper\n().\n__init__\n()\n\n        \nassert\n \nimg_dim\n \n%\n \npatch_dim\n \n==\n \n0\n,\n \nf\n'patch size \n{\npatch_dim\n}\n not divisible'\n\n        \nself\n.\np\n \n=\n \npatch_dim\n\n        \nself\n.\nclassification\n \n=\n \nclassification\n\n        \ntokens\n \n=\n \n(\nimg_dim\n \n//\n \npatch_dim\n)\n \n**\n \n2\n\n        \nself\n.\ntoken_dim\n \n=\n \nin_channels\n \n*\n \n(\npatch_dim\n \n**\n \n2\n)\n\n        \nself\n.\ndim\n \n=\n \ndim\n\n        \nself\n.\ndim_head\n \n=\n \n(\nint\n(\ndim\n \n/\n \nheads\n))\n \nif\n \ndim_head\n \nis\n \nNone\n \nelse\n \ndim_head\n\n        \nself\n.\nproject_patches\n \n=\n \nnn\n.\nLinear\n(\nself\n.\ntoken_dim\n,\n \ndim\n)\n\n\n        \nself\n.\nemb_dropout\n \n=\n \nnn\n.\nDropout\n(\ndropout\n)\n\n        \nif\n \nself\n.\nclassification\n:\n\n            \nself\n.\ncls_token\n \n=\n \nnn\n.\nParameter\n(\ntorch\n.\nrandn\n(\n1\n,\n \n1\n,\n \ndim\n))\n\n            \nself\n.\npos_emb1D\n \n=\n \nnn\n.\nParameter\n(\ntorch\n.\nrandn\n(\ntokens\n \n+\n \n1\n,\n \ndim\n))\n\n            \nself\n.\nmlp_head\n \n=\n \nnn\n.\nLinear\n(\ndim\n,\n \nnum_classes\n)\n\n        \nelse\n:\n\n            \nself\n.\npos_emb1D\n \n=\n \nnn\n.\nParameter\n(\ntorch\n.\nrandn\n(\ntokens\n,\n \ndim\n))\n\n\n        \nif\n \ntransformer\n \nis\n \nNone\n:\n\n            \nself\n.\ntransformer\n \n=\n \nTransformerEncoder\n(\ndim\n,\n \nblocks\n=\nblocks\n,\n \nheads\n=\nheads\n,\n\n                                                  \ndim_head\n=\nself\n.\ndim_head\n,\n\n                                                  \ndim_linear_block\n=\ndim_linear_block\n,\n\n                                                  \ndropout\n=\ndropout\n)\n\n        \nelse\n:\n\n            \nself\n.\ntransformer\n \n=\n \ntransformer\n\n\n    \ndef\n \nexpand_cls_to_batch\n(\nself\n,\n \nbatch\n):\n\n        \n\"\"\"\n        Args:\n            batch: batch size\n        Returns: cls token expanded to the batch size\n        \"\"\"\n\n        \nreturn\n \nself\n.\ncls_token\n.\nexpand\n([\nbatch\n,\n \n-\n1\n,\n \n-\n1\n])\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nimg\n,\n \nmask\n=\nNone\n):\n\n        \nbatch_size\n \n=\n \nimg\n.\nshape\n[\n0\n]\n\n        \nimg_patches\n \n=\n \nrearrange\n(\n\n            \nimg\n,\n \n'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)'\n,\n\n                                \npatch_x\n=\nself\n.\np\n,\n \npatch_y\n=\nself\n.\np\n)\n\n        \n# project patches with linear layer + add pos emb\n\n        \nimg_patches\n \n=\n \nself\n.\nproject_patches\n(\nimg_patches\n)\n\n\n        \nif\n \nself\n.\nclassification\n:\n\n            \nimg_patches\n \n=\n \ntorch\n.\ncat\n(\n\n                \n(\nself\n.\nexpand_cls_to_batch\n(\nbatch_size\n),\n \nimg_patches\n),\n \ndim\n=\n1\n)\n\n\n        \npatch_embeddings\n \n=\n \nself\n.\nemb_dropout\n(\nimg_patches\n \n+\n \nself\n.\npos_emb1D\n)\n\n\n        \n# feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n\n        \ny\n \n=\n \nself\n.\ntransformer\n(\npatch_embeddings\n,\n \nmask\n)\n\n\n        \nif\n \nself\n.\nclassification\n:\n\n            \n# we index only the cls token for classification. nlp tricks :P\n\n            \nreturn\n \nself\n.\nmlp_head\n(\ny\n[:,\n \n0\n,\n \n:])\n\n        \nelse\n:\n\n            \nreturn\n \ny\n\n\n\n\n\nConclusion\n\n\n\n\n  \nThe key engineering part of this work is the formulation of an image classification problem as a sequential problem by using image patches as tokens, and processing it by a Transformer. That sounds good and simple but it needs massive data. Unfortunately, Google owns the pretrained dataset so the results are not reproducible. And even if they were, you would need to have enough computing power.\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledVisionLanguageModels,\n  title   = {Vision Language Models},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/vit/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Skip Connections\n\n  \n\n\n  \n\n  \n\n  \nIntroduction\n\n  \nThe vanishing gradient problem\n\n  \nPrelude: Backpropagation\n\n  \nBackpropagation and partial derivatives\n    \n\n      \nChain rule\n\n    \n\n  \n\n  \nSkip connections for the win\n\n  \nResNet: skip connections via addition\n\n  \nDenseNet: skip connections via concatenation\n\n  \nShort and Long skip connections in Deep Learning\n\n  \nCase Study for long skip connections: U-Nets\n\n  \nConclusion\n\n  \nRelated Papers\n    \n\n      \nDeep Residual Learning for Image Recognition\n\n      \nLAUREL: Learned Augmented Residual Layer\n\n    \n\n  \n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nIntroduction\n\n\n\n\n  \n\n    \nIn order to understand the plethora of design choices involved in building deep neural nets (such as skip connections) that you see in so many works, it is critical to understand a little bit of the mechanisms of backpropagation.\n\n  \n\n  \n\n    \nIf you were trying to train a neural network back in 2014, you would definitely observe the so-called \nvanishing gradient problem\n. In simple terms: you are behind the screen checking the training process of your network and all you see is that the training loss stopped decreasing but your performance metric is still far away from the desired value. You check all your code lines to see if something was wrong all night and you find no clue. Not the best experience in the world, believe me! Wonder why? Because the gradients that facilitate learning weren’t propagating through all the way to the initial layers of the network! Hence leading to “vanishing gradients”!\n\n  \n\n\n\n\n\nThe vanishing gradient problem\n\n\n\n\n  \n\n    \nSo, let’s remind ourselves the update rule of gradient descent without momentum, given \\(L\\) to be the loss function and \\(\\lambda\\) the learning rate:\n\n\n\\[w_{new} = w_{current} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_{current}}\\]\n  \n\n  \n\n    \nWhat is basically happening is that you try to update the parameters by changing them with a small amount \\(\\alpha \\cdot \\frac{\\partial L}{\\partial w_{current}}\\) that was calculated based on the gradient, for instance, let’s suppose that for an early layer the average gradient \\(\\frac{\\partial L}{\\partial w_{current}} = 1e-15\\). Given a learning rate \\(\\alpha\\) of \\(1e-4\\), you basically change the layer parameters by the product of the referenced quantities \\((\\alpha \\cdot \\frac{\\partial L}{\\partial w_{current}})\\), which is \\(1e-19\\), and as such, implies little to no change to the weights. As a result, you aren’t actually able to train your network. This is the vanishing gradient problem.\n\n  \n\n\n\n\n\nPrelude: Backpropagation\n\n\n\n\n  \n\n    \nOne can easily grasp the vanishing gradient problem from the backpropagation algorithm. We will briefly inspect the backpropagation algorithm from the prism of the chain rule, starting from basic calculus to gain an insight on skip connections. In short, backpropagation is the “optimization-magic” behind deep learning architectures. Given that a deep network consists of a finite number of parameters that we want to learn, our goal is to iteratively optimize these parameters using the gradient of the loss function \\(L\\) with respect to the network’s parameters.\n\n  \n\n  \n\n    \nAs you have seen, each architecture has some input (say an image) and produces an output (prediction). The loss function is heavily based on the task we want to solve. For now, what you need to know is the loss function is a quantitative measure of the distance between two tensors, that can represent an image label, a bounding box in an image, a translated text in another language etc. You usually need some kind of supervision to compare the network’s prediction with the desired outcome (ground truth).\n\n  \n\n  \n\n    \nSo, the beautiful idea of backpropagation is to gradually minimize this loss by updating the parameters of the network. But how can you propagate the scalar measured loss inside the network? That’s exactly where backpropagation comes into play.\n\n  \n\n\n\n\n\nBackpropagation and partial derivatives\n\n\n\n\n  \nIn simple terms, backpropagation is about understanding how changing the weights (parameters) in a network impacts the loss function by computing the partial derivatives. For the latter, we use the simple idea of the chain rule, to minimize the distance in the desired predictions. In other words, backpropagation is all about calculating the gradient of the loss function while considering the different weights within that neural network, which is nothing more than calculating the partial derivatives of the loss function with respect to model parameters. By repeating this step many times, we will continually minimize the loss function until it stops reducing, or some other predefined termination criteria are met.\n\n\n\n\n\nChain rule\n\n\n\n\n  \nThe chain rule basically describes the gradient (rate of change) of a function with respect to some input variable. Let the function be the loss function \\(z\\) of a neural network, while \\(x\\) and \\(y\\) be parameters of the neural network, which are in turn functions of a previous layer parameter \\(t\\). Further, let \\(f, g, h\\) be different layers on the network that perform a non-linear operation on the input vector. As such,\n\n\n\n\n\\[z = f(x,y) \\quad x = g(t) \\quad y = h(t)\\]\n\n\n\n  \nUsing the \nchain rule\n of multi-variate calculus to express the gradient of \\(z\\) with respect to the input \\(t\\):\n\n\n\n\n\\[\\frac{\\partial z}{\\partial t } = \\frac{\\partial f}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial t}\\]\n\n\n\n  \n\n    \nInterestingly, the famous algorithm does exactly the same operation but in the opposite way: it starts from the output \\(z\\) and calculates the partial derivatives of each parameter, expressing it only based on the gradients of the later layers.\n\n  \n\n  \n\n    \nIt’s really worth noticing that all these values are often less than 1, independent of the sign. In order to propagate the gradient to the earlier layer’s, backpropagation uses multiplication of the partial derivatives (as in the chain rule). For every layer that we go backwards in the network, the gradient of the network gets smaller and smaller owing to multiplication of the upstream gradient with absolute value less than 1 to compute the downstream gradient at every layer (since \\(\\text{downstream gradient = local gradient }\\times\\text{ upstream gradient}\\)).\n\n  \n\n\n\n\n\nSkip connections for the win\n\n\n\n\n  \n\n    \nSkip connections are standard in many convolutional architectures. By using a skip connection, we provide an \nalternative path for the gradient\n (with backpropagation). It is experimentally validated that this additional paths are often beneficial for model convergence during training. As the name suggests, skip connections in deep architectures, skip some layer in the neural network and feed the output of one layer as the input to the next layers (instead of only the next one).\n\n  \n\n  \n\n    \nAs previously explained, using the chain rule, we must keep multiplying terms with the error gradient as we go backwards. However, in the long chain of multiplication, if we multiply many things together that are less than one, then the resulting gradient will be very small. Thus, the gradient becomes very small as we approach the earlier layers in a deep architecture. In some cases, the gradient becomes zero, meaning that we do not update the early layers at all.\n\n  \n\n  \n\n    \nIn general, there are two fundamental ways that one could use skip connections through different non-sequential layers:\n\n\n    \n\n      \nAddition as in residual architectures,\n\n      \nConcatenation as in densely connected architectures.\n\n    \n\n  \n\n  \n\n    \nLet’s first do a walk-through of skip connections via addition, which are commonly referred as \nresidual skip connections\n.\n\n  \n\n\n\n\n\nResNet: skip connections via addition\n\n\n\n\n  \nThe core idea is to \nbackpropagate through the identity function\n, by just using a vector addition. Then the gradient would simply be multiplied by one and its value will be maintained in the earlier layers. This is the main idea behind Residual Networks (ResNets): they stack these skip residual blocks together, as shown in the figure below (image taken from the \nResNet\n paper). We use an identity function to preserve the gradient.\n\n\n\n\n\n\n\n\n\n  \n\n    \nMathematically, we can represent the residual block, and calculate its partial derivative (gradient), given the loss function like this:\n\n\n\\[\\frac{\\partial L}{\\partial x } = \\frac{\\partial L}{\\partial H} \\frac{\\partial H}{\\partial x} = \\frac{\\partial L}{\\partial H} \\left( \\frac{\\partial F}{\\partial x} + 1 \\right) = \\frac{\\partial L}{\\partial H} \\frac{\\partial F}{\\partial x} + \\frac{\\partial L}{\\partial H}\\]\n\n    \n\n      \nwhere \\(H\\) is the output of the network snippet above and is given by \\(F(x) + x\\)\n\n    \n\n  \n\n  \n\n    \nApart from the vanishing gradients, there is another reason that we commonly use them. For a plethora of tasks (such as semantic segmentation, optical flow estimation, etc.) information captured in the initial layers could be utilized by the later layers for learning. It has been observed that in earlier layers the learned features correspond to \nlow-level semantic information\n that is extracted from the input. Without skip connections, that information would have turned too abstract.\n\n  \n\n\n\n\n\nDenseNet: skip connections via concatenation\n\n\n\n\n  \nAs stated, for many dense prediction problems, there is low-level information shared between the input and output, and it would be desirable to pass this information directly across the net. The alternative way that you can achieve skip connections is by concatenation of previous feature maps. The most famous deep learning architecture is DenseNet. Below you can see an example of feature reusability by concatenation with five convolutional layers (image taken from \nDenseNet\n):\n\n\n\n\n\n\n\n\n\n  \n\n    \nThis architecture heavily uses feature concatenation so as to ensure maximum information flow between layers in the network. This is achieved by connecting via concatenation all layers directly with each other, as opposed to ResNets. Practically, what you basically do is to concatenate the feature channel dimension. This leads to:\n\n\n    \n\n      \n\n        \nAn enormous amount of feature channels on the last layers of the network,\n\n      \n\n      \n\n        \nMore compact models and,\n\n      \n\n      \n\n        \nExtreme feature re-usability.\n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\nShort and Long skip connections in Deep Learning\n\n\n\n\n  \n\n    \nIn more practical terms, you have to be careful when introducing additive skip connections in your deep learning model. The dimensionality has to be the same in addition and also in concatenation apart from the chosen channel dimension. That is the reason why you see that additive skip connections are used in two kinds of setups:\n\n\n    \n\n      \n\n        \nShort skip connections.\n\n      \n\n      \n\n        \nLong skip connections.\n\n      \n\n    \n\n  \n\n  \n\n    \nShort skip connections are used along with consecutive convolutional layers that \ndo not change the input dimension\n (see ResNet), while long skip connections usually exist in encoder-decoder architectures. It is known that the global information (shape of the image and other statistics) resolves what, while local information resolves where (small details in an image patch).\n\n  \n\n  \n\n    \nLong skip connections often exist in \narchitectures that are symmetrical\n, where the \nspatial dimension is gradually reduced\n in the \nencoder\n part and is \ngradually increased\n in the \ndecoder part\n as illustrated below. In the decoder part, one can increase the dimensionality of a feature map via \ntranspose convolutional (ConvT)\n layers. The transposed convolution operation forms the same connectivity as the normal convolution but in the backward direction.\n\n  \n\n\n\n\n\nCase Study for long skip connections: U-Nets\n\n\n\n\n  \n\n    \nMathematically, if we express convolution as a matrix multiplication, then transpose convolution is the reverse order multiplication (\\(B \\times A\\) instead of \\(A \\times B\\)). The aforementioned architecture of the encoder-decoder scheme along with long skip connections is often referred as U-shape (U-net). Long skip connections are utilized for tasks that the prediction has the same spatial dimension as the input such as image segmentation, optical flow estimation, video prediction, etc.\n\n  \n\n  \n\n    \nLong skip connections can be formed in a symmetrical manner, as shown in the diagram below:\n\n  \n\n\n\n\n\n\n\n\n\n  \nBy introducing skip connections in the encoder-decoded architecture, fine-grained details can be recovered in the prediction. Even though there is no theoretical justification, symmetrical long skip connections work incredibly effectively in dense prediction tasks (medical image segmentation).\n\n\n\n\n\nConclusion\n\n\n\n\n  \n\n    \nTo sum up, the motivation behind skip connections is that they enable an \nuninterrupted gradient flow\n during training, which helps tackle the \nvanishing gradient problem\n. Concatenative skip connections enable an alternative way to ensure \nfeature reusability\n of the same dimensionality from the earlier layers and are widely used in symmetrical architectures.\n\n  \n\n  \n\n    \nOn the other hand, long skip connections are used to pass features from the encoder path to the decoder path in order to recover \nspatial information lost\n during \ndownsampling\n. Short skip connections appear to \nstabilize gradient updates\n in deep architectures. Overall, skip connections thus enable feature reusability and stabilize training and convergence.\n\n  \n\n  \n\n    \nIn \n“Visualizing the Loss Landscape of Neural Nets”\n by Li et al. (2017), it has been experimentally validated that the loss landscape changes significantly when introducing skip connections, as illustrated below:\n\n  \n\n\n\n\n\n\n\n\nRelated Papers\n\n\n\nDeep Residual Learning for Image Recognition\n\n\n\n\n  \nResNet paper by He et al. from Facebook AI in CVPR 2016. Most cited in several AI fields.\n\n  \nThe issue of vanishing gradients when training a deep neural network was addressed with two tricks:\n    \n\n      \nBatch normalization and,\n\n      \nShort skip connections\n\n    \n\n  \n\n  \nInstead of \\(H(x) = F(x)\\), the skip connection leads to \\(H(x) = F(x) + x\\), which implies that the model is learning the difference (i.e., residual), \\(F(x) = H(x) - x\\).\n\n\n\n\n\nLAUREL: Learned Augmented Residual Layer\n\n\n\n\n  \n\n    \nThis paper by Gaurav Menghani, Ravi Kumar, and Sanjiv Kumar from Google Research introduces LAUREL (Learned Augmented Residual Layer), a generalization of the canonical residual connection used in deep learning architectures. LAUREL aims to improve model quality while keeping parameter, latency, and memory overhead minimal, making it suitable as a drop-in replacement in both vision and language models.\n\n  \n\n  \n\n    \nCore Concept\n:\n\n\n    \n\n      \nLAUREL extends the standard residual formulation:\n\\(x_{i+1} = f(x_i) + x_i\\)\nto a more expressive form:\n\\(x_{i+1} = \\alpha f(x_i) + g(x_i, x_{i-1}, ..., x_0)\\)\nwhere $\\alpha$ is a learnable scalar and $g(\\cdot)$ is a learnable linear function over the current and previous layer outputs.\n\n      \nThe goal is to enhance the residual stream to support richer interactions and improved information propagation across layers.\n\n    \n\n  \n\n  \n\n    \nVariants\n:\n\n\n    \n\n      \n\n        \nLAUREL-RW (Residual Weights)\n: Introduces learnable weights $\\alpha$ and $\\beta$ for $f(x_i)$ and $x_i$ respectively:\n\\(x_{i+1} = \\alpha f(x_i) + \\beta x_i\\)\nAdds only two parameters per layer. Uses sigmoid or softmax to normalize $\\alpha$, $\\beta$.\n\n      \n\n      \n\n        \nLAUREL-LR (Low-Rank)\n: Adds a low-rank linear transformation \\(W = AB + I\\) on \\(x_i\\):\n\\(x_{i+1} = f(x_i) + B A x_i + x_i\\)\nReduces parameter growth using matrices $A, B \\in \\mathbb{R}^{D \\times r}$ where $r \\ll D$, leading to 2rD new parameters per layer.\n\n      \n\n      \n\n        \nLAUREL-PA (Previous Activations)\n: Incorporates previous $k$ activations:\n\\(x_{i+1} = f(x_i) + \\left( \\sum_{j=0}^{k-1} \\gamma_{i,j} h_i(x_{i-j}) \\right) + x_i\\)\nAdds \\(2rD + k\\) parameters when using low-rank transforms for $h_i$. Supports richer temporal residual interactions.\n\n      \n\n      \n\n        \nThese can be mixed into hybrid variants like LAUREL-RW+LR or LAUREL-RW+LR+PA, allowing flexibility in trade-offs between expressiveness and cost.\n\n      \n\n    \n\n  \n\n  \n\n    \nImplementation and Performance\n:\n\n\n    \n\n      \n\n        \nResNet-50 on ImageNet-1K\n:\n\n\n        \n\n          \nBaseline: 74.95% top-1 accuracy.\n\n          \nAdding one ResNet layer: 75.20% (+4.37% params).\n\n          \nLAUREL-RW: 75.10% (+0.003% params).\n\n          \nLAUREL-RW+LR (r=16): 75.20% (+1.68% params).\n\n          \nLAUREL-RW+LR+PA: 75.25% (+2.40% params), outperforming naive scaling with fewer parameters.\n\n        \n\n      \n\n      \n\n        \n1B LLM Pretraining (LLM-1)\n:\n\n\n        \n\n          \nBaseline vs LAUREL-RW+LR (r=4): 0.012% param increase, no measurable latency increase.\n\n          \nNotable improvements across tasks like GSM8K-CoT (+5.39%), BOOLQ (+13.08%), and BookQA (+20.05%).\n\n        \n\n      \n\n      \n\n        \n4B LLM Pretraining (LLM-2)\n:\n\n\n        \n\n          \nLAUREL-RW+LR (r=64): ~0.1% param increase, 1-2% latency increase.\n\n          \nImprovements on MATH (+4.08%), MGSM (+6.07%), BELEBELE (+8.27%), and multimodal tasks like MMMU (+12.75%).\n\n        \n\n      \n\n      \n\n        \nThe following figure from the paper shows: (Left) A standard residual connection; the model is divided into logical ‘blocks’, and the residual connection combines the output of a non-linear function \\(f\\) and the input to this function. (Right) An illustration of the LAUREL framework; LAUREL can be used to replace the regular residual connection. Again, \\(f\\) can be any non-linear function such as attention, MLPs, and groups of multiple non-linear layers.\n\n      \n\n    \n\n\n    \n\n  \n\n  \n\n    \nEfficiency and Scalability\n:\n\n\n    \n\n      \n\n        \nLAUREL is designed to be footprint-aware:\n\n\n        \n\n          \nLAUREL-RW: ~constant memory and latency.\n\n          \nLAUREL-LR: \\(\\Theta(2rD)\\) memory, \\(O(rD^2)\\) latency.\n\n          \nLAUREL-PA: \\(\\Theta(kD)\\) memory, \\(O(kD)\\) latency.\n\n        \n\n      \n\n      \n\n        \nLAUREL outperforms naive model scaling both in accuracy and parameter efficiency. For instance, it achieved higher ResNet-50 performance using 2.6× fewer parameters than adding an extra layer.\n\n      \n\n    \n\n  \n\n\n\n\n\nReferences\n\n\n\n\n  \n\n    \n3D U-Net: learning dense volumetric segmentation from sparse annotation\n by Çiçek et al. (2016)\n\n  \n\n  \n\n    \nU-net: Convolutional networks for biomedical image segmentation\n by Ronneberger et al. (2015)\n\n  \n\n  \n\n    \nDeep residual learning for image recognition\n by He et al. (2016).\n\n  \n\n  \n\n    \nLearning representations by back-propagating errors\n by Rumelhart et al. (1986)\n\n  \n\n  \n\n    \nNeural networks and deep learning\n by Nielsen et al. (2018)\n\n  \n\n  \n\n    \nDensely connected convolutional networks\n by Huang (2017)\n\n  \n\n  \n\n    \nThe importance of skip connections in biomedical image segmentation\n by Drozdzal et al. (2016)\n\n  \n\n  \n\n    \nVisualizing the loss landscape of neural nets\n by Li et al. (2018)\n\n  \n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledSkipConnections,\n  title   = {Skip Connections},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/skip-connections/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Speech Processing\n\n  \n\n\n  \n\n  \n\n  \nTerminology\n    \n\n      \nPhoneme\n\n      \nPhone\n\n      \nWhy do we need phonemes and phones?\n\n      \nAllophones\n        \n\n          \nEnvironments of phonemes\n\n        \n\n      \n\n      \nGraphemes\n\n      \nMono vs. Stereo Sound\n\n      \nKey takeaways\n\n    \n\n  \n\n  \nFeature Representations\n    \n\n      \nOscillogram\n\n      \nSpectrum\n\n      \nSpectrogram\n\n      \nWhy spectrograms?\n\n    \n\n  \n\n  \nHow are tokenizers used for pre-processing speech input?\n\n  \nWhat are “tokens” when embedding an audio sample using an encoder? How are they different compared to word/sub-word tokens in NLP?\n    \n\n      \nMel-Filterbanks and MFCCs\n\n      \nExample spectrogram and oscillogram\n\n      \nPerceptual Linear Prediction (PLP)\n\n      \nProsodic Features\n\n      \nIdiolectal Features\n\n    \n\n  \n\n  \nSpeech Processing Tasks\n    \n\n      \nArchitectural overview\n\n      \nFundamental speech tasks\n\n      \nHierarchy of phones, words, and sentences\n\n      \nAutomatic Speech Recognition\n        \n\n          \nWhat is automatic speech recognition?\n\n          \nFramework of an ASR System\n\n          \nAcoustic Model (Encoder)\n\n          \nEncoder-Decoder Architectures: Past vs. Present\n\n          \nPutting it all together\n\n        \n\n      \n\n      \nKeyword Spotting / Wakeword Detection\n\n      \nMeasuring Performance\n        \n\n          \nPrecision\n\n          \nRecall\n\n          \nPrecision-Recall (PR) Curves\n\n          \nF1-score\n\n          \nDetection Error Trade-off (DET) Curves\n\n          \nOnline learning with live data to improve robustness\n\n        \n\n      \n\n      \nHandling dynamic language switching and code switching\n\n      \nSpeaker Recognition\n\n    \n\n  \n\n  \nData augmentation\n    \n\n      \nTime domain\n        \n\n          \nGain\n\n          \nTypes of noise\n            \n\n              \nReverberation noise / room impulse response (RIR)\n\n              \nBabble noise\n\n            \n\n          \n\n          \nSpeed and pitch\n\n          \nCodec augmentation\n\n          \nMixup\n\n          \nDynamic Range Compression\n\n        \n\n      \n\n      \nFrequency domain\n        \n\n          \nSpecAugment\n\n        \n\n      \n\n    \n\n  \n\n  \nEvaluation metrics\n    \n\n      \nPrecision and Recall\n        \n\n          \nHistorical Background\n\n          \nExamples\n\n          \nApplications\n\n          \nFormulae\n\n          \nPrecision/Recall Tradeoff\n\n        \n\n      \n\n      \nReceiver Operating Characteristic (ROC) Curve\n\n      \nDetection error tradeoff (DET) curve\n        \n\n          \nComparing ROC and DET curves\n\n          \nArea under the ROC Curve (AUC)\n\n        \n\n      \n\n      \nPrecision-Recall Curve\n        \n\n          \nArea Under the PR Curve (AUC)\n\n        \n\n      \n\n      \nKey takeaways: Precision, Recall and ROC/PR Curves\n\n      \nSpeech Recognition\n\n      \nSpeech-to-Speech Machine Translation\n\n      \nManual evaluation by humans for fluency, grammar, comparative ranking, etc.\n\n    \n\n  \n\n  \n🤗 Open ASR Leaderboard\n\n  \nPopular Models\n    \n\n      \nDistil-Whisper\n\n      \nParakeet\n\n      \nCanary\n\n    \n\n  \n\n  \nSuggested Videos\n\n  \nReferences\n\n  \nAcknowledgements\n\n  \nCitation\n\n\n\n\n\nTerminology\n\n\n\nPhoneme\n\n\n\n\n  \nA phoneme is the smallest unit that distinguishes meaning between sounds in a given language.” What does that mean? Let’s look at a word using IPA, a transcription system created by the International Phonetic Association.\n\n  \nLet’s look at the word puff. We use broad transcription when describing phonemes. When we are using broad transcription we use slashes (\n/ /\n). So the word puff in broad transcription is:\n\n\n\n\n\\[/pʌf/\\]\n\n\n\n  \nHere we see that puff has three phonemes \n/p/\n, \n/ʌ/\n,  and \n/f/\n. When we store the pronunciation of the word puff in our head, this is how we remember it. What happens if we change one phoneme in the word puff? If we change the phoneme (not the letters) \n/f/\n to the phoneme \n/k/\n we get another word. We get the word puck which looks like this in broad transcription:\n\n\n\n\n\\[/pʌk/\\]\n\n\n\n  \nThis is a type of test that we can do to see if \n/f/\n and \n/k/\n are different phonemes. If we swap these two phonemes we get a new word so we can say that in English \n/f/\n and \n/k/\n are different phonemes. We’re going to discuss phones now, but keep this in the back of your head, because we are going to come back to it.\n\n\n\n\n\nPhone\n\n\n\n\n  \nNow that we’ve covered what a phoneme is, we can discuss phones. Remember that we defined a phoneme as “the smallest unit that distinguishes meaning between sounds in a given language.” However, a phoneme is really the mental representation of a sound, not the sound itself. The phoneme is the part that is stored in your brain. When you actually produce a sound you are producing a phone.\nTo give an example, let’s say you want to say the word for a small four-legged animal that meows, a cat. Your brain searches for the word in your lexicon to see if you know the word. You find the lexical entry. You see that phonemic representation of the word is \n/kæt/\n. Then you use your vocal tract to produce the sounds \n[k]\n, \n[æ]\n, and \n[t]\n and you get the word \n[kæt]\n.\n\n  \nPhones, the actual sound part that you can hear, are marked with brackets (\n[]\n) and the phonemes, the mental representation of the sound, are marked with slashes (\n/ /\n).\n\n\n\n\n\nWhy do we need phonemes and phones?\n\n\n\n\n  \nRecap: Phonemes are the mental representation of the how a word sounds and phones are the actual sounds themselves. If we take an example from above, the word puff we can write out the phonemic representation (with phonemes using slashes) and the phonetic representation (with phones using brackets).\n\n\n\n\n\\[/pʌf/\\\\\n\n[pʌf]\\]\n\n\n\n  \n\n    \nWhat does the above show us? Not a whole lot really. So why do we need two different versions? Recall that the transcription that uses phonemes is called broad transcription while the transcription that uses phones is called narrow transcription. These names can give us a clue about the differences.\n\n  \n\n  \n\n    \nBy looking at the broad transcription, \n/pʌf/\n, we can know how to pronounce the word puff. Not only we (as native English speakers) can pronounce the word, but also a non-native English speaker can  pronounce it as well. We should all be able to understand what we are saying. However, what if we wanted more information about how the word actually sounds? Narrow transcription can help us with that.\n\n  \n\n  \n\n    \nNarrow transcription just gives us extra information about how a word sounds. So the word puff can be written like this in narrow transcription:\n\n  \n\n\n\n\n\\[[pʰʌf]\\]\n\n\n\n  \nWell, that’s new. This narrow transcription of the word puff gives us a little more information about how the word sounds. Here, we see that the \n[p]\n is aspirated. This means that when pronouncing the sound \n[p]\n,  we have an extra puff of air that comes out. We notate this by using the superscript \nʰ\n.\nSo you are probably asking yourself, why don’t we just put the \nʰ\n in the broad transcription? Remember that broad transcription uses phonemes and by definition, if we change a phoneme in a word, we will get a different word. Look at the following:\n\n\n\n\n\\[/pʌf/\\\\\n\n/pʰʌf/ *\\]\n\n\n\n  \nAn asterisk denotes that the above is incorrect. But why? Because in English, an aspirated p and an unaspirated p don’t change the meaning of a word. That is, you can pronounce the same sound two different ways, but it wouldn’t change the meaning. And by definition, if we change a phoneme, we change the meaning of a word. That means there’s only one \n/p/\n phoneme in English. If we were speaking a language where aspiration does change the meaning of a word, then that language could have two phonemes, \n/p/\n and \n/pʰ/\n. Since it doesn’t change the meaning in English, we just mark it in narrow transcription.\n\n\n\n\n\nAllophones\n\n\n\n\n  \n\n    \nWe can pronounce the \n/p/\n phoneme in at least two different ways: \n[p]\n and \n[pʰ]\n.  This means that \n[p]\n and \n[pʰ]\n are allophones of the phoneme \n/p/\n. The prefix “-allo” comes from the Greek állos meaning “other,” so you can think of allopones are just “another way to pronounce a phoneme.”\n\n  \n\n  \n\n    \nThis really helps us when we talk about different accents. Take the word water for example. I’m American, so the phonemic representation that I have for the word water is:\n\n  \n\n\n\n\n\\[/wɑtəɹ/\\]\n\n\n\n  \nBut if you’ve ever heard an American pronounce the word water before, you know that many Americans don’t pronounce a \n[t]\n sound. Instead, most Americans will pronounce that \n[t]\n similar to a \n[d]\n sound. It’s not pronounced the same was as a \n[d]\n sound is pronounced, though. It’s actually a “flap” and written like this:\n\n\n\n\n\\[[ɾ]\\]\n\n\n\n  \nSo the actual phonetic representation of the word water for many Americans is:\n\n\n\n\n\\[[wɑɾɚ]\\]\n\n\n\n  \n\n    \nSo, in the phonemic representation (broad transcription), we have the \n/t/\n phoneme, but most Americans will produce a \n[ɾ]\n here. That means we can say that in American English \n[ɾ]\n is an allophone of the phoneme \n/t/\n.\n\n  \n\n  \n\n    \nBut sometimes the \n/t/\n phoneme does use a \n[t]\n sound like in the name Todd:\n\n  \n\n\n\n\n\\[/tɑd/\\]\n\n\n\n  \nand in narrow transcription:\n\n\n\n\n\\[[tʰɑd]\\]\n\n\n\n  \nWith these examples, we can see that the phoneme \n/t/\n has at least two allophones: \n[tʰ]\n and \n[ɾ]\n. We can even look at the word putt and see that the \n[t]\n can be pronounced as a “regular” \n[t]\n sound:\n\n\n\n\n\\[[pʌt]\\]\n\n\n\n  \nFantastic! So now we know that the phoneme \n/t/\n has at least three allophones, \n[t]\n,  \n[tʰ]\n,  and \n[ɾ]\n.  But how do we know when to say each one?\n\n\n\n\n\nEnvironments of phonemes\n\n\n\n\n  \nWhen we talk about the environments of a phoneme we are talking about where the phoneme occurs, usually in relation to other phonemes in a word. We can use this information to predict how a phoneme will be pronounced. Take for example the name Todd:\n\n\n\n\n\\[/tɑd/\\]\n\n\n\n  \n\n    \nIf this was a word that we had never heard before, how would we know how to pronounce the \n/t/\n phoneme? Well, we can already narrow it down to \n[t]\n,  \n[tʰ]\n,  or \n[ɾ]\n because we’ve seen in past examples that we can pronounce these when we have \n/t/\n phoneme. But how do we know which phone is the correct one?\n\n  \n\n  \n\n    \nIf you combed through many many words in English, you would find out that the phoneme \n/t/\n is often aspirated when it’s at the beginning of a word. By looking at other words that start with \n/t/\n like tap, take, tack, etc. you’ll find that \n/t/\n becomes \n[tʰ]\n when at the beginning of a word and that the narrow transcription of the name Todd would be:\n\n  \n\n\n\n\n\\[[tʰɑd]\\]\n\n\n\n  \nWe can use the same process to find out how to pronounce other words in an American accent. If we look at the words eating, little, latter, etc… we can see that in American English all of \n/t/\n are pronounced as \n[ɾ]\n.  Deciding all of the requirements for realizing \n/t/\n as \n[ɾ]\n is beyond the scope of this post, but you can see that in similar environments the \n/t/\n becomes a \n[ɾ]\n.\n\n\n\n\n\nGraphemes\n\n\n\n\n  \nIn linguistics, a grapheme is the smallest functional unit of a \nwriting system\n.\n\n  \nThe name grapheme is given to the letter or combination of letters that represents a phoneme. For example, the word ‘ghost’ contains five letters and four graphemes (\ngh\n, \no\n, \ns\n, and \nt\n), representing four phonemes.\n\n\n\n\n\nMono vs. Stereo Sound\n\n\n\n\n  \nThe difference between monophonic (mono) and stereophonic (stereo) sound is the number of channels used to record and playback audio.\n\n  \nMono signals are recorded and played back using a single audio channel, while stereo sounds are recorded and played back using two audio channels.\n\n  \nAs a listener, the most noticeable difference is that stereo sounds are capable of producing the perception of width, whereas mono sounds are not.\n\n  \nUsing Mono instead of Stereo playback is most useful for users with certain types of hearing loss or for safety reasons, for example when you need to listen to your surroundings.\n\n\n\n\n\nKey takeaways\n\n\n\n\n  \nA phoneme is a mental representation of a sound, not necessarily a letter. Also, when we swap a phoneme we change the word.\n\n  \nA phone is the phonetic representation of a phoneme (the actual sound).\n\n  \nAllophones are different ways to pronounce the same phoneme while keeping the same meaning.\n\n  \nSometimes allophones are predictable depending on their environment and who is speaking.\n\n  \nWith Mono audio, both left and right audio channels get played back simultaneously via a single channel when playing audio (as opposed to Stereo audio where individual channels retain their presence and are played back separately).\n\n\n\n\n\nFeature Representations\n\n\n\nOscillogram\n\n\n\n\n  \nAn oscillogram (also called a “time-domain waveform” or simply, a “waveform” in speech context) is a plot of amplitude vs. time. It is a record produced by an oscillograph or oscilloscope.\n\n\n\n\n\nSpectrum\n\n\n\n\n  \nTaking the Fourier transform of a slice of a speech signal yields the spectrum/spectral vector for that slice. A sequence of these spectral vectors yields the plot of frequency vs. time as shown below.\n\n\n\n\n\n\n\n\nSpectrogram\n\n\n\n\n  \nFrequency vs. time representation of a speech signal is referred to as a spectrogram.\n\n  \nTo obtain a spectrogram, first obtain the spectrum (image source: \nSpeech Technology - Kishore Prahallad\n):\n\n\n\n\n\n\n\n\n\n  \nRotate it by 90 degrees:\n\n\n\n\n\n\n\n\n\n  \nColor-code the amplitude:\n\n\n\n\n\n\n\n\n\n  \nHorizontally tiling the color-coded spectrums yields a spectrogram as shown below. A spectrogram is thus formally defined as a plot of frequency vs. time.\n\n\n\n\n\n\n\n\n\n  \nAn example spectrogram from \nWikipedia: Spectrogram\n is as shown below:\n\n\n\n\n\n\n\n\n\n  \nAn example 3D spectrogram is as shown below:\n\n\n\n\n\n\n\n\nWhy spectrograms?\n\n\n\n\n  \nDark regions indicate peaks (formants) in the spectrum:\n\n\n\n\n\n\n\n\n\n  \nPhones and their properties are better observed in spectrogram:\n\n\n\n\n\n\n\n\n\n  \nSounds can be identified much better by the formants and their transitions. Hidden Markov Models implicitly model these spectrograms to perform speech recognition.\n\n\n\n\n\n\n\n\n\n  \nKey takeaways\n\n    \n\n      \nA spectrogram is a time-frequency representation of the speech signal.\n\n      \nA spectrogram is a tool to study speech sounds (phones).\n\n      \nPhones and their properties are visually studied by phoneticians using a spectrogram.\n\n      \nHidden Markov Models implicitly model spectrograms for speech to text systems.\n\n      \nUseful for evaluation of text to speech systems: A high quality text to speech system should produce synthesized speech whose spectrograms nearly match those with with spoken language.\n\n    \n\n  \n\n\n\n\n\nHow are tokenizers used for pre-processing speech input?\n\n\n\n\n  \n\n    \nTokenizers are used for processing speech input in neural networks, especially in tasks like speech recognition, where the goal is to convert audio data into a textual format. These tokenizers in the context of speech processing work differently from those in text processing. Here’s a brief overview:\n\n\n    \n\n      \n\n        \nFeature Extraction\n: Before tokenization, the raw audio data is usually processed to extract meaningful features. Common features include spectrograms, Mel-frequency cepstral coefficients (MFCCs), or directly using raw waveform data. These features represent the audio in a more compact and informative way than raw waveforms.\n\n      \n\n      \n\n        \nFrame-Based Tokenization\n: The extracted features are often segmented into frames, which can be considered as tokens. Each frame captures a short segment of the audio signal (e.g., 20-30 milliseconds). This is similar to how text tokenization breaks down sentences into words or subwords.\n\n      \n\n      \n\n        \nAcoustic Modeling\n: These frames or tokens are then fed into an acoustic model, typically a neural network, which aims to map the audio features to a set of acoustic units. These units might be phonemes (basic units of sound), characters, or subword units.\n\n      \n\n      \n\n        \nSubword and Character Tokenizers\n: In some modern speech recognition systems, especially those based on end-to-end deep learning, subword or character tokenizers are used. These systems directly map the audio features to sequences of characters or subwords, bypassing the traditional phoneme-level representation. This approach has gained popularity with the advent of models like the Connectionist Temporal Classification (CTC) and attention-based models.\n\n      \n\n      \n\n        \nByte Pair Encoding (BPE) and WordPiece\n: In advanced models, tokenizers like Byte Pair Encoding or WordPiece (common in NLP for text tokenization) are adapted for speech recognition. These methods dynamically create a vocabulary of common subword units based on the training data, which helps in handling a wide range of spoken language including rare words or names.\n\n      \n\n      \n\n        \nEnd-to-End Systems\n: In some end-to-end speech recognition systems, the concept of explicit tokenization is less clear. These systems take in raw audio and output text directly, using complex neural network architectures to implicitly handle the tokenization and recognition processes.\n\n      \n\n    \n\n  \n\n  \n\n    \nIn summary, while the term “tokenizer” in speech processing might not always refer to a discrete component as in text processing, the concept of breaking down audio into manageable, meaningful units for further processing by neural networks is a fundamental part of modern speech recognition and processing systems.\n\n  \n\n\n\n\n\nWhat are “tokens” when embedding an audio sample using an encoder? How are they different compared to word/sub-word tokens in NLP?\n\n\n\n\n  \nWhen embedding an audio sample using an encoder, especially in modern machine learning models, the concept of “tokens” is also applicable, albeit in a slightly different manner compared to text or image processing.\n\n  \n\n    \nIn the context of audio processing, particularly with neural network models, the audio signal is typically transformed into a series of numerical representations that can be analogously considered as “tokens.” Here’s how this process generally works:\n\n\n    \n\n      \n\n        \nSignal Segmentation\n: The continuous audio signal is first divided into smaller, manageable segments. This can be done using various methods such as windowing (e.g., short-time Fourier transform) or by directly splitting the raw waveform into chunks.\n\n      \n\n      \n\n        \nFeature Extraction\n: From each segment, features are extracted. These features can be in various forms, like Mel-frequency cepstral coefficients (MFCCs), spectrograms, or even raw audio waveforms. Each set of features represents a segment of the audio and can be thought of as an “audio token.”\n\n      \n\n      \n\n        \nToken Embeddings\n: These audio tokens are then transformed into a numerical format suitable for processing by a neural network. In some models, this might involve a linear transformation or the use of a more complex network like a convolutional neural network (CNN) to create a fixed-size vector, i.e., embeddings.\n\n      \n\n      \n\n        \nSequence Processing\n: The resulting sequence of embeddings (tokens) is then fed into an encoder, such as a Transformer or a recurrent neural network (RNN), which processes the sequence to capture temporal relationships and patterns within the audio.\n\n      \n\n      \n\n        \nPositional Information\n: For models like Transformers, positional encodings might be added to the embeddings to provide temporal context, as Transformers do not inherently process sequential data in order.\n\n      \n\n    \n\n  \n\n  \nIn the realm of advanced audio processing, such as in speech recognition, music generation, or audio classification, this approach of segmenting audio into tokens and processing them through neural networks has become increasingly common. It allows for capturing complex patterns and relationships in audio data, analogous to how text and images are processed in their respective domains.\n\n\n\n\n\nMel-Filterbanks and MFCCs\n\n\n\n\n  \n\n    \nEmpirical studies have shown that the human auditory system resolves frequencies non-linearly, and the non-linear resolution can be approximated using the Mel-scale which is given by\n\n\n\\[M(f)=1127.01048 \\bullet \\log _{e} f\\]\n\n    \n\n      \nwhere \\(f\\) is a frequency (Volkman, Stevens, & Newman, 1937).\n\n    \n\n  \n\n  \nThis indicates that the human auditory system is more sensitive to frequency difference in lower frequency band than in higher frequency band.\n\n  \nThe figure below illustrates the process of extracting Mel-frequency cepstrum coefficients (MFCCs) with triangular filters that are equally-spaced in Mel-scale. In the linear scale, note that as the frequency increases, the width of the filters increases.\n\n\n\n\n\n\n\n\n\n  \nAn input speech is transformed using the Discrete Fourier Transform, and the filter-bank energies (also called the Mel filter-bank energies or Mel spectrogram) are computed using triangular filters mentioned above. The log-values of the filter-bank energies (also called the log-Mel filterbanks or log-Mel spectrogram) are then decorrelated using the discrete cosine transform (DCT). Finally,  M-dimensional MFCCs are extracted by taking M-DCT coefficients, and passed on as speech features for downstream processing.\n\n  \nHowever, deep learning models are able to exploit spectro-temporal correlations, enabling the use of the log-Mel spectrogram (instead of MFCCs) as equivalent or better options, especially as seen with tasks that involve automatic speech recognition (ASR) and keyword spotting (KWS). As a result, a good number of ASR and KWS papers utilize log-Mel or Mel filterbank speech features with temporal context.\n\n  \nResearch has reported that using MFCCs is beneficial for both speaker and speech recognition. While MFCCs are still commonly fed as input speech features to neural nets. However, newer neural net architectures typically rely on the log-Mel filterbank energies (LMFBE) (which can be used to generate MFCCs after DCT-based decorrelation as indicated above).\n\n\n\n\n\nExample spectrogram and oscillogram\n\n\n\n\n\n\n\n  \nThis is an oscillogram and spectrogram of the boatwhistle call of the toadfish Sanopus astrifer. The upper blue-colored plot is an oscillogram presenting the waveform and amplitude of the sound over time, X-axis is Time (sec) and the Y-axis is Amplitude. The lower figure is a plot of the sounds’ frequency over time, X-axis is Time (sec) and the Y-axis is Frequency (kHz). The amount of energy present in each frequency is represented by the intensity of the color. The brighter the color, the more energy is present in the sound at that frequency. \n\n\n\n\n\nPerceptual Linear Prediction (PLP)\n\n\n\n\n  \nSimilar from MFCCs, perceptual linear prediction (PLP) features can also be fed into neural nets as input. PLP is a combination of spectral analysis and linear prediction analysis. It uses concepts from the psychophysics of hearing to compute a simple auditory spectrum.\n\n  \nRead more in \nH. Hermansky (1990)\n.\n\n\n\n\n\nProsodic Features\n\n\n\n\n  \nProsodic features include pitch and its dynamic variations, inter-pause statistics, phone duration, etc. (Shriberg, 2007)\n\n  \nVery often, prosodic features are extracted with larger frame size than acoustical features since prosodic features exist over a long speech segment such as syllables. The pitch and energy-contours change slowly compared to the spectrum, which implies that the variation can be captured over a long speech segment. Many pieces of literature have reported that prosodic features usually do not outperform acoustical features but incorporating prosodic features in addition to acoustical features can improve speaker recognition performance (Shriberg, 2007; Sönmez, Shriberg, Heck, & Weintraub, 1998; Peskin et al., 2003; Campbell, Reynolds, & Dunn, 2003; Reynolds et al., 2003).\n\n\n\n\n\nIdiolectal Features\n\n\n\n\n  \nThe idiolectal features are motivated by the fact that people usually use idiolectal information to recognize speakers.\n\n  \nIn telephone conversation corpus, Doddington (2001) reported enrolled speakers can be verified not using acoustic features that are extracted from a speech signal but using idiolectal features that are observed in true-underlying transcription of speech. The phonetic speaker verification, motivated by Doddington (2001)’s work, creates a speaker using his/her phone n-gram probabilities that are obtained using multiple-language speech recognizers (Andrews, Kohler, & Campbell, 2001).\n\n\n\n\n\nSpeech Processing Tasks\n\n\n\nArchitectural overview\n\n\n\n\n  \nA top-level overview of Siri architecture from \nDeep Learning in Speech Recognition by Alex Acero, Apple Inc.\n is shown below.\n\n\n\n\n\n\n\n\n\n  \nNote that this is an oversimplification of a complex system. Usually, the input data (which is pre-processed into speech features such as Mel-Filterbanks or MFCCs) gets fed as an input to the keyword spotting module which further gates two modules: (i) speaker recognition module, and (ii) automatic speech recognition (ASR) module. The encoder part of the ASR module can run in parallel with a language identification (LID) module which can select the language model correspond to the detected language which feeds the ASR decoder. The ASR output further feeds the natural language understanding (NLU) block which performs intent classification and slot filling. Dialog acts are generated next. Finally, the text-to-speech synthesis (TTS) block yields the end result: a response from the voice assistant. Note that the detected language also helps select which NLU and TTS block to use.\n\n  \nAlso note that the first phase of the above flow, keyword spotting, can be gated by a voice activity detector (VAD) which is usually a simple neural network with a couple of layers whose job is to just figure out if there’s speech or not in the audio it is listening to. This is usually an always-on on-device block. This helps save power by not having a more complex model like the keyword spotter run as an always-on system.\n\n\n\n\n\nFundamental speech tasks\n\n\n\n\n  \nThere four fundamental speech tasks, as applied to digital voice assistants:\n    \n\n      \nWake word detection/keyword spotting (KWS):\n On the device, detect the wakeword/trigger keyword to get the device’s attention;\n\n      \nAutomatic speech recognition (ASR):\n Upon detecting the wake word, convert audio streamed to the cloud into words;\n\n      \nNatural-language understanding (NLU):\n Extract the meaning of the recognized words so that the assistant can take the appropriate action in response to the customer’s request; and\n\n      \nText-to-speech synthesis (TTS):\n Convert the assistant’s textual response to the customer’s request into spoken audio.\n\n    \n\n  \n\n\n\n\n\nHierarchy of phones, words, and sentences\n\n\n\n\n  \n\n    \nThe input audio (typically stored as a wave-file, but could be another audio format as well, such as FLACC, OGG, etc.) is converted to a feature sequence \\((f_1, \\ldots, f_T)\\). The goal is to find a sequence of words \\((w_1, \\ldots, w_N)\\) that matches the feature sequence best.\n\n  \n\n  \n\n    \nAn HMM is used to model basic units (phones) in speech. To bridge the gap between phone-level HMMs and word decoding, let’s look at the hierarchical structure of HMM states, phones, words and sentences.\n\n    \n\n      \nA phone is usually modeled by a HMM with three states.\n The three states correspond to three subphones, which are \ntransition-in, steady-state, transition-out regions of the phone\n.\n\n      \nA word is consisted by a sequence of phones.\n For example, “one” is composed of three phones: \nw\n, \nah\n, and \nn\n. The word model is just the sequential concatenation of the phone models.\n\n      \nLexicon contains information about phone sequence of words.\n There is a dictionary (lexicon) which includes phone sequence for each of word. Therefore once all phones are represented by pre-trained HMM models, the word models become available by searching in the dictionary and concatenating phone level models belonging to that word. Compared with fitting a HMM for individual word, this strategy greatly reduced the complexity.\n\n      \nA sentence is a grammatically valid sequence of words.\n Words do not randomly connect to form a sentence. The transition between adjacent words can be estimated from a large text corpus.\n\n    \n\n  \n\n\n\n\n\nAutomatic Speech Recognition\n\n\n\nWhat is automatic speech recognition?\n\n\n\n\n  \nResearch in ASR (Automatic Speech Recognition) aims to enable computers to “understand” human speech and convert it into text. ASR is the next frontier in intelligent human-machine interaction and also a precondition for perfecting machine translation and natural language understanding. Research into ASR can be traced back to the 1950s in its initial isolated word speech recognition system. Since then, with persistent efforts of numerous scholars, ASR has made significant progress and can now power large-vocabulary continuous speech recognition systems.\n\n  \nEspecially in the emerging era of big data and application of deep neural networks, ASR systems have achieved notable performance improvements. ASR technology has also been gradually gaining practical use, becoming more product-oriented. Smart speech recognition software and applications based on ASR are increasingly entering our daily lives, in form of voice input methods, intelligent voice assistants, and interactive voice recognition systems for vehicles.\n\n\n\n\n\nFramework of an ASR System\n\n\n\n\n  \n\n    \nThe purpose of ASR is to map input waveform sequences to their corresponding word or character sequences. Therefore, implementing ASR can be considered a channel decoding or pattern classification problem. Statistical modeling is a core ASR method, in which, for a given speech waveform sequence \\(O\\), we can use a “maximum a posteriori” (MAP) estimator, based on the mode of a posterior Bayesian distribution, to estimate the most likely output sequence \\(W*\\), with the formula shown in the figure below.\n\n\n    \n\n\n    \n\n      \nwhere, \\(P(O \\mid W)\\) is the probability of generating the correct observation sequence, i.e. corresponding to the acoustic model (AM) of the ASR system, conditional on \\(W\\). Likelihood \\(P(W)\\) is the ‘a priori probability’ of the exact sequence \\(W\\) occurring. It is called the language model (LM).\n\n    \n\n  \n\n  \n\n    \nThe figure below shows the structure diagram of a marked ASR system, which mainly comprises a front-end processing module, acoustic model, language model, and decoder. The decoding process is primarily to use the trained acoustic model and language model to obtain the optimal output sequence.\n\n  \n\n\n\n\n\n\n\n\n\n  \nA version of the ASR system (with components such as pronunciation models, language models for re-scoring) from \nEnd-to-End Models for Speech Processing at Stanford by Navdeep Jaitly, NVIDIA\n is as below:\n\n\n\n\n\n\n\n\nAcoustic Model (Encoder)\n\n\n\n\n  \nAn acoustic model’s task is to compute \\(P(O \\mid W)\\), i.e. the probability of generating a speech waveform for the mode. An acoustic model, as an important part of the ASR system, accounts for a large part of the computational overhead and also determines the system’s performance. GMM-HMM-based acoustic models are widely used in traditional speech recognition systems.\n\n  \nIn this model, GMM is used to model the distribution of the acoustic characteristics of speech and HMM is used to model the time sequence of speech signals. Since the rise of deep learning in 2006, deep neural networks (DNNs) have been applied in speech acoustic models. In \nMohamed et al. (2009)\n, Hinton and his students used feedforward fully-connected deep neural networks in speech recognition acoustic modeling.\n\n  \nTypical pre-processing involves converting the audio to mono (similar to how computer vision models flatten the input image across the three channels, viz., R, G, B) and resampling it to 16kHz before it is input to the model.\n\n\n\n\n\nEncoder-Decoder Architectures: Past vs. Present\n\n\n\n\n  \nTraditional speech recognition systems adopt a GMM-HMM architecture where the GMM is the acoustic model that computes the state/observation probabilities and the HMM decoder combines these probabilities using dynamic programming (DP). With the advent of deep learning, the role played by the GMM as the acoustic model is now replaced by a DNN. Rather than the GMM modeling the observation probabilities, a DNN is trained to output them. The HMM still acts as a decoder. The figure below illustrates both approaches side-by-side.\n\n\n\n\n\n\n\n\n\n  \nThe DNN computes the observation probabilities and outputs a probability distribution over as many classes as the HMM states for each speech frame using a softmax layer. The number of HMM states depend of the number of phones, with a typical setup of 3 target labels for each phone for the beginning, middle and end of the segment, 1 state for silence, and 1 state for background.  The DNN is typically trained to minimize the average cross-entropy loss over all frames between the predicted and the ground-truth distributions. The HMM decoder computes the word detection score using the observation, the state transition, and the prior probabilities. An architectural overview of a DNN-HMM is shown in the diagram below.\n\n\n\n\n\n\n\n\n\n  \nCompared to traditional GMM-HMM acoustic models, DNN-HMM-based acoustic models perform better in terms of TIMIT database. When compared with GMM, DNN is advantageous in the following ways:\n    \n\n      \nDe-distribution hypothesis is not required for characteristic distribution when DNN models the posterior probability of the acoustic characteristics of speech.\n\n      \nGMM requires de-correlation processing for input characteristics, but DNN is capable of using various forms of input characteristics.\n\n      \nGMM can only use single-frame speech as inputs, but DNN is capable of capturing valid context information by means of splicing adjoining frames.\n\n    \n\n  \n\n  \nOnce the HMM model is built, to figure out the maximum a posteriori probability estimate of the most likely sequence of hidden states, i.e., the Viterbi path, a graph-traversal algorithm like the \nViterbi decoder algorithm\n (typically implemented using the concept of dynamic programming in algorithms) can be applied.\n\n  \nGiven that speech comprises of sequential data, RNNs (and it’s variants, GRUs and LSTMs) are natural choices for DNNs. However, for tasks such as speech recognition, where the alignment between the inputs and the labels is unknown, RNNs have so far been limited to an auxiliary role. The problem is that the standard training methods require a separate target for every input, which is usually not available. The traditional solution — the so-called hybrid approach — is to use Hidden Markov Models (HMMs) to generate targets for the RNN, then invert the RNN outputs to provide observation probabilities \n(Bourlard and Morgan, 1994)\n. However the hybrid approach does not exploit the full potential of RNNs for sequence processing, and it also leads to an awkward combination of discriminative and generative training.\n\n  \nThe connectionist temporal classification (CTC) output layer \n(Graves et al., 2006)\n removes the need for HMMs for providing alignment altogether by directly training RNNs to label sequences with unknown alignments, using a single discriminative loss function. CTC can also be combined with probabilistic language models for word-level speech and handwriting recognition. Note that the HMM-based decoder is still a good idea for cases where the task at hand involves a limited vocabulary and as a result, a smaller set of pronunciations, such as keyword spotting (vs. speech recognition).\n\n  \nUsing the CTC loss enables all-neural encoder-decoder seq2seq architectures that utilize an end-to-end neural architecture which generates phonemes at the output of the encoder as an intermediate step, which are consumed by a decoder which utilizes a language model and pronunciation model to generate transcriptions. The language model helps with secondary aspects of ASR such as punctuation, capitalization, and suggesting the right spellings based on the best word match.\n\n  \nRecent models in this area such as \nListen Attend Spell (LAS)\n forgo the intermediate phoneme labels altogether and train an end-to-end architecture that directly emits transcriptions at its output.\n\n  \nFor more in this area, Alex Graves’s book on \nSupervised Sequence Labelling with Recurrent Neural Networks\n is a great reference.\n\n\n\n\n\nPutting it all together\n\n\n\n\n  \nLet’s take an example of a simple digit recognition task. The words are the digits from one to nine. THe following hierarchical graph from \nSpeech and Language Processing by Jurafsky and Martin, 2008\n shows the hierarchical transitions of this task. \\(P(one \\| two)\\) represents the transition probability from digit two to one.\n\n\n\n\n\n\n\n\n\n  \nGiven this hierarchical transition matrix, a Viterbi trellis decoding method is used. The following figure from \nSpeech and Language Processing by Jurafsky and Martin, 2008\n shows the scheme of this decoding process. The words (digits) are stacked vertically and the feature sequence is shown horizontally.\n\n\n\n\n\n\n\n\nKeyword Spotting / Wakeword Detection\n\n\n\n\n  \nTo achieve always-on keyword spotting, running a complex neural network all the time is a recipe for battery drain.\n\n  \nThe trick is to use a two-pass system by breaking down the problem into two pieces: (i) low-compute (running on an always-on processor), and (ii) more accurate (running on the main processor) as shown below:\n\n\n\n\n\n\n\n\n\n  \nThe first low-compute piece typically runs on a special always-on processor (AOP) which is named so because it does not enter sleep states (unlike the main processor) and is always-on.\nThe trade-off is that it is much less powerful than the main processor so you can only do small number of computations if you don’t want to impact the battery life.\n\n  \nThe second more accurate piece running on the main processor consumes high power but only uses that power for a very limited amount of time when you actually want it to compute something, i.e., when it is not sleeping. The rest of the time it goes to several levels of sleep and most of the time the main processor is completely asleep.\n\n  \nThus, we have a two layer system similar to a processor cache hierarchy. The first level is running all the time with a very simple DNN and then if it is confident with its predictions, it does not wake up the main system. However, if it is unsure of its predictions, it wakes up the main processor which runs a bigger, fancier DNN that is more accurate.\n\n  \nNote that both passes share the same set of computed speech features which are typically generated on the AOP.\n\n  \nThe level-1/first-stage thresholds are tuned to trade-off false-reject rates for high false-accept rates so we don’t easily miss an event where the user was looking to interact with the device due to it not waking up the second-stage (negatives/rejects do not wake up stage-2). Since the level-2/second-stage model is a beefier model, it can mitigate false-accepts easily. It is also important to make sure the precision is not too low as to wake up the level-2/second-stage model (also known as the “checker” model) frequently, and eventually lead to a battery drain.\n    \n\n      \nThis requires the level-1 stage to have high recall with modest precision to ensure that the voice assistant doesn’t miss a true positive. At the same time, the level-1 stage needs to optimize for speed to ensure we deliver a great set of candidates and satisfy the strict latency requirements of the system.\n\n      \nThe level-2 stage, on the other hand, has high precision with modest recall to ensure a smooth user-experience by ensuring that the voice-assistant minimizes false positives (and avoids waking up frequently due to excessive false positives). The level-2 stage can also incorporate a more complex architecture; since the level-2 stage is only invoked on a handful of instances compared to level-1, the latencies involved with such complex architectures are usually manageable.\n\n      \nThis paradigm is similar to how recommender systems optimize for recall during their retrieval stage and precision during their ranking stage.\n\n    \n\n  \n\n\n\n\n\nMeasuring Performance\n\n\n\n\n  \nEvaluating performance in keyword spotting involves several key metrics, primarily focusing on Precision, Recall, F1-score, and Detection Error Trade-off (DET) curves. Each metric provides a distinct perspective on the system’s performance and effectiveness in different operational scenarios.\n\n\n\n\n\nPrecision\n\n\n\n\n  \nPrecision measures the accuracy of the model in identifying wakewords. It indicates how many of the model’s wakeword detections are correct, calculated as:\n\n\n\n\n\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]\n\n\n\n  \nA high precision value ensures fewer false activations, conserving device power and enhancing user experience by reducing unintended interactions.\n\n\n\n\n\nRecall\n\n\n\n\n  \nRecall assesses the reliability of the model in detecting actual wakeword occurrences. It shows the proportion of actual wakeword instances the model successfully identifies, calculated as:\n\n\n\n\n\\[\\text{Recall} = \\frac{TP}{TP + FN}\\]\n\n\n\n  \nHigh recall ensures the voice assistant is consistently responsive, minimizing missed interactions and improving user satisfaction.\n\n\n\n\n\nPrecision-Recall (PR) Curves\n\n\n\n\n  \nPrecision-Recall curves visually illustrate the trade-off between precision and recall at various threshold settings. By plotting precision against recall, developers can analyze system behavior and select optimal operational thresholds that suit specific precision and recall requirements.\n\n  \nHowever, PR curves have limitations in keyword spotting scenarios, especially when evaluating systems where false positives and false negatives must both be strictly managed.\n\n\n\n\n\nF1-score\n\n\n\n\n  \nThe F1-score provides a balanced evaluation by combining precision and recall into a single metric. It is especially useful in scenarios where balancing false positives and false negatives is critical. The F1-score is the harmonic mean of precision and recall:\n\n\n\n\n\\[F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\n\n\n\n  \nOptimizing the F1-score helps maintain a balanced performance between accuracy and reliability.\n\n\n\n\n\nDetection Error Trade-off (DET) Curves\n\n\n\n\n  \n\n    \nDET curves offer a comprehensive visual assessment of keyword spotting system performance, particularly highlighting the trade-offs between false alarm rates and missed detection rates across varying thresholds. Unlike precision-recall curves, DET curves specifically plot the False Negative Rate (FNR) against the False Positive Rate (FPR) on a normal deviate scale.\n\n  \n\n  \n\n    \nFalse Negative Rate (FNR)\n: Probability of missing the wakeword (i.e., not waking the device when the wakeword is spoken), calculated as:\n\n  \n\n\n\n\n\\[\\text{FNR} = \\frac{FN}{TP + FN}\\]\n\n\n\n  \nFalse Positive Rate (FPR)\n: Probability of falsely detecting a wakeword when it is not present, calculated as:\n\n\n\n\n\\[\\text{FPR} = \\frac{FP}{FP + TN}\\]\n\n\n\n  \n\n    \nDET curves are generally preferred for keyword spotting tasks due to several advantages:\n\n\n    \n\n      \nSymmetric Visualization\n: DET curves equally emphasize both false positives and false negatives, enabling balanced optimization.\n\n      \nClear Operational Trade-offs\n: DET curves provide a clearer depiction of trade-offs critical for user experience and battery efficiency.\n\n      \nDirect Threshold Selection\n: They facilitate intuitive threshold tuning aligned with real-world performance requirements, directly addressing operational priorities.\n\n    \n\n  \n\n  \n\n    \nBy employing DET curves alongside precision, recall, and F1-score, developers can holistically assess and optimize keyword spotting systems, ensuring effective balance across responsiveness, accuracy, and power efficiency.\n\n  \n\n\n\n\n\nOnline learning with live data to improve robustness\n\n\n\n\n  \nTo improve the robustness of a speech model such as ASR, Keyword Spotting etc., we can train it on online data gathered from the field. To do so, we run the classifier with a very low threshold, for instance, in case of keyword spotting, if the usual detection threshold is 0.8, we run it at a threshold of 0.5. This leads to the keyword spotter firing on a lot of keywords – most of them being false positives and the others being true positives. This is because for a classifier, lower the classification threshold, more the number of false positives, while higher the classification threshold, more the number of false negatives.\n\n  \nNext, we manually sift through the results and identify the true positives and use them for re-training the classifier. The false positives, in turn, are used as hard negatives to also re-train the classifier and thus, make it more robust to erroneous scenarios involving false accepts.\n\n\n\n\n\nHandling dynamic language switching and code switching\n\n\n\n\n  \nIn order to support such dynamic language switching from one interaction to the next (which is different from code switching, where words from more than one language are used within the same interaction or utterance), the backend intelligence must perform language identification (LID) along with automatic speech recognition (ASR) so that the detected language can be used to trigger appropriate downstream systems such as natural language understanding, text-to-speech synthesis, etc., which usually are language specific.\n\n  \nTo enable dynamic language switching between two or more pre-specified languages, a commonly adopted strategy is to run several monolingual ASR systems in parallel along with a standalone acoustic LID module, and pass only one of the recognized transcripts downstream depending on the outcome of language detection. While this approach works well in practice, it is neither cost-effective for more than two languages, nor suitable for on-device scenarios where compute resources and memory are limited. To this end, \nJoint ASR and Language Identification Using RNN-T: An Efficient Approach to Dynamic Language Switching (2021)\n proposed all-neural architectures that can jointly perform ASR and LID for a group of pre-specified languages and thereby significantly improve the cost effectiveness and efficiency of dynamic language switching. Joint ASR-LID modeling, by definition, involves multilingual speech recognition. Multilingual ASR models are trained by pooling data from the languages of interest, and it is often observed that languages with smaller amounts of data (i.e., low-resource languages) benefit more from this.\n\n\n\n\n\nSpeaker Recognition\n\n\n\n\n  \nSpeaker recognition can be classified into (i) speaker verification, and (ii) speaker identification. Speaker verification aims to verify whether an input speech corresponds to the claimed identity. Speaker identification aims to identify an input speech by selecting one model from a set of enrolled speaker models. In some cases, speaker verification follows speaker identification in order to validate the identification result.\n\n  \nSpeaker verification is gauged using the EER metric on the DET-curve while speaker identification is gauged using accuracy.\n\n  \ni-vectors\n by Dehak et al. in 2010 were the leading technology behind speaker recognition, up until DNNs took over with \nd-vectors\n by Variani et al. in 2014. The latest in speaker recognition is \nx-vectors\n by Snyder et al. in 2018 which proposed using data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The figure below shows the embedding extraction process with d-vectors:\n\n\n\n\n\n\n\n\n\n  \nRead \nmore\n on speaker recognition in this book chapter by Jin and Yoo from KAIST.\n\n\n\n\n\nData augmentation\n\n\n\nTime domain\n\n\n\nGain\n\n\n\n\n  \nGain augmentation can be applied by simply multiplying the raw audio by a scalar.\n\n\n\n\n\nTypes of noise\n\n\n\nReverberation noise / room impulse response (RIR)\n\n\n\n\n  \nReverberation noise, also known as room impulse response (RIR), is produced when a sound source stops within an enclosed space. Sound waves continue to reflect off the ceiling, walls and floor surfaces until they eventually die out. These reflected sound waves are known as reverberation.\n\n\n\n\n\nBabble noise\n\n\n\n\n  \nBabble noise is considered as one of the best noises for masking speech. Refer \nBabble Noise: Modeling, Analysis, and Applications\n.\n\n\n\n\n\nSpeed and pitch\n\n\n\n\n  \nThese augmentations, while slow to run, are mainly used to artificially increase the number of training speakers.\n\n\n\n\n\nCodec augmentation\n\n\n\n\n  \nTranscoding audio in a new codec can increase diversity in the training dataset, however it is extremely slow and we expect Alexa data to have limited variation in used codecs.\n\n\n\n\n\nMixup\n\n\n\n\n  \nMixup has shown promising results on small datasets with a limited number of utterances per training speaker.\n\n\n\n\n\nDynamic Range Compression\n\n\n\n\n  \nWe do expect most Alexa recordings to have a constant volume due to the audio preprocessing. Therefore we expect that introducing or suppressing volume variations within a recording to have a limited impact on real world data.\n\n\n\n\n\nFrequency domain\n\n\n\nSpecAugment\n\n\n\n\n  \nThis paper by \nPark et al. (2019)\n from Google presents SpecAugment, a simple data augmentation method for speech recognition.\n\n  \nSpecAugment greatly improves the performance of ASR networks. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. They apply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end speech recognition tasks.\n\n  \nThey achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks on end-to-end LAS networks by augmenting the training set using simple handcrafted policies, surpassing the performance of hybrid systems even without the aid of a language model. SpecAugment converts ASR from an over-fitting to an under-fitting problem, and they are able to gain performance by using bigger networks and training longer. On LibriSpeech, they achieve 6.8% WER (more on WER in the section on \nEvaluation metrics\n) on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, they achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5’00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.\n\n\n\n\n\nEvaluation metrics\n\n\n\nPrecision and Recall\n\n\n\n\n  \nIn case of an imbalanced dataset scenario, precision and recall are appropriate performance metrics. Precision and recall are typically juxtaposed together when reported.\n\n  \nPrecision is defined as the fraction of relevant instances among all retrieved instances.\n\n  \nRecall, sometimes referred to as \nsensitivity\n, is the fraction of retrieved instances among all relevant instances.\n\n  \nNote that precision and recall are computed for each class. They are commonly used to evaluate the performance of classification or information retrieval systems.\n\n\n\n\n\n\n  \nA perfect classifier has precision and recall both equal to 1.\n\n\n\n\n\n\n  \nIt is often possible to calibrate the number of results returned by a model and improve precision at the expense of recall, or vice versa.\n\n  \nPrecision and recall should always be reported together and are not quoted individually. This is because it is easy to vary the sensitivity of a model to improve precision at the expense of recall, or vice versa.\n    \n\n      \nAs an example, imagine that the manufacturer of a pregnancy test needed to reach a certain level of precision, or of specificity, for FDA approval. The pregnancy test shows one line if it is moderately confident of the pregnancy, and a double line if it is very sure. If the manufacturer decides to only count the double lines as positives, the test will return far fewer positives overall, but the precision will improve, while the recall will go down. This shows why precision and recall should always be reported together.\n\n    \n\n  \n\n  \nThe figure below (taken from the Wikipedia article on \nprecision and recall\n) shows a graphical representation of precision and recall:\n\n\n\n\n\n\n\n\n\n  \nFormally, precision and recall can be defined as:\n    \n\n      \nPrecision\n: Out of all the samples marked positive, how many were actually positive (i.e., the true positives)?\n\n      \nRecall\n: Out of all the samples that are actually positive, how many were marked positive (i.e., the true positives)?\n\n      \nFrom the above definitions it is clear that with PR metrics, the focus is on the positive class (also called the relevant class).\n\n    \n\n  \n\n  \n\n    \nIn the section on \nPrecision-Recall (PR) Curves\n, we explore how to get the best out of these two metrics using PR curves.\n\n  \n\n  \nKey takeaways\n\n    \n\n      \nPrecision: how many \nselected\n items are \nrelevant\n?\n\n      \nRecall: how many \nrelevant\n items are \nselected\n?\n\n    \n\n  \n\n\n\n\n\nHistorical Background\n\n\n\n\n  \nThis section is optional and offers a historical walk-through of how precision, recall and F1-score came about, so you may skip to the next section if so desired.\n\n  \nPrecision and recall were first defined by the American scientist Allen Kent and his colleagues in their 1955 paper Machine literature searching VIII. Operational criteria for designing information retrieval systems.\n\n  \nKent served in the US Army Air Corps in World War II, and was assigned after the war by the US military to a classified project at MIT in mechanized document encoding and search.\n\n  \nIn 1955, Kent and his colleagues Madeline Berry, Fred Luehrs, and J.W. Perry were working on a project in information retrieval using punch cards and reel-to-reel tapes. The team found a need to be able to quantify the performance of an information retrieval system objectively, allowing improvements in a system to be measured consistently, and so they published their definition of precision and recall.\n\n  \nThey described their ideas as a theory underlying the field of information retrieval, just as the second law of thermodynamics “underlies the design of a steam engine, regardless of its type or power rating”.\n\n  \nSince then, the definitions of precision and recall have remained fundamentally the same, although for search engines the definitions have been modified to take into account certain nuances of human behavior, giving rise to the modified metrics precision @ \\(k\\) and mean average precision (mAP), which are the values normally quoted in information retrieval contexts today.\n\n  \nIn 1979, the Dutch computer science professor Cornelis Joost van Rijsbergen recognized the problems of defining search engine performance in terms of two numbers and decided on a convenient scalar function that combines the two. He called this metric the Effectiveness function and assigned it the letter E. This was later modified to the \\(F_1\\) score, or \\(F_{\\beta}\\) score, which is still used today to summarize precision and recall.\n\n\n\n\n\nExamples\n\n\n\n\n  \nPrecision and recall can be best explained using examples. Consider the case of evaluating how well does a robot sifts good apples from rotten apples. A robot looks into the basket and picks out all the good apples, leaving the rotten apples behind, but is not perfect and could sometimes mistake a rotten apple for a good apple orange.\n\n  \nAfter the robot finishes picking the good apples, precision and recall can be calculated as:\n    \n\n      \nPrecision\n: number of good apples picked out of all the picked apples.\n\n      \nRecall\n: number of good apples picked out of all possible good apples.\n\n    \n\n  \n\n  \nPrecision\n is about \nexactness\n, classifying only one instance correctly yields 100% precision, but a very low recall, it tells us how well the system identifies samples from a given class.\n\n  \nRecall\n is about \ncompleteness\n, classifying all instances as positive yields 100% recall, but a very low precision, it tells how well the system does and identify all the samples from a given class.\n\n  \nAs another example, consider the task of information retrieval. As such, precision and recall can be calculated as:\n    \n\n      \nPrecision\n: number of relevant documents retrieved out of all retrieved documents.\n\n      \nRecall\n: number of relevant documents retrieved out of all relevant documents.\n\n    \n\n  \n\n\n\n\n\nApplications\n\n\n\n\n  \nPrecision and recall are measured for every possible class in your dataset. So, \nprecision\n and \nrecall\n metrics are relatively much more appropriate (especially compared to accuracy) when dealing with \nimbalanced classes\n.\n\n\n\n\n\n\n  \nAn important point to note is that PR are able to handle class imbalance in scenarios where the positive class (also called the minority class) is rare. If, however, the dataset is imbalanced in such a way that the negative class is the one that’s rare, PR curves are sub-optimal and can be misleading. In these cases, ROC curves might be a better fit.\n\n\n\n\n\n\n  \nSo when do we use PR metrics? Here’s the typical use-cases:\n    \n\n      \nWhen two classes are equally important\n: PR would be the metrics to use if the goal of the model is to perform equally well on both classes. Image classification between cats and dogs is a good example because the performance on cats is equally important on dogs.\n\n      \nWhen minority class is more important\n: PR would be the metrics to use if the focus of the model is to identify correctly as many positive samples as possible. Take spam detectors for example, the goal is to find all the possible spam emails. Regular emails are not of interest at all — they overshadow the number of positives.\n\n    \n\n  \n\n\n\n\n\nFormulae\n\n\n\n\n  \n\n    \nMathematically, precision and recall are defined as,\n\n\n\\[\\operatorname {Precision}=\\frac{TP}{TP + FP} \\\\\\]\n\n\\[\\operatorname{Recall}=\\frac{TP}{TP + FN} \\\\\\]\n\n    \n\n      \nwhere,\n        \n\n          \n\\(TP\\) is the True Positive Rate, i.e., the number of instances which are relevant and which the model correctly identified as relevant.\n\n          \n\\(FP\\) is the False Positive Rate, i.e., the number of instances which are not relevant but which the model incorrectly identified as relevant.\n\n          \n\\(FN\\) is the false negative rate, i.e., the number of instances which are relevant and which the model incorrectly identified as not relevant.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n  \n\n    \nIn the content of the robot sifting good apples from the rotten ones,\n\n\n\\[\\operatorname {Precision}=\\frac{\\text { # of picked good apples }}{\\text { # of picked apples }}\\]\n\n\\[\\operatorname{Recall}=\\frac{\\text { # of picked good apples }}{\\text { # of good apples }}\\]\n  \n\n  \n\n    \nIn the context of information retrieval,\n\n\n\\[\\operatorname {Precision}=\\frac{\\text { retrieved relevant documents }}{\\text { all retrieved documents }}\\]\n\n\\[\\operatorname{Recall}=\\frac{\\text { retrieved relevant documents }}{\\text { all relevant documents }}\\]\n  \n\n\n\n\n\nPrecision/Recall Tradeoff\n\n\n\n\n  \nDepending on the problem at hand, you either care about high precision or high recall.\n\n  \nExamples of high precision:\n    \n\n      \nFor a model that detects shop lifters, the focus should be on developing a high precision model by reducing false positives (note that precision is given by \\(\\frac{TP}{TP+FP}\\) and since \\(FP\\) features in the denominator, reducing \\(FP\\) leads to high precision). This implies that if we tag someone as a shop lifter, we’d like to make sure we do so with high confidence.\n\n    \n\n  \n\n  \nExamples of high recall:\n    \n\n      \nIn an adult content detection problem, the focus should be on developing a high recall model by reducing false negatives (note that recall is given by \\(\\frac{TP}{TP+FN}\\) and since \\(FN\\) features in the denominator, reducing \\(FN\\) leads to high recall). This implies that if the model classified a video as good for kids (i.e., not having adult content), it should be marked so with high confidence.\n\n      \nIn a disease detection scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model classified a patient as not having the disease, it should be done do with high confidence else it can prove fatal.\n\n      \nIn an autonomous car driving scenario, the focus should be on developing a high recall model by reducing false negatives. This implies that if the model determined that there was no obstacle in the car’s surrounding radius, it should be done do with high confidence else fatalities can occur.\n\n    \n\n  \n\n  \nOften, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. This is called the precision/recall tradeoff. However, in some scenarios, it is important to strike the right balance between both:\n    \n\n      \nAs an example (from the Wikipedia article on \nPrecision and Recall\n), brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumour cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).\n\n    \n\n  \n\n\n\n\n\nReceiver Operating Characteristic (ROC) Curve\n\n\n\n\n  \nSuppose we have the probability prediction for each class in a multiclass classification problem, and as the next step, we need to calibrate the threshold on how to interpret the probabilities. Do we predict a positive outcome if the probability prediction is greater than 0.5 or 0.3? The Receiver Operating Characteristic (ROC) curve ROC helps answer this question.\n\n  \nAdjusting threshold values like this enables us to improve either precision or recall at the expense of the other. For this reason, it is useful to have a clear view of how the False Positive Rate and True Positive Rate vary together.\n\n  \nThe ROC curve shows the variation of the error rates for all values of the manually-defined threshold. The curve is a plot of the \nFalse Positive Rate (also called the False Acceptance Rate) on the X-axis\n versus the \nTrue Positive Rate on the Y-axis\n for a number of different candidate threshold values between 0.0 and 1.0. A data analyst may plot the ROC curve and choose a threshold that gives a desirable balance between the false positives and false negatives.\n    \n\n      \nFalse Positive Rate (also called the False Acceptance Rate) on the X-axis\n: the False Positive Rate is also referred to as the inverted specificity where specificity is the total number of true negatives divided by the sum of the number of true negatives and false positives.\n\n    \n\n\n\\[\\textrm{False Positive Rate} = \\frac{FP}{\\text{number of negatives}} = \\frac{FP}{FP+TN}\\]\n\n    \n\n      \nTrue Positive Rate on the Y-axis\n: the True Positive Rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.\n\n    \n\n\n\\[\\textrm{True Positive Rate} = \\frac{FP}{\\text{number of positives}} = \\frac{TP}{TP+FN}\\]\n  \n\n  \nNote that both the False Positive Rate and the True Positive Rate are calculated for different probability thresholds.\n\n  \nAs another example, if a search engine assigns a score to all candidate documents that it has retrieved, we can set the search engine to display all documents with a score greater than 10, or 11, or 12. The freedom to set this threshold value generates a smooth curve as below. The figure below shows a ROC curve for a binary classifier with AUC = 0.93. The orange line shows the model’s false positive and false negative rates, and the dotted blue line is the baseline of a random classifier with zero predictive power, achieving AUC = 0.5.\n\n\n\n\n\n\n\n\n\n  \nNote that another way to obtain FPR and TPR is through TNR and FNR respectively, as follows:\n\n\n\n\n\\[FPR = 1 - TNR \\\\\nTPR = 1 - FNR\\]\n\n\n\n  \nNote that the equal error rate (EER) in ROC curves is obtained as follows:\n\n\n\n\n\n\n\n\n\n  \nAlso, the \\(y = x\\) line in the ROC curve signifies the performance of a random classifier (so we need our curve to nudge towards the top-left to yield better AUC and thus better performance than a random classifier):\n\n\n\n\n\n\n\n\nDetection error tradeoff (DET) curve\n\n\n\n\n  \nA detection error tradeoff (DET) curve is a graphical plot of error rates for binary classification systems, plotting the false rejection rate (FRR) vs. false acceptance rate (FAR) for different probability thresholds.\n\n  \nThe X- and Y-axes are scaled non-linearly by their standard normal deviates (or just by logarithmic transformation), yielding tradeoff curves that are more linear than ROC curves, and use most of the image area to highlight the differences of importance in the critical operating region.\n\n\n\n\n\nComparing ROC and DET curves\n\n\n\n\n  \n\n    \nLet’s compare receiver operating characteristic (ROC) and detection error tradeoff (DET) curves for different classification algorithms for the same classification task.\n\n  \n\n  \n\n    \nDET curves are commonly plotted in normal deviate scale. To achieve this the DET display transforms the error rates as returned by sklearn’s \ndet_curve\n and the axis scale using \nscipy.stats.norm\n.\n\n  \n\n  \n\n    \nThe point of this example is to demonstrate two properties of DET curves, namely:\n\n\n    \n\n      \n\n        \nIt might be easier to visually assess the overall performance of different classification algorithms using DET curves over ROC curves. Due to the linear scale used for plotting ROC curves, different classifiers usually only differ in the top left corner of the graph and appear similar for a large part of the plot. On the other hand, because DET curves represent straight lines in normal deviate scale. As such, they tend to be distinguishable as a whole and the area of interest spans a large part of the plot.\n\n      \n\n      \n\n        \nDET curves give the user direct feedback of the detection error tradeoff to aid in operating point analysis. The user can deduct directly from the DET-curve plot at which rate false-negative error rate will improve when willing to accept an increase in false-positive error rate (or vice-versa).\n\n      \n\n    \n\n  \n\n  \n\n    \nThe plots in this example compare the ROC curve on the left with the corresponding DET curve on the right. There is no particular reason why these classifiers have been chosen for the example plot over other classifiers available in scikit-learn.\n\n  \n\n\n\n\n\n\n\n\n\n  \nNote that the equal error rate (EER) in DET curves is the intersection of the \\(y = x\\) line with the DET curve:\n\n\n\n\n\n\n\n\n\n  \nTo generate DET curves using scikit-learn:\n\n\n\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nfrom\n \nsklearn.metrics\n \nimport\n \ndet_curve\n\n\n\ny_true\n \n=\n \nnp\n.\narray\n([\n0\n,\n \n0\n,\n \n1\n,\n \n1\n])\n\n\ny_scores\n \n=\n \nnp\n.\narray\n([\n0.1\n,\n \n0.4\n,\n \n0.35\n,\n \n0.8\n])\n\n\n\nfpr\n,\n \nfnr\n,\n \nthresholds\n \n=\n \ndet_curve\n(\ny_true\n,\n \ny_scores\n)\n\n\n# fpr: \t\t\tarray([0.5, 0.5, 0. ])\n# fnr: \t\t\tarray([0. , 0.5, 0.5])\n# thresholds: \tarray([0.35, 0.4 , 0.8 ])\n\n\n\n\n\n  \n\n    \nNote the formulae to obtain FAR (FPR) and FRR (FNR):\n\n\n\\[FAR = FPR = \\frac{FP}{\\text{number of negatives}} = \\frac{FP}{FP + TN} \\\\\n  FRR = FNR = \\frac{FP}{\\text{number of positives}} = \\frac{FN}{FN + TP}\\]\n\n    \n\n      \nwhere, FP: False positive; FN: False Negative; TN: True Negative; TP: True Positive\n\n    \n\n  \n\n  \n\n    \nAnother way to obtain FAR and FRR is through TNR and TPR respectively, as follows:\n\n  \n\n\n\n\n\\[FAR = 1 - TNR \\\\\nFRR = 1 - TPR\\]\n\n\nArea under the ROC Curve (AUC)\n\n\n\n\n  \nThe area under the ROC curve (AUROC) is a good metric for measuring the classifier’s performance. This value is normally between 0.5 (for a bad classifier) and 1.0 (a perfect classifier). The better the classifier, the higher the AUC and the closer the ROC curve will be to the top left corner.\n\n\n\n\n\nPrecision-Recall Curve\n\n\n\n\n  \nWe saw \nearlier\n that when a dataset has imbalanced classes, precision and recall are better metrics than accuracy. Similarly, for imbalanced classes, a Precision-Recall curve is more suitable than a ROC curve.\n\n  \nA Precision-Recall curve is a plot of the \nPrecision\n (y-axis) and the \nRecall\n (x-axis) for different thresholds, much like the ROC curve. Note that in computing precision and recall there is never a use of the true negatives, these measures only consider correct predictions.\n\n\n\n\n\nArea Under the PR Curve (AUC)\n\n\n\n\n  \nSimilar to the \nAUROC\n, the PR AUC summarizes the curve with a range of threshold values as a single score.\n\n  \nThe score can then be used as a point of comparison between different models on a binary classification problem where a score of 1.0 represents a model with perfect skill.\n\n\n\n\n\nKey takeaways: Precision, Recall and ROC/PR Curves\n\n\n\n\n  \nROC Curve\n: summaries the trade-off between the True Positive Rate and False Positive Rate for a predictive model using different probability thresholds.\n\n  \nPrecision-Recall Curve\n: summaries the trade-off between the True Positive Rate and the positive predictive value for a predictive model using different probability thresholds.\n\n  \nIn the same way it is better to rely on \nprecision\n and \nrecall\n rather than \naccuracy\n in an imbalanced dataset scenario (since it can offer you an incorrect picture of the classifier’s performance), a Precision-Recall curve is better to calibrate the probability threshold compared to the ROC curve. In other words, ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases, the area under the curve (AUC) can be used as a summary of the model performance.\n\n\n\n\n\n\n\n\n \n\n\n\n\nMetric\n\n\nFormula\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAccuracy\n\n\n$$\\frac{TP+TN}{TP+TN + FP+FN}$$\n\n\nOverall performance of model\n\n\n\n\n\n\nPrecision\n\n\n$$\\frac{TP}{TP + FP}$$  \n\n\nHow accurate the positive predictions are\n\n\n\n\n\n\nRecall/Sensitivity\n\n\n$$\\frac{TP}{TP + FN}$$\n\n\nCoverage of actual positive sample\n\n\n\n\n\n\nSpecificity\n\n\n$$\\frac{TN}{TN + FP}$$\n\n\nCoverage of actual negative sample\n\n\n\n\n\n\nF1-score\n\n\n$$2 \\times\\frac{\\textrm{Precision} \\times \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}}$$\n\n\nHarmonic mean of Precision and Recall\n\n\n\n\n\n\n\n\n\n\n\nSpeech Recognition\n\n\n\n\n  \nWord Error Rate (WER)\n\n  \nCharacter Error Rate (CER)\n\n\n\n\n\nSpeech-to-Speech Machine Translation\n\n\n\n  \nBLEU (BiLingual Evaluation Understudy)\n\n  \nMETEOR (Metric for Evaluation of Translation with Explicit ORdering)\n\n\n\n\n\nManual evaluation by humans for fluency, grammar, comparative ranking, etc.\n\n\n\n  \nMean Opinion Score (MOS)\n\n\n\n\n\n🤗 Open ASR Leaderboard\n\n\n\n\n  \nThe \nHugging Face ASR Leaderboard\n ranks and evaluates speech recognition models on the Hugging Face Hub.\n\n  \nThey report the Average \nWER\n and \nRTF\n – lower the better. Models are ranked based on their Average WER, from lowest to highest.\n\n\n\n\n\n\n\n\nPopular Models\n\n\n\nDistil-Whisper\n\n\n\n\n  \nHuggingFace’s \ndistil-whisper\n: Distilled OpenAI’s Whisper on 20,000 hours of open-sourced audio data.\n\n  \nDistil-Whisper is significantly smaller and faster (5.8 times faster, 51% fewer parameters) than the original Whisper model, maintaining similar performance (within 1% WER) on out-of-distribution test data in a zero-shot setting.\n\n  \nThe authors use a large-scale pseudo-labelling approach to assemble an open-source dataset for training, selecting high-quality pseudo-labels based on a word error rate (WER) heuristic.\n\n  \nThe motivation was the fact that OpenAI’s Whisper yields astonishing accuracy for most audio, but it’s too slow and expensive for most production use cases. In addition, it has a tendency to hallucinate.\n\n  \nEncoding takes \\(O(1)\\) passes while decoding takes \\(O(N)\\). This implies that reducing decoder layers is \\(N\\) time more effective. They kept the whole encoder, but utilized only two decoder layers.\n\n  \nThe encoder is frozen during distillation to ensure Whisper’s robustness to noise is kept.\n\n  \nThe model demonstrates improved robustness against hallucination errors in long-form audio, and its design allows it to be paired with Whisper for speculative decoding, doubling the inference speed while maintaining output accuracy.\n\n  \nThe paper highlights the utility of large-scale pseudo-labelling in speech recognition and the effectiveness of the WER threshold filter in distillation. The training and inference code, along with the models, are made publicly available by the authors.\n\n  \nTo make sure Distil-Whisper does not inherit hallucinations, they filtered out all data samples below a certain WER threshold. By doing so, we were able to reduce hallucinations and actually beat the teacher on long-form audio evaluation.\n\n  \nThe checkpoints are \nhere\n and also directly \navailable\n in 🤗 Transformers. All with MIT License.\n\n\n\n\n\n\n\n\nParakeet\n\n\n\n\n  \nParakeet\n is jointly developed by NVIDIA NeMo and \nSuno.ai\n teams. It is up to 10x faster and 30% more accurate than Whisper models!\n\n  \nParakeet is an XXL version of FastConformer Transducer (around 1.1B parameters) model that only supports the English language. The model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams using NeMo toolkit for over several hundred epochs.\n\n\n\n\n\nCanary\n\n\n\n\n  \nCanary-1B, with 1 billion parameters, is trained to excel in automatic speech-to-text recognition (ASR) and translation tasks, supporting English, German, French, and Spanish.\n\n  \nModel Highlights:\n\n    \n\n      \nLanguage Support:\n Fluent in 4 languages for ASR and capable of translating between English and German/French/Spanish, including punctuation and capitalization control.\n\n      \nArchitecture:\n An innovative encoder-decoder setup combining a FastConformer encoder with a Transformer Decoder, ensuring efficient and accurate text generation.\n\n      \nEase of Use:\n Leveraging concatenated SentencePiece tokenizers for scalability and simplicity in language expansion.\n\n    \n\n  \n\n  \nGetting Started with NeMo:\n\n    \n\n      \nTo harness the power of Canary-1B, NVIDIA recommends installing the NeMo toolkit post-Cython and the latest PyTorch version. This toolkit not only facilitates model training and fine-tuning but also simplifies the inference process for developers and researchers.\n\n    \n\n  \n\n  \nUsage and Performance:\n\n    \n\n      \nCanary-1B shines in both ASR and AST (automatic speech-to-text translation), providing exceptional accuracy and versatility. Whether you’re translating speeches or transcribing multi-lingual audio files, Canary-1B delivers with precision. It’s trained on a diverse dataset totaling 85k hours of speech, ensuring robust performance across its supported languages.\n\n    \n\n  \n\n  \nDemo\n; \nModel\n\n\n\n\n\nSuggested Videos\n\n\n\n\n  \nDeep Learning in Speech Recognition by Alex Acero, Apple Inc.\n\n  \nEnd-to-End Models for Speech Processing at Stanford by Navdeep Jaitly, NVIDIA\n\n  \nSpeech Recognition (ASR) by Andrew Senior, Google\n\n  \nText to Speech (TTS) by Andrew Senior, Google\n\n  \nAutomatic Speech Recognition: An Overview by Preethi Jyothi, IIT Bombay\n\n  \nMachine Learning and Causal Inference by Susan Athey, Stanford\n\n\n\n\n\nReferences\n\n\n\n\n  \nSpeech Technology - Kishore Prahallad\n\n  \nSpeech and Language Processing by Jurafsky and Martin, 2nd Edition, 2008\n\n  \nInterspeech 2017 Series Acoustic Model for Speech Recognition Technology\n\n  \nWhat’s the difference between a phoneme, a phone, and an allophone?\n\n  \nSpeaker Verification and Identification\n\n  \nUnderstanding automatic speech recognition (ASR)\n\n  \nGap between DNN and HMM\n\n\n\n\n\nAcknowledgements\n\n\n\n\n  \nThanks to \nBharani Kempaiah\n for helping proof-read this article and offering suggestions for improvement.\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledSpeechProcessing,\n  title   = {Speech Processing},\n  author  = {Chadha, Aman and Kempaiah, Bharani},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/speech-processing/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Parameter Efficient Fine-Tuning\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nParameter-Efficient Fine-Tuning (PEFT)\n\n  \nAdvantages\n    \n\n      \nPractical use-case\n\n    \n\n  \n\n  \nPEFT methods\n    \n\n      \nPrompt Modifications\n        \n\n          \nSoft Prompt Tuning\n\n          \nSoft Prompt vs. Prompting\n\n          \nPrefix Tuning\n\n          \nHard prompt tuning\n\n        \n\n      \n\n      \nAdapters\n        \n\n          \nWhat is an Adapter Module?\n\n          \nHow do you decide the value of \\(m\\)?\n\n          \nLLaMA-Adapters\n\n        \n\n      \n\n      \nReparameterization\n        \n\n          \nLow-Rank Adaptation (LoRA)\n            \n\n              \nBackground\n                \n\n                  \nRank of a Matrix\n\n                  \nRelated: Rank of a Tensor\n\n                \n\n              \n\n              \nOverview\n\n              \nAdvantages\n                \n\n                  \nParameter Efficiency\n\n                  \nGPU Memory (and Storage) Savings\n\n                  \nEfficient Task Switching\n\n                  \nFaster Training Speed\n\n                  \nNo additional inference latency\n\n                \n\n              \n\n              \nLimitations\n\n              \nHyperparameters\n                \n\n                  \nRank (\\(r\\))\n\n                  \nScaling Factor (\\(\\alpha\\))\n\n                  \nDropout Probability (\\(p\\))\n\n                  \nLearning Rate (lr)\n\n                  \nBatch Size (bs)\n\n                  \nSummary\n\n                \n\n              \n\n              \nHow does having a low-rank matrix in LoRA help the fine-tuning process?\n                \n\n                  \nWhat is a Low-rank Matrix?\n\n                  \nLow-Rank in LoRA Context\n\n                  \nExample\n\n                  \nWhy rank matters\n\n                \n\n              \n\n              \nHow does low-rank constraint introduced by LoRA inherently act as a form of regularization, especially for the lower layers of the model?\n                \n\n                  \nLow-Rank Constraint as Regularization\n\n                  \nEffect on Lower Layers\n\n                  \nWhy This Matters for Generalization\n\n                \n\n              \n\n              \nHow does LoRA help avoid catastrophic forgetting?\n                \n\n                  \nFreezing the Original Weights\n\n                  \nLow-Rank Adaptation Layers for Task-Specific Adjustments\n\n                  \nLayer-Specific Impact\n\n                  \nParameter-Efficient Fine-Tuning\n\n                  \nEasy Reversibility\n\n                  \nModular and Reusable Adapters\n\n                \n\n              \n\n              \nHow does multiplication of two low-rank matrices in LoRA lead to lower attention layers being impacted less than higher attention layers?\n                \n\n                  \nRole of Low-Rank Matrices in LoRA\n\n                  \nHigher Attention Layers: Task-Specific Focus\n\n                  \nLimited Capacity of Low-Rank Matrices and Layer Impact\n\n                  \nWhy Lower Layers are Less Affected\n\n                \n\n              \n\n              \nIn LoRA, why is \\(A\\) initialized using a Gaussian and \\(B\\) set to 0?\n                \n\n                  \nPreserving Initial Model Behavior\n\n                  \nGradual Learning and Adaptation\n\n                  \nEnsuring Controlled Updates\n\n                  \nFocused Adaptation\n\n                \n\n              \n\n              \nFor a given task, how do we determine whether to fine-tune the attention layers or feed-forward layers?\n                \n\n                  \nNature of the Task\n\n                  \nModel Architecture\n\n                  \nComputational Constraints\n\n                  \nEmpirical Testing\n\n                  \nTask-Specific Research and Insights\n\n                \n\n              \n\n              \nAssuming we’re fine-tuning attention weights, which specific attention weight matrices should we apply LoRA to?\n                \n\n                  \nContext and Setup\n\n                  \nExperimental Findings\n\n                  \nKey Results and Recommendations\n\n                  \nConclusion and Strategy for Applying LoRA\n\n                \n\n              \n\n              \nIs there a relationship between setting scaling factor and rank in LoRA?\n                \n\n                  \nUnderstanding \\(\\alpha\\) and \\(r\\)\n\n                  \nRelationship and Interaction\n\n                  \nPractical Considerations\n\n                  \nConclusion\n\n                \n\n              \n\n              \nHow do you determine the optimal rank \\(r\\) for LoRA?\n\n              \nHow do LoRA hyperparameters interact with each other? Is there a relationship between LoRA hyperparameters?\n                \n\n                  \nPractical Considerations\n\n                \n\n              \n\n              \nWhy does a higher rank make it the easier to overfit?\n\n              \nDoes LoRA adapt weights in all layers?\n                \n\n                  \nDoes LoRA impact lower attention layers less than higher attention layers?\n\n                \n\n              \n\n            \n\n          \n\n          \nQuantized Low-Rank Adaptation (QLoRA)\n\n          \nQuantization-Aware Low-Rank Adaptation (QA-LoRA)\n\n          \nRefined Low-Rank Adaptation (ReLoRA)\n\n          \nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n            \n\n              \nPredibase\n\n            \n\n          \n\n          \nWeight-Decomposed Low-Rank Adaptation (DoRA)\n\n          \nSummary of LoRA Techniques\n\n          \nLow-rank Linear Subspace ReFT (LoReFT)\n\n          \nStratified Progressive Adaptation Fine-tuning (SPAFIT)\n\n          \nBitFit\n\n          \nNOLA\n\n          \nMatrix of Rank Adaptation (MoRA)\n\n        \n\n      \n\n    \n\n  \n\n  \nWhich PEFT Technique to Choose: A Mental Model\n    \n\n      \nSoft Prompt Tuning\n\n      \nPrefix Tuning\n\n      \nAdapters\n\n      \nBitFit\n\n      \nLoRA\n\n      \nQLoRA\n\n      \nQA-LoRA\n\n      \nReLoRA\n\n      \nS-LoRA\n\n      \nDoRA\n\n      \nSPAFIT\n\n      \nNOLA\n\n      \nMoRA\n\n    \n\n  \n\n  \nComparative Analysis of Popular PEFT Methods\n\n  \nPractical Tips for Finetuning LLMs Using LoRA\n\n  \nRelated: Surgical fine-tuning\n    \n\n      \nLoRA vs. QLoRA experimentation by Sebastian Raschka\n\n    \n\n  \n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \nFine-tuning of large pre-trained models on downstream tasks is called “transfer learning”.\n\n  \nWhile full fine-tuning pre-trained models on downstream tasks is a common, effective approach, it is an inefficient approach to transfer learning.\n\n  \nThe simplest way out for efficient fine-tuning could be to freeze the networks’ lower layers and adapt only the top ones to specific tasks.\n\n  \nIn this article, we’ll explore Parameter Efficient Fine-Tuning (PEFT) methods that enable us to adapt a pre-trained model to downstream tasks more efficiently – in a way that trains lesser parameters and hence saves cost and training time, while also yielding performance similar to full fine-tuning.\n\n\n\n\n\nParameter-Efficient Fine-Tuning (PEFT)\n\n\n\n  \nLet’s start off by defining what parameter-efficient fine-tuning is and give some context on it.\n\n  \nParameter-efficient fine-tuning is particularly used in the context of large-scale pre-trained models (such as in NLP), to adapt that pre-trained model to a new task without drastically increasing the number of parameters.\n\n  \nThe challenge is this: modern pre-trained models (like BERT, GPT, T5, etc.) contain hundreds of millions, if not billions, of parameters. Fine-tuning all these parameters on a downstream task, especially when the available dataset for that task is small, can easily lead to overfitting. The model may simply memorize the training data instead of learning genuine patterns. Moreover, introducing additional layers or parameters during fine-tuning can drastically increase computational requirements and memory consumption.\n\n  \nAs mentioned earlier, PEFT allows to only fine-tune a small number of model parameters while freezing most of the parameters of the pre-trained LLM. This helps overcome the \ncatastrophic forgetting\n issue that full fine-tuned LLMs face where the LLM forgets the original task it was trained on after being fine-tuned.\n\n  \nThe image below \n(source)\n gives a nice overview of PEFT and its benefits.\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\n\n  \nParameter-efficient fine-tuning is useful due the following reasons:\n    \n\n      \nReduced computational costs (requires fewer GPUs and GPU time).\n\n      \nFaster training times (finishes training faster).\n\n      \nLower hardware requirements (works with cheaper GPUs with less VRAM).\n\n      \nBetter modeling performance (reduces overfitting).\n\n      \nLess storage (majority of weights can be shared across different tasks).\n\n    \n\n  \n\n\n\n\n\nPractical use-case\n\n\n\n\n  \nCredits to the below section go to \nPranay Pasula\n.\n\n  \nPEFT obviates the need for 40 or 80GB A100s to make use of powerful LLMs. In other words, you can fine-tune 10B+ parameter LLMs for your desired task for free or on cheap consumer GPUs.\n\n  \nUsing PEFT methods like \nLoRA\n, especially 4-bit quantized base models via \nQLoRA\n, you can fine-tune 10B+ parameter LLMs that are 30-40GB in size on 16GB GPUs. If it’s out of your budget to buy a 16GB GPU/TPU, Google Colab occasionally offers a 16GB VRAM Tesla T4 for free. Remember to save your model checkpoints every now and then and reload them as necessary, in the event of a Colab disconnect/kernel crash.\n\n  \nIf you’re fine-tuning on a single task, the base models are already so expressive that you need only a few (~10s-100s) of examples to perform well on this task. With PEFT via LoRA, you need to train only a trivial fraction (in this case, 0.08%), and though the weights are stored as 4-bit, computations are still done at 16-bit.\n\n  \nNote that while a good amount of VRAM is still needed for the fine-tuning process, using PEFT, with a small enough batch size, and little gradient accumulation, can do the trick while still retaining ‘fp16’ computation. In some cases, the performance on the fine-tuned task can be comparable to that of a fine-tuned 16-bit model.\n\n  \nKey takeaway: You can fine-tune powerful LLMs to perform well on a desired task using free compute. Use a <10B parameter model, which is still huge, and use quantization, PEFT, checkpointing, and provide a small training set, and you can quickly fine-tune this model for your use case.\n\n\n\n\n\nPEFT methods\n\n\n\n\n  \nBelow, we will delve into individual PEFT methods and delve deeper into their nuances.\n\n\n\n\n\nPrompt Modifications\n\n\n\nSoft Prompt Tuning\n\n\n\n\n  \nFirst introduced in the \nThe Power of Scale for Parameter-Efficient Prompt Tuning\n; this paper by Lester et al. introduces a simple yet effective method called soft prompt tuning, which prepends a trainable tensor to the model’s input embeddings, essentially creating a soft prompt to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts, soft prompts are learned through backpropagation and can be fine-tuned to incorporate signals from any number of labeled examples.\n\n  \nSoft prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pre-trained model.\n\n  \nThe authors show that prompt tuning outperforms few-shot learning by a large margin, and becomes more competitive with scale.\n\n  \nThis is an interesting approach that can help to effectively use a single frozen model for multi-task serving.\n\n  \nModel tuning requires making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pretrained model. With a T5 “XXL” model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task—a reduction of over five orders of magnitude – assuming a prompt length of 5 tokens.\n\n  \nThus, instead of using discrete text prompts, prompt tuning employs soft prompts. Soft prompts are learnable and conditioned through backpropagation, making them adaptable for specific tasks.\n\n\n\n\n\n\n\n\n\n  \nPrompt Tuning offers many benefits such as:\n    \n\n      \nMemory-Efficiency: Prompt tuning dramatically reduces memory requirements. For instance, while a T5 “XXL” model necessitates 11 billion parameters for each task-specific model, prompt-tuned models need a mere 20,480 parameters (assuming a prompt length of 5 tokens).\n\n      \nVersatility: Enables the use of a single frozen model for multi-task operations.\n\n      \nPerformance: Outshines few-shot learning and becomes more competitive as the scale grows.\n\n    \n\n  \n\n\n\n\n\nSoft Prompt vs. Prompting\n\n\n\n  \nSoft prompt tuning and prompting a model with extra context are both methods designed to guide a model’s behavior for specific tasks, but they operate in different ways. Here’s how they differ:\n\n\n\n\n\n\n  \nMechanism\n:\n    \n\n      \nSoft Prompt Tuning\n: This involves introducing trainable parameters (soft prompts) that are concatenated or added to the model’s input embeddings. These soft prompts are learned during the fine-tuning process and are adjusted through backpropagation to condition the model to produce desired outputs for specific tasks.\n\n      \nPrompting with Extra Context\n: This method involves feeding the model with handcrafted or predefined text prompts that provide additional context. There’s no explicit fine-tuning; instead, the model leverages its pre-trained knowledge to produce outputs based on the provided context. This method is common in few-shot learning scenarios where the model is given a few examples as prompts and then asked to generalize to a new example.\n\n    \n\n  \n\n  \nTrainability\n:\n    \n\n      \nSoft Prompt Tuning\n: The soft prompts are trainable. They get adjusted during the fine-tuning process to optimize the model’s performance on the target task.\n\n      \nPrompting with Extra Context\n: The prompts are static and not trainable. They’re designed (often manually) to give the model the necessary context for the desired task.\n\n    \n\n  \n\n  \nUse Case\n:\n    \n\n      \nSoft Prompt Tuning\n: This method is particularly useful when there’s a need to adapt a pre-trained model to various downstream tasks without adding significant computational overhead. Since the soft prompts are learned and optimized, they can capture nuanced information necessary for the task.\n\n      \nPrompting with Extra Context\n: This is often used when fine-tuning isn’t feasible or when working with models in a zero-shot or few-shot setting. It’s a way to leverage the vast knowledge contained in large pre-trained models by just guiding their behavior with carefully crafted prompts.\n\n    \n\n  \n\n\n\n\n\n\n  \nIn essence, while both methods use prompts to guide the model, soft prompt tuning involves learning and adjusting these prompts, whereas prompting with extra context involves using static, handcrafted prompts to guide the model’s behavior.\n\n\n\n\n\nPrefix Tuning\n\n\n\n\n  \nProposed in \nPrefix-Tuning: Optimizing Continuous Prompts for Generation\n, prefix-tuning is a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix).\n\n  \nInstead of adding a soft prompt to the model input, it prepends trainable parameters to the hidden states of all transformer blocks. During fine-tuning, the LM’s original parameters are kept frozen while the prefix parameters are updated.\n\n  \nPrefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”.\n\n  \nThe figure below from the paper shows that fine-tuning (top) updates all Transformer parameters (the red Transformer box) and requires storing a full model copy for each task. They propose prefix-tuning (bottom), which freezes the Transformer parameters and only optimizes the prefix (the red prefix blocks). Consequently, prefix-tuning only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step.\n\n\n\n\n\n\n\n\n\n  \n\n    \nThey apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. They find that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.\n\n  \n\n  \n\n    \nThe image below \n(source)\n illustrate how in prefix tuning, trainable tensors are addted to each transformer block instead of only in the input embedding.\n\n\n  \n\n\n\n\n\nHard prompt tuning\n\n\n\n  \nHard prompt tuning directly modifies the input prompt to the model. This can involve a vast multitude of things such as:\n    \n\n      \nWe can add examples of outputs we expect from the prompt\n\n      \nWe can add tags specifically relating to our task at hand\n\n    \n\n  \n\n  \nIn essence, it is just the modification of the string input, or prompt, to the model.\n\n\n\n\n\nAdapters\n\n\n\n\n  \nAdapter layers, often termed “Adapters”, add minimal additional parameters to the pretrained model. These adapters are inserted between existing layers of the network.\n\n  \nAdapters is a PEFT technique shown to achieve similar performance as compared to tuning the top layers while requiring as fewer parameters as two orders of magnitude.\n\n  \nAdapter-based tuning simply inserts new modules called “adapter modules” between the layers of the pre-trained network.\n\n  \nThe image below \n(source)\n illustrates this concept for the transformer block:\n\n\n\n\n\n\n\n\n\n  \nDuring fine-tuning, only the parameters of these adapter layers are updated, while the original model parameters are kept fixed. This results in a model with a small number of additional parameters that are task-specific.\n\n  \nKeeping the full PT model frozen, these modules are the only optimizable ones while fine-tuning – this means only a very few parameters are introduced per task yielding “compact” models.\n\n  \nThey offer many benefits such as:\n    \n\n      \nParameter-Efficiency: By keeping the main model frozen and only updating the adapter layers, a minimal number of parameters are added per task. This results in compact models that are memory-efficient.\n\n      \nPerformance: Despite the small parameter footprint, adapters often achieve performance comparable to conventional fine-tuning.\n\n    \n\n  \n\n  \n\n    \nThe adapter module consists of two fully connected layers with a bottleneck structure. This structure is inspired by autoencoders, which are designed to encode information into a compressed representation and then decode it back to its original form.\n\n  \n\n  \nHere’s how the parameter efficiency is achieved:\n\n\n\n\n\n\n  \n\n    \nBottleneck Structure\n: The first layer of the adapter reduces the dimensionality of the input (e.g., from 1024 to 24 dimensions). This drastic reduction means that the information from the original 1024 dimensions must be compressed into just 24 dimensions. The second layer then projects these 24 dimensions back to the original 1024 dimensions.\n\n  \n\n  \n\n    \nReduction in Parameters\n: This bottleneck approach significantly reduces the number of parameters. In your example, the total number of parameters introduced by the adapter is 49,152 (from the computation 1024x24 + 24x1024). If we were to use a single fully connected layer to project a 1024-dimensional input to a 1024-dimensional output directly, it would require 1,048,576 parameters (1024x1024).\n\n  \n\n  \n\n    \nEfficiency Analysis\n: By using the adapter approach, the number of parameters is substantially lower. Comparing 49,152 parameters to 1,048,576 parameters shows a dramatic reduction, making the adapter much more efficient in terms of parameter usage.\n\n  \n\n  \n\n    \nWhy is this Beneficial?\n: This efficiency is particularly beneficial when fine-tuning large pre-trained models. Instead of retraining or adapting the entire network (which would be computationally expensive and memory-intensive), adapters allow for targeted adjustments with far fewer additional parameters. This makes the process more manageable and practical, especially when resources are limited.\n\n  \n\n\n\n\n\n\n  \nThe adapter’s bottleneck structure allows it to achieve similar functionality (adapting the model to new tasks or data) as a full-sized layer would, but with a significantly reduced number of parameters. This efficiency makes adapters a popular choice for fine-tuning large pre-trained models in a resource-effective manner.\n\n\n\n\n\nWhat is an Adapter Module?\n\n\n\n\n  \nLet’s look at the application of the adapter module in the transformer architecture in three points:\n    \n\n      \nThe adapter module (right) first projects the original \\(d\\)-dimensional features into a smaller \\(m\\)-dimensional vector, applies a non-linearity, and then projects it back to \\(d\\) dimensions.\n\n      \nAs can be seen, the module features a skip-connection - With it in place, when the parameters of the projection layers are initialized to near-zero which eventually leads to near identity initialization of the module. This is required for stable fine-tuning and is intuitive as with it, we essentially do not disturb the learning from pre-training.\n\n      \nIn a transformer block (left), the adapter is applied directly to the outputs of each of the layers (attention and feedforward).\n\n    \n\n  \n\n\n\n\n\nHow do you decide the value of \\(m\\)?\n\n\n\n\n  \nThe size \\(m\\) in the Adapter module determines the number of optimizable parameters and hence poses a parameter vs performance tradeoff.\n\n  \nThe original paper experimentally investigates that the performance remains fairly stable across varying adapter sizes \\(m\\) and hence for a given model a fixed size can be used for all downstream tasks.\n\n\n\n\n\n\n\n\nLLaMA-Adapters\n\n\n\n\n  \n\n    \nThis paper introduces an efficient fine-tuning method called LLaMA-Adapter. This method is designed to adapt the LLaMA model into an instruction-following model with high efficiency in terms of resource usage and time. Key aspects of this paper include:\n\n\n    \n\n      \n\n        \nParameter Efficiency\n: LLaMA-Adapter introduces only 1.2 million learnable parameters on top of the frozen LLaMA 7B model, which is significantly fewer than the full 7 billion parameters of the model. This approach leads to a more efficient fine-tuning process both in terms of computational resources and time, taking less than one hour on 8 A100 GPUs.\n\n      \n\n      \n\n        \nLearnable Adaption Prompts\n: The method involves appending a set of learnable adaption prompts to the input instruction tokens in the higher transformer layers of LLaMA. These prompts are designed to adaptively inject new instructions into the frozen LLaMA while preserving its pre-trained knowledge, effectively guiding the subsequent contextual response generation.\n\n      \n\n      \n\n        \nZero-initialized Attention Mechanism\n: To avoid disturbances from randomly initialized adaption prompts, which can harm fine-tuning stability and effectiveness, the paper proposes a zero-initialized attention mechanism with a learnable gating factor. This mechanism allows for a stable learning process and progressive incorporation of instructional signals during training. It ensures that the newly acquired instructional signals are effectively integrated into the transformer while retaining the pre-trained knowledge of LLaMA.\n\n      \n\n      \n\n        \nGeneralization and Multi-modal Reasoning\n: LLaMA-Adapter is not only effective for language tasks but can also be extended to multi-modal instructions, allowing for image-conditioned LLaMA models. This capability enables superior reasoning performance on benchmarks like ScienceQA and COCO Caption. Additionally, the approach has demonstrated strong generalization capacity in traditional vision and language tasks.\n\n      \n\n    \n\n  \n\n  \n\n    \nIn summary, the LLaMA-Adapter represents a significant advancement in the field of parameter-efficient fine-tuning of large language models. Its innovative use of learnable adaption prompts and zero-initialized attention mechanism provides a highly efficient method for adapting pre-trained models to new tasks and domains, including multi-modal applications.\n\n  \n\n  \n\n    \nThe image below \n(source)\n illustrates this concept below.\n\n\n  \n\n\n\n\n\nReparameterization\n\n\n\nLow-Rank Adaptation (LoRA)\n\n\n\nBackground\n\n\n\nRank of a Matrix\n\n\n\n\n  \nThe rank of a matrix is a measure of the number of linearly independent rows or columns in the matrix.\n\n  \nIf a matrix has rank 1, it means all rows or all columns can be represented as multiples of each other, so there’s essentially only one unique “direction” in the data.\n\n  \nA full-rank matrix has rank equal to the smallest of its dimensions (number of rows or columns), meaning all rows and columns are independent.\n\n  \n\n    \nOn a related note, a matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. For more, refer \nWikipedia: Rank\n.\n\n  \n\n  \n\n    \nExample\n:\n\n\n    \n\n      \nConsider the following 3x3 matrix \\(A\\):\n\n    \n\n\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\]\n  \n\n  \n\n    \nStep-by-Step to Determine the Rank\n:\n\n\n    \n\n      \n\n        \nRow Reduction\n: To find the rank, we can use Gaussian elimination to transform the matrix into its row echelon form, making it easier to see linearly independent rows.\n\n\n        \nAfter row-reducing \\(A\\), we get:\n\n\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & -3 & -6 \\\\ 0 & 0 & 0 \\end{bmatrix}\\]\n      \n\n      \n\n        \nCount Independent Rows\n: Now we look at the rows with non-zero entries:\n\n        \n\n          \nThe first row \\([1, 2, 3]\\) is non-zero.\n\n          \nThe second row \\([0, -3, -6]\\) is also non-zero and independent of the first row.\n\n          \nThe third row is all zeros, which does not contribute to the rank.\n\n        \n\n\n        \nSince there are two non-zero, independent rows in the row echelon form, the rank of \\(A\\) is 2.\n\n      \n\n    \n\n  \n\n  \n\n    \nExplanation\n:\n\n\n    \n\n      \nThe rank of 2 indicates that only two rows or columns in \\(A\\) contain unique information, and the third row (or column) can be derived from a combination of the other two. Essentially, this matrix can be thought of as existing in a 2-dimensional space rather than a full 3-dimensional space, despite its 3x3 size.\n\n    \n\n  \n\n  \nIn summary:\n\n    \n\n      \nThe rank of matrix \\(A\\) is 2.\n\n      \nThis rank tells us the matrix’s actual dimensionality in terms of its independent information.\n\n    \n\n  \n\n\n\n\n\nRelated: Rank of a Tensor\n\n\n\n\n  \nWhile LoRA injects trainable low-rank matrices, it is important to understand rank in the context of tensors as well.\n\n  \n\n    \nThe rank of a tensor refers to the number of dimensions in the tensor. This is different from the rank of a matrix, which relates to the number of linearly independent rows or columns. For tensors, rank simply tells us how many dimensions or axes the tensor has.\n\n  \n\n  \n\n    \nExplanation with Examples\n:\n\n\n    \n\n      \nScalar (Rank 0 Tensor)\n:\n        \n\n          \nA scalar is a single number with no dimensions.\n\n          \nExample: \n5\n or \n3.14\n\n          \nShape: \n()\n (no dimensions)\n\n          \nRank\n: 0\n\n        \n\n      \n\n      \nVector (Rank 1 Tensor)\n:\n        \n\n          \nA vector is a one-dimensional array of numbers.\n\n          \nExample: \n[3, 7, 2]\n\n          \nShape: \n(3,)\n (one dimension with 3 elements)\n\n          \nRank\n: 1\n\n        \n\n      \n\n      \nMatrix (Rank 2 Tensor)\n:\n        \n\n          \nA matrix is a two-dimensional array of numbers, like a table.\n\n          \nExample:\n\\(\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\\)\n\n          \nShape: \n(2, 3)\n (two dimensions: 2 rows, 3 columns)\n\n          \nRank\n: 2\n\n        \n\n      \n\n      \n3D Tensor (Rank 3 Tensor)\n:\n        \n\n          \nA 3D tensor can be thought of as a “stack” of matrices, adding a third dimension.\n\n          \nExample:\n\\(\\begin{bmatrix}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix},\n\\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix}\n\\end{bmatrix}\\)\n\n          \nShape: \n(2, 2, 3)\n (three dimensions: 2 matrices, each with 2 rows and 3 columns)\n\n          \nRank\n: 3\n\n        \n\n      \n\n      \n4D Tensor (Rank 4 Tensor)\n:\n        \n\n          \nA 4D tensor might represent multiple “stacks” of 3D tensors.\n\n          \nExample: In deep learning, a 4D tensor is commonly used to represent batches of color images, with dimensions \n[batch size, channels, height, width]\n.\n\n          \nShape: \n(10, 3, 64, 64)\n for a batch of 10 images, each with 3 color channels and a resolution of 64x64.\n\n          \nRank\n: 4\n\n        \n\n      \n\n    \n\n  \n\n  \nGeneral Rule\n:\n    \n\n      \nRank\n = Number of dimensions (or axes) of the tensor.\n\n    \n\n  \n\n  \nWhy Rank Matters\n:\n    \n\n      \nThe rank of a tensor tells us about its structural complexity and the data it can represent. Higher-rank tensors can represent more complex data structures, which is essential in fields like deep learning, physics simulations, and data science for handling multi-dimensional data.\n\n    \n\n  \n\n\n\n\n\nOverview\n\n\n\n\n  \nIntrinsic Rank Hypothesis\n:\n    \n\n      \nLow-Rank Adaptation (LoRA) is motivated by the hypothesis that the updates to a model’s weights during task-specific adaptation exhibit a low “intrinsic rank.” This suggests that the weight changes required for effective adaptation are inherently low-dimensional. Thus, LoRA constrains these updates by representing them through low-rank decomposition matrices, enabling efficient adaptation without fine-tuning all model parameters.\n\n      \nAs illustrated in the figure below from the paper, LoRA leverages this by introducing two trainable low-rank matrices, \\(A\\) and \\(B\\), which capture the adaptation. \\(A\\) is initialized as a random Gaussian matrix and \\(B\\) as a zero matrix. During training, the product of \\(A\\) and \\(B\\) forms a low-rank update matrix that is added to the original, pre-trained weight matrix \\(W\\) to produce the adapted model output \\(h\\). This approach allows for efficient adaptation by modifying only a small subset of parameters while keeping the pre-trained weights \\(W\\) frozen.\n\n    \n\n\n    \n\n\n    \n\n      \nThis product, \\(BA\\), is low-rank because the rank of a matrix product is at most the minimum rank of the two factors. For instance, if \\(B\\) is a \\(d \\times r\\) matrix and \\(A\\) is an \\(r \\times d\\) matrix, where \\(r\\) is much smaller than \\(d\\), the resulting product \\(BA\\) will have a maximum rank of \\(r\\), regardless of the dimensions of \\(d\\). This means the update to \\(W\\) is constrained to a lower-dimensional space, efficiently capturing the essential information needed for adaptation.\n\n      \nFor example, if \\(d = 1000\\) and \\(r = 2\\), the update matrix \\(BA\\) will have a rank of at most 2 (since the rank of a product cannot exceed the rank of its factors), significantly reducing the number of parameters and the computational overhead required for fine-tuning. This means \\(BA\\) is a low-rank approximation that captures only the most essential directions needed for adaptation, thus enabling efficient updates without full matrix adjustments while keeping the pre-trained weights \\(W\\) frozen.\n\n    \n\n  \n\n  \nEssence\n:\n    \n\n      \nLoRA fundamentally changes the approach to fine-tuning large neural networks by introducing a method to decompose high-dimensional weight matrices into lower-dimensional forms, preserving essential information while reducing computational load. Put simply, LoRA efficiently fine-tunes large-scale neural networks by introducing trainable low-rank matrices, simplifying the model’s complexity while retaining its robust learning capabilities.\n\n      \nLoRA is similar to methods like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD), leverages the observation that weight updates during fine-tuning are often rank-deficient. By imposing a low-rank constraint on these updates, LoRA enables a lightweight adaptation process that captures the most critical directions or “components” in the weight space. This means the model can retain essential knowledge from its pre-training phase while focusing on the specific nuances of the new task, resulting in efficient adaptation with a smaller memory and computational footprint.\n\n    \n\n  \n\n  \nApplication\n:\n    \n\n      \nLoRA’s primary application is in the efficient fine-tuning of large neural networks, particularly in transformer-based architectures. In practice, LoRA identifies key dimensions within the original weight matrix that are crucial for a specific task, significantly reducing the adaptation’s dimensionality.\n\n      \nInstead of fine-tuning all weights, which is computationally prohibitive for large models, LoRA introduces two trainable low-rank matrices, \\(A\\) and \\(B\\), in specific layers of the transformer (e.g., in the query and value projections of the attention mechanism). These matrices are optimized during training while keeping the core model parameters frozen. This architecture adjustment means that only a fraction of the original parameters are actively updated, thus lowering memory usage and enabling faster computations.\n\n      \nBy focusing only on the most relevant dimensions for each task, LoRA enables the deployment of large models on hardware with limited resources and facilitates task-switching by updating only the low-rank matrices without retraining the entire model.\n\n    \n\n  \n\n  \nBenefits\n:\n    \n\n      \nLoRA provides several advantages that make it particularly suitable for practical use in industry and research settings where resource constraints are a concern. The primary benefit is in memory and computational efficiency.\n\n      \nBy keeping the majority of the model’s parameters frozen and only updating the low-rank matrices, LoRA significantly reduces the number of trainable parameters, leading to lower memory consumption and faster training times. This reduction in training complexity also means that fewer GPUs or lower-spec hardware can be used to fine-tune large models, broadening accessibility.\n\n      \nAdditionally, LoRA avoids introducing any additional latency during inference because, once the low-rank matrices have been trained, they can be merged back into the pre-trained weights without altering the architecture.\n\n      \nThis setup makes LoRA ideal for environments where models need to switch between tasks frequently, as only the task-specific low-rank weights need to be loaded, allowing the model to quickly adapt to new tasks or datasets with minimal overhead.\n\n    \n\n  \n\n  \nIn Summary\n:\n    \n\n      \nLoRA represents a highly efficient approach to fine-tuning, striking a balance between the depth of knowledge encapsulated in large pre-trained models and the need for targeted adaptation to new tasks or data.\n\n      \nBy leveraging the low intrinsic rank of weight updates, LoRA retains the model’s robustness and generalization capability while allowing for quick, cost-effective adaptations. This approach redefines efficiency in the era of massive language models, making it feasible to use and adapt large-scale pre-trained models across various applications and domains without the extensive computational burden associated with traditional fine-tuning.\n\n    \n\n\n    \n\n  \n\n  \nAs a recap of traditional finetuning vs. LoRA \n(source)\n:\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\nParameter Efficiency\n\n\n\n\n  \nCompared to full fine-tuning GPT-3 175B with Adam, LoRA can reduce the number of trainable parameters by 10,000 times. Specifically, this means that LoRA only fine-tunes approximately 0.01% of the parameters of the original model.\n\n  \nThe below table from the LoRA paper indicates that for GPT-3 with LoRA, we see that we only fine-tune \\(\\frac{4.7}{175255} \\times 100 = 0.002\\%\\) and \\(\\frac{38}{175255} \\times 100 = 0.02\\%\\) parameters.\n\n\n\n\n\n\n\n\nGPU Memory (and Storage) Savings\n\n\n\n\n  \nCompared to full fine-tuning GPT-3 175B with Adam, LoRA can reduce the GPU memory requirement by 3 times. Specifically, this means that LoRA fine-tunes the original model with 33% of the memory.\n\n  \nFor a large Transformer trained with Adam, LoRA reduces VRAM usage by up to two-thirds by avoiding the need to store optimizer states for the frozen parameters. On GPT-3 175B, VRAM consumption during training drops from 1.2TB to 350GB. When adapting only the query and value projection matrices with a rank \\(r = 4\\), the checkpoint size decreases significantly from approximately 350GB to 35MB. This efficiency allows training with significantly fewer GPUs and avoids I/O bottlenecks.\n\n\n\n\n\nEfficient Task Switching\n\n\n\n\n  \nTask switching is more cost-effective as only the LoRA weights need swapping, enabling the creation of numerous customized models that can be dynamically swapped on machines storing the pre-trained weights in VRAM.\n\n\n\n\n\nFaster Training Speed\n\n\n\n\n  \nTraining speed also improves by 25% compared to full fine-tuning, as the gradient calculation for the vast majority of the parameters is unnecessary.\n\n\n\n\n\nNo additional inference latency\n\n\n\n\n  \nLoRA ensures no additional inference latency when deployed in production by allowing explicit computation and storage of the combined weight matrix \\(W = W_0 + BA\\). During inference, this approach uses the pre-computed matrix \\(W\\), which includes the original pre-trained weights \\(W_0\\) and the low-rank adaptation matrices \\(B\\) and \\(A\\). This method eliminates the need for dynamic computations during inference.\n\n  \nWhen switching to another downstream task, the pre-trained weights \\(W_0\\) can be quickly restored by subtracting the current low-rank product \\(BA\\) and adding the new task-specific low-rank product \\(B' A'\\). This operation incurs minimal memory overhead and allows for efficient task switching without impacting inference speed. By merging the low-rank matrices with the pre-trained weights in advance, LoRA avoids the extra computational burden during real-time inference (unlike adapters), ensuring latency remains on par with that of fully fine-tuned models.\n\n\n\n\n\nLimitations\n\n\n\n\n  \nWhile LoRA offers significant advantages in terms of parameter efficiency and memory savings, it also has some limitations. One notable limitation is the complexity involved in batching inputs for different tasks when using distinct low-rank matrices \\(A\\) and \\(B\\). If the goal is to absorb \\(A\\) and \\(B\\) into the combined weight matrix \\(W\\) to avoid additional inference latency, it becomes challenging to batch inputs from different tasks in a single forward pass. This is because each task would require a different set of \\(A\\) and \\(B\\) matrices, complicating the batching process.\n\n  \nAdditionally, although it is possible to avoid merging the weights and dynamically select the appropriate LoRA modules for each sample in a batch, this approach is more feasible in scenarios where latency is not a critical concern. This workaround does not fully address the need for seamless integration when low-latency inference is required across multiple tasks.\n\n  \nIn summary, while LoRA provides a highly efficient adaptation method, the complexity in handling multiple tasks simultaneously and the need for careful management of low-rank matrices during batching are important considerations for its practical deployment.\n\n\n\n\n\nHyperparameters\n\n\n\n\n  \nLoRA-specific hyperparameters include rank (\\(r\\)) and alpha (\\(\\alpha\\)). Others, while still used for LoRA-based fine-tuning, such as learning rate (lr), dropout probability (\\(p\\)), and batch size (bs), are more generic to deep learning-based model training/fine-tuning. Here’s a detailed explanation of each:\n\n\n\n\n\nRank (\\(r\\))\n\n\n\n\n  \n\n    \nDescription\n: In LoRA, instead of fine-tuning the full weight matrix, a low-rank approximation is used, where a weight matrix \\(W_0\\) is decomposed into two smaller matrices, \\(A \\in \\mathbb{R}^{d \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times k}\\), where \\(r\\) is much smaller than \\(d\\) or \\(k\\). The rank (\\(r\\)) of matrices \\(A\\) and \\(B\\) – one of the core hyperparameters in LoRA – represents the rank of the low-rank decomposition applied to the weight matrices. The new weight is then modeled as:\n\n\n\\[W = W_0 + \\Delta W = W_0 + A \\cdot B\\]\n  \n\n  \nRole\n: The rank controls the dimensionality of the low-rank matrices and hence the number of additional parameters introduced during fine-tuning.\n\n  \n\n    \nInterpretation\n: Lower values of \\(r\\) will impose stronger restrictions on how much the weight matrices can adapt, potentially limiting the model’s flexibility but greatly reducing the computational and memory footprint. Higher values of \\(r\\) allow for more expressive updates but increase the number of parameters and computation required.\n\n  \n\n  \n\n    \nEquation\n: In matrix form, for any original weight matrix \\(W_0 \\in \\mathbb{R}^{d \\times k}\\), the adapted weight update is expressed as:\n\n\n\\[\\Delta W = A \\cdot B\\]\n\n    \n\n      \nwhere, \\(A \\in \\mathbb{R}^{d \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times k}\\), where \\(r \\ll d, k\\).\n\n    \n\n  \n\n  \nTypical Values\n: 1–16, depending on the size of the model and the complexity of the task. In most tasks, a small rank (e.g., 4 or 8) provides a good trade-off between performance and efficiency.\n\n\n\n\n\nScaling Factor (\\(\\alpha\\))\n\n\n\n\n  \n\n    \nDescription\n: \\(\\alpha\\) is a scaling factor applied to the LoRA updates. Specifically, it scales the low-rank updates \\(A \\cdot B\\) before adding them to the base weight matrix \\(W_0\\). The weight update rule becomes:\n\n\n\\[W = W_0 + \\frac{\\alpha}{r} \\cdot (A \\cdot B)\\]\n  \n\n  \n\n    \nRole\n: The purpose of \\(\\alpha\\) is to control the magnitude of the low-rank updates to prevent the model from diverging too far from the pre-trained weights. By dividing \\(\\alpha\\) by the rank \\(r\\), LoRA ensures that the update magnitude is normalized according to the size of the low-rank decomposition. This is crucial because a larger rank would introduce more freedom for updates, and the division by \\(r\\) keeps the updates in check.\n\n  \n\n  \n\n    \nInterpretation\n: A higher \\(\\alpha\\) means that the low-rank updates will have a larger impact on the final weight, while a smaller \\(\\alpha\\) means the low-rank updates will contribute less to the adapted model. The division by \\(r\\) helps keep the effect of the low-rank update consistent across different choices of rank.\n\n  \n\n  \n\n    \nEquation\n: The weight update is now written as:\n\n\n\\[\\Delta W = \\frac{\\alpha}{r} \\cdot (A \\cdot B)\\]\n  \n\n  \n\n    \nTypical Values\n: Common values for \\(\\alpha\\) are in the range of 1–32. The typical recommendation is to set \\(\\alpha = \\frac{r}{\\text{base rank}}\\), where \\(\\text{base rank}\\) is a predetermined scale for the model.\n\n  \n\n\n\n\n\nDropout Probability (\\(p\\))\n\n\n\n\n  \n\n    \nDescription\n: \nDropout\n is a regularization technique used to prevent overfitting, and it is applied in the LoRA framework as well. The dropout probability (p) refers to the probability with which a particular element in the low-rank matrices \\(A\\) and \\(B\\) is randomly set to zero during training. Dropout is typically used to reduce overfitting by introducing noise during training.\n\n\n    \n\n      \n\n        \nRole\n: The role of dropout in LoRA is to regularize the low-rank weight updates and ensure they do not overfit to the fine-tuning data. By randomly zeroing out parts of the matrices, the model learns more robust and generalizable updates.\n\n      \n\n      \n\n        \nInterpretation\n: Higher values of dropout probability \\(p\\) imply more aggressive regularization, which can reduce overfitting but also may slow down learning. Lower values of \\(p\\) imply less regularization and could potentially lead to overfitting on small datasets.\n\n      \n\n      \n\n        \nEquation\n: The dropout operation is typically represented as:\n\n\n\\[A_{dropped} = A \\odot \\text{Bernoulli}(1-p)\\]\n\n        \n\n          \nwhere, \\(\\odot\\) denotes element-wise multiplication, and \\(\\text{Bernoulli}(1-p)\\) is a binary mask where each element is independently drawn from a Bernoulli distribution with probability \\(1 - p\\).\n\n        \n\n      \n\n      \n\n        \nTypical Values\n: Dropout probabilities \\(p\\) are typically set between 0.0 (no dropout) and 0.3 for LoRA tasks.\n\n      \n\n    \n\n  \n\n\n\n\n\nLearning Rate (lr)\n\n\n\n\n  \n\n    \nDescription\n: The learning rate is a fundamental hyperparameter in any optimization process, and it determines the step size at which the model’s parameters are updated during training. In the context of LoRA, it controls the update of the low-rank matrices \\(A\\) and \\(B\\) rather than the full model weights.\n\n\n    \n\n      \n\n        \nRole\n: The learning rate governs how fast or slow the low-rank matrices adapt to the new task. A high learning rate can lead to faster convergence but risks overshooting the optimal solution, while a small learning rate can provide more stable convergence but might take longer to adapt to the new task.\n\n      \n\n      \n\n        \nInterpretation\n: A higher learning rate might be used in the early stages of fine-tuning to quickly adapt to the new task, followed by a lower rate to refine the final performance. However, too high a learning rate may destabilize training, especially when \\(\\alpha\\) is large.\n\n      \n\n      \n\n        \nEquation\n: The update to the low-rank parameters follows the standard gradient descent update rule:\n\n\n\\[\\theta_{t+1} = \\theta_t - lr \\cdot \\nabla_{\\theta} L\\]\n\n        \nWhere \\(L\\) is the loss function, \\(\\nabla_{\\theta} L\\) is the gradient of the loss with respect to the low-rank parameters \\(\\theta\\), and \\(lr\\) is the learning rate.\n\n      \n\n      \n\n        \nTypical Values\n: Learning rates for LoRA typically range from \\(10^{-5}\\) to \\(10^{-3}\\), depending on the model, the task, and the scale of adaptation needed.\n\n      \n\n    \n\n  \n\n\n\n\n\nBatch Size (bs)\n\n\n\n\n  \n\n    \nDescription\n: The batch size is the number of examples that are passed through the model at one time before updating the weights. It is a crucial hyperparameter for stabilizing the training process.\n\n\n    \n\n      \n\n        \nRole\n: In LoRA, the batch size affects how stable and efficient the low-rank adaptation process is. A larger batch size can stabilize the gradient estimates and speed up convergence, while smaller batches introduce more noise into the gradient, which may require a smaller learning rate to maintain stability.\n\n      \n\n      \n\n        \nInterpretation\n: Smaller batch sizes allow for faster updates but with noisier gradients, whereas larger batch sizes reduce noise but require more memory. Finding the right balance is important for both computational efficiency and effective adaptation.\n\n      \n\n      \n\n        \nEquation\n: The loss for a given batch of size \\(bs\\) is averaged over the batch:\n\n\n\\[L_{\\text{batch}} = \\frac{1}{bs} \\sum_{i=1}^{bs} L_i\\]\n\n        \n\n          \nwhere, \\(L_i\\) is the loss for the \\(i\\)-th example in the batch.\n\n        \n\n      \n\n      \n\n        \nTypical Values\n: Batch sizes can vary widely depending on the available hardware resources. Typical values range from 8 to 64.\n\n      \n\n    \n\n  \n\n\n\n\n\nSummary\n\n\n\n\n  \nThe main hyperparameters involved in LoRA—rank (\\(r\\)), alpha (\\(\\alpha\\)), dropout probability (p), learning rate (lr), and batch size (bs)—are crucial for controlling the behavior and effectiveness of LoRA. By adjusting these parameters, LoRA can offer an efficient way to fine-tune large pre-trained models with significantly reduced computational costs and memory usage while maintaining competitive performance. Each of these hyperparameters impacts the trade-off between model flexibility, computational efficiency, and training stability.\n\n  \nThese hyperparameters are interconnected, especially scaling factor and rank; changes in one can require adjustments in others; more on this in the section on \nIs There a Relationship Between Setting Scaling Factor and Rank in LoRA?\n. Effective tuning of these parameters is critical for leveraging LoRA’s capabilities to adapt large models without extensive retraining.\n\n\n\n\n\nHow does having a low-rank matrix in LoRA help the fine-tuning process?\n\n\n\n\n  \nIn LoRA, a low-rank matrix is a matrix with a rank significantly smaller than its full dimensionality, which enables efficient and focused adjustments to model parameters. This lightweight adaptation mechanism allows large language models to learn new tasks without overfitting by capturing only the most essential adjustments, thus optimizing both information representation and parameter efficiency.\n\n\n\n\n\nWhat is a Low-rank Matrix?\n\n\n\n\n  \nA matrix is considered low-rank when its rank (the number of independent rows or columns) is much smaller than its dimensions. For example, a 1000x1000 matrix with rank 10 is low-rank because only 10 of its rows or columns contain unique information, and the others can be derived from these. This smaller rank indicates that the matrix contains a limited variety of independent patterns or directions, meaning it has a reduced capacity to capture complex relationships.\n\n\n\n\n\nLow-Rank in LoRA Context\n\n\n\n\n  \nIn LoRA, low-rank matrices are introduced to fine-tune large language models with fewer trainable parameters. Here’s how it works:\n    \n\n      \nAdding Low-Rank Matrices\n: LoRA adds small, low-rank matrices to the model’s layers (typically linear or attention layers). These matrices serve as “adaptation” layers that adjust the original layer’s output.\n\n      \nFreezing the Original Weights\n: The original model weights remain frozen during fine-tuning. Only the low-rank matrices are trained, which reduces the number of parameters to update.\n\n    \n\n  \n\n  \nBy limiting the rank of these new matrices, LoRA effectively limits the number of patterns they can represent. For instance, a rank-5 matrix in a high-dimensional space can only capture 5 independent directions, which forces the model to learn only essential, low-dimensional adjustments without becoming too complex.\n\n\n\n\n\nExample\n\n\n\n\n  \nSuppose we have a pre-trained model layer represented by a 512x512 matrix (common in large language models). Instead of fine-tuning this large matrix directly, LoRA adds two low-rank matrices, \\(A\\) and \\(B\\), with dimensions 512x10 and 10x512, respectively. Here:\n    \n\n      \nThe product \\(A \\times B\\) has a rank of 10, much smaller than 512.\n\n      \nThis product effectively adds a low-rank adaptation to the original layer, allowing it to adjust its output in just a few key directions (10 in this case), rather than making unrestricted adjustments.\n\n    \n\n  \n\n\n\n\n\nWhy rank matters\n\n\n\n\n  \nThe rank of the LoRA matrices directly affects the model’s ability to learn task-specific patterns:\n    \n\n      \nLower Rank\n: Imposes a strong constraint on the model, which helps it generalize better and reduces the risk of overfitting.\n\n      \nHigher Rank\n: Provides more flexibility but also increases the risk of overfitting, as the model can learn more complex adjustments that may fit the fine-tuning data too closely.\n\n    \n\n  \n\n\n\n\n\nHow does low-rank constraint introduced by LoRA inherently act as a form of regularization, especially for the lower layers of the model?\n\n\n\n\n  \nIn LoRA, the low-rank constraint serves as a built-in regularization mechanism by limiting the model’s flexibility during fine-tuning. This constraint especially impacts lower layers, which are designed to capture general, foundational features. By further restricting these layers, LoRA minimizes their adaptation to task-specific data, reducing the risk of overfitting. This regularization preserves the model’s foundational knowledge in the lower layers, while allowing the higher layers—where task-specific adjustments are most beneficial—to adapt more freely.\n\n\n\n\n\nLow-Rank Constraint as Regularization\n\n\n\n\n  \n\n    \nLow-Rank Matrices Limit Complexity\n: By adding only low-rank matrices to the model’s layers, LoRA restricts the model’s capacity to represent highly complex, task-specific patterns. A low-rank matrix has fewer independent “directions” or dimensions in which it can vary. This means that the model, even when fine-tuned, can only make adjustments within a constrained range, learning broad, generalizable patterns rather than memorizing specific details of the training data. This limited capacity serves as a form of regularization, preventing the model from overfitting.\n\n  \n\n  \n\n    \nReduced Sensitivity to Noisy Patterns\n: Low-rank matrices inherently ignore minor or highly detailed variations in the training data, focusing only on dominant, overarching patterns. This makes LoRA less sensitive to the idiosyncrasies of the fine-tuning dataset, enhancing the model’s robustness and generalization ability.\n\n  \n\n\n\n\n\nEffect on Lower Layers\n\n\n\n\n  \nThe \nlower layers\n of a neural network, especially in a transformer model, are primarily responsible for extracting general-purpose features from the input data. In language models, for example:\n    \n\n      \nLower layers capture basic syntactic relationships, such as sentence structure and word dependencies.\n\n      \nThese layers learn representations that are widely applicable across tasks and domains.\n\n    \n\n  \n\n  \nBecause these lower layers are already optimized to represent broad, generalizable patterns from pre-training, they are naturally less flexible and more constrained in what they capture compared to higher layers, which focus on more task-specific details. Adding a low-rank constraint in LoRA further reinforces this effect:\n\n\n\n\n\n\n  \n\n    \nEnhanced Regularization on Lower Layers\n: Since lower layers are already constrained to capture only general patterns, the addition of a low-rank constraint essentially adds a second layer of regularization. This means that these layers become even less likely to adapt in ways that would compromise their general-purpose functionality. The low-rank constraint reinforces their role as foundational feature extractors, preserving their generalization capability while preventing overfitting on the specific details of the fine-tuning data.\n\n  \n\n  \n\n    \nMinimal Disruption of Pre-Trained Knowledge\n: The low-rank adaptation in LoRA ensures that lower layers maintain the knowledge they acquired during pre-training. Because these layers are regularized by the low-rank constraint, they are less likely to overfit to new data patterns introduced during fine-tuning. This preservation of pre-trained knowledge is crucial for maintaining the model’s transferability to other tasks or domains, as lower layers retain their broad, foundational representations.\n\n  \n\n\n\n\n\nWhy This Matters for Generalization\n\n\n\n\n  \nWhen fine-tuning with LoRA:\n    \n\n      \nHigher Layers Adapt More Easily\n: Higher layers, being closer to the output, are more adaptable and can more readily accommodate task-specific changes introduced during fine-tuning.\n\n      \nLower Layers Remain Generalized\n: Lower layers, reinforced by the low-rank constraint, retain their focus on general patterns. This balanced approach helps the model generalize well to unseen data because the lower layers still provide robust, general-purpose representations while the higher layers adapt to the new task.\n\n    \n\n  \n\n\n\n\n\nHow does LoRA help avoid catastrophic forgetting?\n\n\n\n\n  \n\n    \nLoRA helps prevent catastrophic forgetting by fine-tuning large pre-trained models in a way that preserves their foundational knowledge while allowing for task-specific adaptations. Catastrophic forgetting occurs when fine-tuning neural networks, particularly large pre-trained models, causes them to overwrite or disrupt previously learned information, reducing performance on earlier tasks. LoRA mitigates this risk through a few key strategies:\n\n\n    \n\n      \nFreezing Original Weights\n: The core model parameters remain untouched, preserving the base knowledge and preventing interference.\n\n      \nIntroducing Low-Rank Matrices\n: These matrices have limited capacity, focusing solely on task-specific adjustments, which allows the model to adapt to new tasks without losing general knowledge.\n\n      \nTargeting Specific Layers\n: LoRA typically modifies higher attention layers, avoiding disruption to fundamental representations in lower layers.\n\n      \nParameter-Efficient, Modular Adaptation\n: LoRA’s modular design allows for reversible, task-specific adjustments, making it suitable for flexible multi-task and continual learning.\n\n    \n\n  \n\n  \n\n    \nThrough this approach, LoRA enables large models to adapt efficiently to new tasks while retaining previously learned information, which is especially valuable for applications requiring retention of prior knowledge.\n\n  \n\n\n\n\n\nFreezing the Original Weights\n\n\n\n\n  \nOne of the core aspects of LoRA is that it freezes the original model weights and adds new, low-rank matrices that handle the fine-tuning process:\n    \n\n      \nThe frozen original weights retain the model’s general knowledge from pre-training. This means that core information, patterns, and representations acquired from extensive pre-training on large datasets remain unaffected.\n\n      \nSince only the low-rank matrices are adjusted for the new task, there is no direct modification of the original weights. This minimizes the risk of overwriting or disrupting the knowledge captured in those weights.\n\n    \n\n  \n\n  \nBy keeping the original parameters intact, LoRA avoids catastrophic forgetting in a way that typical fine-tuning (where the original weights are updated) does not.\n\n\n\n\n\nLow-Rank Adaptation Layers for Task-Specific Adjustments\n\n\n\n\n  \nLoRA introduces low-rank matrices as additional layers to the model, which have the following properties:\n    \n\n      \nLimited Capacity\n: Low-rank matrices have a constrained capacity to represent new information, which forces them to focus only on essential, task-specific adaptations. This means they cannot significantly alter the underlying model’s behavior, preserving the broader general knowledge.\n\n      \nFocused Adaptation\n: By adding task-specific information via low-rank matrices rather than altering the model’s entire parameter space, LoRA ensures that the new task-specific changes are confined to these auxiliary matrices. This helps the model adapt to new tasks without losing its prior knowledge.\n\n    \n\n  \n\n\n\n\n\nLayer-Specific Impact\n\n\n\n\n  \nLoRA often targets specific layers in the model, commonly the attention layers:\n    \n\n      \nHigher Attention Layers\n: These layers (closer to the output) are responsible for more task-specific representations and are typically the ones modified by LoRA. This selective adaptation means that the deeper, more task-general features in lower layers are left intact, reducing the risk of catastrophic forgetting.\n\n      \nMinimal Lower-Layer Impact\n: Since lower layers (closer to the input) remain unchanged or minimally affected, the model retains the general-purpose, foundational features learned during pre-training, which are crucial for generalization.\n\n    \n\n  \n\n  \nThis selective impact allows LoRA to introduce new, task-specific representations while preserving fundamental information, balancing new task learning with knowledge retention.\n\n\n\n\n\nParameter-Efficient Fine-Tuning\n\n\n\n\n  \nLoRA is designed for parameter-efficient fine-tuning, meaning it uses a fraction of the parameters that traditional fine-tuning would require:\n    \n\n      \nLoRA adds only a small number of new parameters through the low-rank matrices. This efficiency keeps the model changes lightweight, making it less likely to interfere with the original model’s representations.\n\n      \nThe low-rank constraint also regularizes the fine-tuning process, helping to prevent overfitting to the new task, which can indirectly support retention of general knowledge. Overfitting can cause catastrophic forgetting if the model becomes too specialized, as it loses flexibility in dealing with tasks beyond the fine-tuning data.\n\n    \n\n  \n\n\n\n\n\nEasy Reversibility\n\n\n\n\n  \nSince LoRA’s approach is to add new matrices rather than alter the original model’s weights, it makes it easy to revert the model to its original state or apply it to different tasks:\n    \n\n      \nThe low-rank matrices can be removed or swapped out without affecting the base model. This modularity allows for rapid switching between tasks or models, making it easy to adapt the model to different tasks while maintaining the pre-trained knowledge.\n\n      \nThis adaptability is particularly useful for multi-task learning or continual learning, as it allows LoRA-enhanced models to apply distinct low-rank adaptations for different tasks without compromising the model’s underlying pre-trained knowledge.\n\n    \n\n  \n\n\n\n\n\nModular and Reusable Adapters\n\n\n\n\n  \nWith LoRA, fine-tuning for different tasks can be achieved by creating different low-rank matrices for each new task:\n    \n\n      \nThese modular, reusable matrices enable task-specific tuning without overwriting previous adaptations or the original model. This is especially valuable for applications where the model needs to perform multiple tasks or domains interchangeably.\n\n      \nBy associating each task with its own set of low-rank matrices, LoRA enables the model to maintain knowledge across tasks without interference, effectively circumventing catastrophic forgetting.\n\n    \n\n  \n\n\n\n\n\nHow does multiplication of two low-rank matrices in LoRA lead to lower attention layers being impacted less than higher attention layers?\n\n\n\n\n  \nIn LoRA, the use of low-rank matrices enables efficient, controlled updates by selectively applying them to specific layers—primarily in the higher attention layers rather than the lower ones. This targeted approach allows the model to adjust effectively to task-specific nuances in these higher layers, which capture more complex patterns and contextual information, while preserving the general features encoded in the lower layers. By focusing fine-tuning efforts on the higher layers, LoRA minimizes overfitting and retains foundational knowledge from pre-training, making it an efficient and effective fine-tuning strategy.\n\n\n\n\n\nRole of Low-Rank Matrices in LoRA\n\n\n\n\n  \nLoRA adds two low-rank matrices, \\(A\\) and \\(B\\), to certain layers, typically in the form:\n  \\(W_{\\text{new}} = W + A \\times B\\)\n    \n\n      \nwhere:\n        \n\n          \n\\(W\\) is the original (frozen) weight matrix in the model layer.\n\n          \n\\(A\\) and \\(B\\) are low-rank matrices (with ranks much smaller than the original dimensionality of \\(W\\)), creating a low-rank adaptation.\n\n        \n\n      \n\n    \n\n  \n\n  \nThe product \\(A \\times B\\) has a limited rank and thus introduces only a restricted adjustment to \\(W\\). This adjustment constrains the layer to learn only a few independent patterns rather than a full set of complex, task-specific transformations.\n\n\n\n\n\nHigher Attention Layers: Task-Specific Focus\n\n\n\n\n  \nIn large models, higher attention layers (closer to the output) tend to capture task-specific, abstract features, while lower attention layers (closer to the input) capture general, reusable patterns. By applying LoRA-based fine-tuning primarily to higher attention layers:\n\n  \nThe model’s low-rank adaptation focuses on high-level, task-specific adjustments rather than modifying general representations.\n\n  \nHigher layers, which already deal with more specific information, are more sensitive to the small adjustments made by \\(A \\times B\\) since they directly influence task-related outputs.\n\n  \nIn practice, LoRA-based fine-tuning modifies these higher layers more significantly because these layers are more directly responsible for adapting the model to new tasks. Lower layers, in contrast, require less task-specific adjustment and retain their general-purpose features.\n\n\n\n\n\nLimited Capacity of Low-Rank Matrices and Layer Impact\n\n\n\n\n  \nThe low-rank matrices \\(A\\) and \\(B\\) have limited expressive power (due to their low rank), meaning they can only introduce a small number of directional adjustments in the weight space. This limited capacity aligns well with higher layers because:\n\n  \nHigher layers don’t need drastic changes but rather subtle adjustments to fine-tune the model to specific tasks.\n\n  \nThe constraint imposed by low-rank matrices helps avoid overfitting by restricting the number of learned patterns, which is ideal for the high-level, abstract representations in higher layers.\n\n  \nFor lower layers, which capture broad, general-purpose features, such limited adjustments don’t significantly impact the model. Lower layers still operate with the general features learned during pre-training, while higher layers adapt to task-specific details.\n\n\n\n\n\nWhy Lower Layers are Less Affected\n\n\n\n\n  \nLower layers in the attention stack are less impacted by LoRA’s low-rank updates because:\n\n  \nThey are often not fine-tuned at all in LoRA-based setups, preserving the general features learned during pre-training.\n\n  \nEven when fine-tuned with low-rank matrices, the limited capacity of \\(A \\times B\\) is not sufficient to drastically alter their broader, foundational representations.\n\n\n\n\n\nIn LoRA, why is \\(A\\) initialized using a Gaussian and \\(B\\) set to 0?\n\n\n\n\n  \nIn LoRA, the initialization strategy where matrix \\(A\\) is initialized with a Gaussian distribution and matrix \\(B\\) is set to zero is crucial for ensuring a smooth integration of the adaptation with minimal initial disruption to the pre-trained model. This approach is designed with specific goals in mind:\n\n\n\n\n\nPreserving Initial Model Behavior\n\n\n\n\n  \nRationale\n: By setting \\(B\\) to zero, the product \\(\\Delta W = BA\\) initially equals zero. This means that the adapted weights do not alter the original pre-trained weights at the beginning of the training process.\n\n  \nImpact\n: This preserves the behavior of the original model at the start of fine-tuning, allowing the model to maintain its pre-trained performance and stability. The model begins adaptation from a known good state, reducing the risk of drastic initial performance drops.\n\n\n\n\n\nGradual Learning and Adaptation\n\n\n\n  \nRationale\n: Starting with \\(\\Delta W = 0\\) allows the model to gradually adapt through the updates to \\(B\\) during training. This gradual adjustment is less likely to destabilize the model than a sudden, large change would.\n\n  \nImpact\n: As \\(B\\) starts updating from zero, any changes in the model’s behavior are introduced slowly. This controlled adaptation is beneficial for training dynamics, as it allows the model to incrementally learn how to incorporate new information effectively without losing valuable prior knowledge.\n\n\n\n\n\nEnsuring Controlled Updates\n\n\n\n  \nRationale\n: Gaussian initialization of \\(A\\) provides a set of initial values that, while random, are statistically regularized by the properties of the Gaussian distribution (such as having a mean of zero and a defined variance). This regularity helps in providing a balanced and predictable set of initial conditions for the adaptation process.\n\n  \nImpact\n: The Gaussian distribution helps ensure that the values in \\(A\\) are neither too large nor too biased in any direction, which could lead to disproportionate influence on the updates when \\(B\\) begins to change. This helps in maintaining a stable and effective learning process.\n\n\n\n\n\nFocused Adaptation\n\n\n\n  \nRationale\n: The low-rank matrices \\(A\\) and \\(B\\) are intended to capture the most essential aspects of the new data or tasks relative to the model’s existing capabilities. By starting with \\(B = 0\\) and \\(A\\) initialized randomly, the learning focuses on identifying and optimizing only those aspects that truly need adaptation, as opposed to re-learning aspects that the model already performs well.\n\n  \n\n    \nImpact\n: This focus helps optimize training efficiency by directing computational resources and learning efforts towards making meaningful updates that enhance the model’s capabilities in specific new areas.\n\n  \n\n  \nThis initialization strategy supports the overall goal of LoRA: to adapt large, pre-trained models efficiently with minimal resource expenditure and without compromising the foundational strengths of the original model. This approach ensures that any new learning builds on and complements the existing pre-trained model structure.\n\n\n\n\n\nFor a given task, how do we determine whether to fine-tune the attention layers or feed-forward layers?\n\n\n\n\n  \nDeciding whether to fine-tune the attention layers or the feed-forward (MLP) layers in a model adapted using LoRA involves several considerations. These include the nature of the task, the model architecture, and the distribution of parameters between attention and feed-forward layers.\n\n  \nNote that the LoRA paper originally only adapted the attention weights for downstream tasks and froze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency. Thus, the number of attention weights relative to feed-forward weights can impact the choice of .\n\n  \nHere are some key factors to guide this decision:\n\n\n\n\n\nNature of the Task\n\n\n\n  \nTask Requirements\n: Attention mechanisms are particularly effective for tasks that benefit from modeling relationships between different parts of the input, such as sequence-to-sequence tasks or tasks requiring contextual understanding. If the task demands strong relational reasoning or context sensitivity, fine-tuning attention layers might be more beneficial.\n\n  \nFeed-Forward Layer Role\n: MLPs generally focus on transforming the representation at individual positions without considering other positions. They are effective for tasks requiring more substantial non-linear transformation of features. If the task demands significant feature transformation at individual positions, MLPs may need adaptation.\n\n\n\n\n\nModel Architecture\n\n\n\n  \nProportion of Parameters\n: In transformer architectures, MLPs typically contain a larger number of parameters compared to attention mechanisms (of the order of 2x to 5x). For example, in standard configurations like those seen in BERT or GPT models, the MLPs can contain around three times more parameters than the attention layers.\n\n  \nImpact on Efficiency\n: Because MLPs are parameter-heavy, fine-tuning them can significantly increase the number of trainable parameters, impacting training efficiency and computational requirements. If parameter efficiency is a priority, you might opt to adapt only the attention layers, as originally done in the LoRA approach.\n\n\n\n\n\nComputational Constraints\n\n\n\n  \nResource Availability\n: The decision can also be influenced by available computational resources. Adapting attention layers only can save computational resources and training time, making it a preferable option when resources are limited.\n\n  \nBalance of Adaptation and Performance\n: If computational resources allow, experimenting with both components can be useful to understand which contributes more to performance improvements on specific tasks.\n\n\n\n\n\nEmpirical Testing\n\n\n\n  \nA/B Testing\n: One effective way to determine the optimal strategy for a specific model and task is to conduct empirical tests where you fine-tune the attention layers alone, the MLP layers alone, and both together in different experiments to compare the performance impacts.\n\n  \nPerformance Metrics\n: Monitoring key performance metrics specific to the task during these tests will guide which components are more critical to fine-tune.\n\n\n\n\n\nTask-Specific Research and Insights\n\n\n\n  \n\n    \nLiterature and Benchmarks\n: Insights from research papers and benchmarks on similar tasks can provide guidelines on what has worked well historically in similar scenarios. For example, tasks that require nuanced understanding of input relationships (like question answering or summarization) might benefit more from tuning attention mechanisms.\n\n  \n\n  \n\n    \nIn summary, the choice between tuning attention or MLP layers depends on the specific demands of the task, the model’s architecture, the balance of parameters, and empirical results. Considering these aspects can help in making a decision that optimizes both performance and efficiency.\n\n  \n\n\n\n\n\nAssuming we’re fine-tuning attention weights, which specific attention weight matrices should we apply LoRA to?\n\n\n\n\n  \nThe question of which attention weight matrices in the transformer architecture should be adapted using LoRA to optimize performance on downstream tasks is central to maximizing the effectiveness of parameter usage, especially when dealing with large models like GPT-3. Based on the findings reported in the LoRA paper and the specific experiment mentioned, here’s a detailed explanation and recommendation:\n\n\n\n\n\nContext and Setup\n\n\n\n\n  \nThe LoRA paper explores the adaptation of various weight matrices within the self-attention module of GPT-3 under a limited parameter budget. With a constraint of 18 million trainable parameters, the authors tested different configurations of adapting the weights associated with the query (\\(W_q\\)), key (\\(W_k\\)), value (\\(W_v\\)), and output (\\(W_o\\)) matrices. This setup allows for a comparison of the effectiveness of adapting different combinations of weights at varying ranks.\n\n\n\n\n\nExperimental Findings\n\n\n\n  \nParameter Allocation\n: The experiment considered adapting individual weight types at a rank of 8 and combinations of weights at lower ranks (4 and 2) due to the fixed parameter budget. This arrangement allowed assessing whether it is more beneficial to distribute the available parameters across multiple weight types or concentrate them on fewer weights at a higher rank.\n\n  \nPerformance Metrics\n: The validation accuracies on the WikiSQL and MultiNLI datasets served as the primary performance indicators. The results show varying degrees of success depending on which weights were adapted and how the ranks were distributed. The table below from the LoRA paper shows validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both \\(W_q\\) and \\(W_v\\) gives the best performance overall. They find the standard deviation across random seeds to be consistent for a given dataset, which they report in the first column.\n\n\n\n\n\n\n\n\nKey Results and Recommendations\n\n\n\n\n  \nSingle vs. Multiple Weight Adaptation\n: Adapting single weight matrices (\\(W_q\\), \\(W_k\\), \\(W_v\\), or \\(W_o\\) individually) at a higher rank generally resulted in lower performance compared to adapting combinations of weights at a reduced rank. Specifically, putting all parameters in ∆\\(W_q\\) or ∆\\(W_k\\) alone did not yield optimal results.\n\n  \nOptimal Combination\n: The combination of adapting both \\(W_q\\) and \\(W_v\\) at a rank of 4 emerged as the most effective strategy, achieving the highest validation accuracies on both datasets. This suggests a balanced approach to distributing the parameter budget across multiple types of attention weights, rather than focusing on a single type, leads to better performance.\n\n  \nEffectiveness of Rank Distribution\n: The result indicates that a lower rank (such as 4) is sufficient to capture essential adaptations in the weights, making it preferable to spread the parameter budget across more types of weights rather than increasing the rank for fewer weights.\n\n\n\n\n\nConclusion and Strategy for Applying LoRA\n\n\n\n\n  \nBased on these findings, when applying LoRA within a limited parameter budget, it is advisable to:\n    \n\n      \nDistribute Parameters Across Multiple Weights\n: Focus on adapting multiple types of attention weights (such as \\(W_q\\) and \\(W_v\\)) rather than a single type, as this approach leverages the synergistic effects of adapting multiple aspects of the attention mechanism.\n\n      \nUse Lower Ranks for Multiple Weights\n: Opt for a lower rank when adapting multiple weights to ensure that the parameter budget is used efficiently without compromising the ability to capture significant adaptations.\n\n    \n\n  \n\n  \nThis strategy maximizes the impact of the available parameters by enhancing more dimensions of the self-attention mechanism, which is crucial for the model’s ability to understand and process input data effectively across different tasks.\n\n\n\n\n\nIs there a relationship between setting scaling factor and rank in LoRA?\n\n\n\n\n  \nIn the LoRA framework, the relationship between the scaling factor \\(\\alpha\\) and the rank \\(r\\) of the adaptation matrices \\(A\\) and \\(B\\) is an important consideration for tuning the model’s performance and managing how adaptations are applied to the pre-trained weights. Both \\(\\alpha\\) and \\(r\\) play significant roles in determining the impact of the low-rank updates on the model, and their settings can influence each other in terms of the overall effect on the model’s behavior.\n\n\n\n\n\nUnderstanding \\(\\alpha\\) and \\(r\\)\n\n\n\n  \nScaling Factor \\(\\alpha\\)\n: This parameter scales the contribution of the low-rank updates \\(\\Delta W = BA\\) before they are applied to the original model weights \\(W\\). It controls the magnitude of changes introduced by the adaptation, effectively modulating how aggressive or subtle the updates are.\n\n  \nRank \\(r\\)\n: This determines the dimensionality of the low-rank matrices \\(A\\) and \\(B\\). The rank controls the expressiveness of the low-rank updates, with higher ranks allowing for more complex adaptations but increasing computational costs and potentially the risk of overfitting.\n\n\n\n\n\nRelationship and Interaction\n\n\n\n\n  \nBalancing Impact\n: A higher rank \\(r\\) allows the model to capture more complex relationships and nuances in the adaptations, potentially leading to more significant changes to the model’s behavior. In such cases, \\(\\alpha\\) might be adjusted downward to temper the overall impact, ensuring that the modifications do not destabilize the model’s pre-trained knowledge excessively.\n\n  \nAdjusting for Subtlety\n: Conversely, if the rank \\(r\\) is set lower, which constrains the flexibility and range of the updates, \\(\\alpha\\) may need to be increased to make the limited updates more impactful. This can help ensure that the adaptations, though less complex, are sufficient to achieve the desired performance improvements.\n\n  \nExperimental Tuning\n: The optimal settings for \\(\\alpha\\) and \\(r\\) often depend on the specific task, the dataset, and the desired balance between adapting to new tasks and retaining generalizability. Experimentation and validation are typically necessary to find the best combination.\n\n\n\n\n\nPractical Considerations\n\n\n\n  \nOverfitting vs. Underfitting\n: Higher ranks with aggressive scaling factors can lead to overfitting, especially when the model starts fitting too closely to nuances of the training data that do not generalize well. Conversely, too low a rank and/or too conservative an \\(\\alpha\\) might lead to underfitting, where the model fails to adapt adequately to new tasks.\n\n  \nComputational Efficiency\n: Higher ranks increase the number of parameters and computational costs. Balancing \\(\\alpha\\) and \\(r\\) can help manage computational demands while still achieving meaningful model improvements.\n\n\n\n\n\nConclusion\n\n\n\n  \nThe relationship between \\(\\alpha\\) and \\(r\\) in LoRA involves a delicate balance. Adjusting one can necessitate compensatory changes to the other to maintain a desired level of adaptation effectiveness without sacrificing the model’s stability or performance. Understanding how these parameters interact can significantly enhance the strategic deployment of LoRA in practical machine learning tasks.\n\n\n\n\n\nHow do you determine the optimal rank \\(r\\) for LoRA?\n\n\n\n\n  \nThe optimal rank \\(r\\) for LoRA is influenced by the specific task and the type of weight adaptation. Based on the results reported in the paper from the experiments on the WikiSQL and MultiNLI datasets:\n    \n\n      \nFor WikiSQL\n:\n        \n\n          \nWhen adapting only \\(W_q\\), the optimal rank is \\(r = 4\\), with a validation accuracy of 70.5%.\n\n          \nWhen adapting \\(W_q\\) and \\(W_v\\), the optimal rank is \\(r = 8\\), with a validation accuracy of 73.8%.\n\n          \nWhen adapting \\(W_q, W_k, W_v, W_o\\), the optimal ranks are \\(r = 4\\) and \\(r = 8\\), both achieving a validation accuracy of 74.0%.\n\n        \n\n      \n\n      \nFor MultiNLI\n:\n        \n\n          \nWhen adapting only \\(W_q\\), the optimal rank is \\(r = 4\\), with a validation accuracy of 91.1%.\n\n          \nWhen adapting \\(W_q\\) and \\(W_v\\), the optimal rank is \\(r = 8\\), with a validation accuracy of 91.6%.\n\n          \nWhen adapting \\(W_q, W_k, W_v, W_o\\), the optimal ranks are \\(r = 2\\) and \\(r = 4\\), both achieving a validation accuracy of 91.7%.\n\n        \n\n      \n\n    \n\n  \n\n  \nThe table below from the paper shows the validation accuracy on WikiSQL and MultiNLI with different rank \\(r\\) by adapting \\(\\left\\{W_q, W_v\\right\\}\\), \\(\\left\\{W_q, W_k, W_v, W_c\\right\\}\\), and just \\(W_q\\) for a comparison.. To our surprise, a rank as small as one suffices for adapting both \\(W_q\\) and \\(W_v\\) on these datasets while training \\(W_q\\) alone needs a larger \\(r\\).\n\n\n\n\n\n\n\n\n\n  \nIn summary, while the optimal rank \\(r\\) varies depending on the dataset and the type of weight adaptation, a rank of \\(r = 4\\) or \\(r = 8\\) generally yields the best performance. Specifically, a rank of \\(r = 4\\) is often sufficient for single weight types like \\(W_q\\), and a rank of \\(r = 8\\) is more effective for adapting multiple weight types such as \\(W_q\\) and \\(W_v\\).\n\n  \nHowever, a small \\(r\\) cannot be expected to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model (similar to LoRA with \\(r = d_{model}\\)) could certainly outperform LoRA with a small \\(r\\).\n\n  \nIn summary, selecting a rank that is too high can counteract the benefits of the low-rank adaptation by allowing the model to become overly complex and fit the training data too precisely. Conversely, choosing a rank that’s too low may limit the model’s ability to capture necessary information, leading to underfitting. Therefore, setting the rank in LoRA fine-tuning involves finding a balance: enough capacity to adapt to new data without overfitting.\n\n\n\n\n\nHow do LoRA hyperparameters interact with each other? Is there a relationship between LoRA hyperparameters?\n\n\n\n\n  \n\n    \nThere is a significant relationship among the hyperparameters in the Low-Rank Adaptation (LoRA) technique, particularly how they interact and influence each other to affect the adaptation and performance of the model. Understanding the interactions between these hyperparameters is crucial for effectively tuning the model to achieve desired behaviors and performance improvements. Here’s a detailed breakdown of the primary hyperparameters in LoRA and how they are interrelated:\n\n  \n\n  \nRank and Scaling Factor\n:\n    \n\n      \nHigher ranks allow \\(A\\) and \\(B\\) to capture more detailed and complex modifications. However, with increased rank, the potential for overfitting and destabilizing the original model’s behavior also rises. The scaling factor \\(\\alpha\\) often needs to be adjusted in response to the rank; a higher rank might require a smaller \\(\\alpha\\) to moderate the effect of these more complex updates.\n\n    \n\n  \n\n  \nRank and Regularization\n:\n    \n\n      \nAs the rank increases, the number of parameters in \\(A\\) and \\(B\\) also increases, which can lead to overfitting. Regularization becomes more critical with higher ranks to ensure that the model generalizes well and does not just memorize the training data.\n\n    \n\n  \n\n  \nLearning Rate and Scaling Factor\n:\n    \n\n      \nThe learning rate for \\(A\\) and \\(B\\) can influence how quickly the model adapts the low-rank updates. If \\(\\alpha\\) is high, leading to stronger updates, a lower learning rate might be necessary to prevent training instability. Conversely, with a lower \\(\\alpha\\), a higher learning rate might be feasible to ensure that the updates are sufficiently impactful.\n\n    \n\n  \n\n  \nRegularization and Learning Rate\n:\n    \n\n      \nRegularization settings might need adjustment based on the learning rate. A higher learning rate can cause larger updates, which might increase the risk of overfitting unless balanced by stronger regularization.\n\n    \n\n  \n\n\n\n\n\nPractical Considerations\n\n\n\n\n  \nTuning Strategy\n:\n    \n\n      \nTuning these hyperparameters requires careful experimentation and validation. Often, changes to one parameter necessitate adjustments to others to maintain a balanced and effective training regime.\n\n    \n\n  \n\n  \nTrade-offs\n:\n    \n\n      \nThere are trade-offs between model flexibility, training stability, computational efficiency, and the risk of overfitting. Effective management of LoRA’s hyperparameters is key to navigating these trade-offs.\n\n    \n\n  \n\n  \nApplication-Specific Adjustments\n:\n    \n\n      \nDepending on the specific requirements of the task and characteristics of the data, the optimal settings for these hyperparameters can vary significantly. Task-specific performance metrics and validation are essential to guide these adjustments.\n\n    \n\n  \n\n  \nIn summary, understanding and managing the relationships between these LoRA hyperparameters enables practitioners to finely tune their models to specific tasks without extensive retraining while leveraging pre-trained model architectures efficiently.\n\n\n\n\n\nWhy does a higher rank make it the easier to overfit?\n\n\n\n\n  \nIn LoRA-based fine-tuning, a higher rank can indeed lead to easier overfitting. To understand why, let’s break down the mechanics of LoRA and how rank affects model capacity and overfitting.\n\n  \nThe \nrank\n in LoRA determines the dimensions of these additional matrices, effectively controlling their capacity to capture information:\n    \n\n      \nLow Rank\n: Small matrices that can represent only limited information.\n\n      \nHigh Rank\n: Larger matrices with greater capacity to capture complex patterns.\n\n    \n\n  \n\n  \n\n    \nIn mathematical terms, a higher rank means more degrees of freedom in the low-rank matrices, allowing them to approximate more complex relationships in the data.\n\n  \n\n  \nHere’s why a higher rank increases overfitting in LoRA:\n    \n\n      \n\n        \nIncreased Capacity to Capture Training Noise\n: A higher rank increases the expressive power of the LoRA matrices. This means they can capture not only meaningful patterns in the training data but also noise or spurious correlations. This added capacity can lead the model to “memorize” the training data rather than generalize from it, making it prone to overfitting.\n\n      \n\n      \n\n        \nLess Regularization Effect\n: Low-rank matrices act as a form of regularization by constraining the model’s capacity to learn only the most essential patterns. When the rank is increased, this regularization effect diminishes. The model can then adjust more parameters, fitting closely to the training data distribution, which can hurt its performance on unseen data.\n\n      \n\n      \n\n        \nReduced Ability to Generalize\n: The initial idea behind LoRA is to adapt large models with minimal parameter changes to preserve generalization. By increasing the rank, we deviate from this minimalist adaptation, moving toward a more specialized adaptation to the training data. This specialization makes it harder for the model to generalize to different data distributions.\n\n      \n\n      \n\n        \nHigher Variance in Learned Features\n: With higher-rank matrices, the LoRA-based adjustments might capture a wider variety of features from the training data, leading to high variance in the learned representations. This increased variance can cause the model’s predictions to vary more significantly with small changes in the input, reducing its robustness and making it overfit the nuances of the training set.\n\n      \n\n    \n\n  \n\n\n\n\n\nDoes LoRA adapt weights in all layers?\n\n\n\n\n  \n\n    \nLoRA does not typically adapt weights across all layers of a neural network; instead, it targets specific layers, often the \nattention layers\n in large language models. This selective adaptation is a design choice aimed at balancing the effectiveness of fine-tuning with computational efficiency and minimizing the risk of overfitting. By modifying only key layers, like attention layers, LoRA efficiently focuses on layers where task-specific information is most impactful while preserving the general-purpose features of the lower layers.\n\n  \n\n  \n\n    \nLayers Typically Adapted in LoRA\n:\n\n  \n\n  \nIn the original \nLoRA implementation\n:\n    \n\n      \nAttention Layers\n: LoRA primarily targets attention layers (such as the query and value projections in transformers) because they play a critical role in capturing contextual information. By adapting only these layers, LoRA can achieve significant task-specific improvements without needing to modify the entire model.\n\n      \nFew Additional Layers (if necessary)\n: Sometimes, LoRA may extend adaptation to a few other layers (like feed-forward layers in transformers) if the new task requires it. However, this is usually done with caution to avoid overfitting and to keep the parameter footprint low.\n\n    \n\n  \n\n  \n\n    \nWhy not all layers?\n:\n\n\n    \n\n      \nComputational Efficiency\n: Adapting all layers would introduce a large number of low-rank matrices throughout the model, greatly increasing the memory and computation requirements, which LoRA is specifically designed to reduce.\n\n      \nRisk of Overfitting\n: Adapting all layers, especially the lower (more general) layers, could lead the model to overfit to the fine-tuning dataset, particularly if the dataset is small. Lower layers tend to capture general features, and adapting them might make the model too specialized, losing generalization ability.\n\n      \nFocus on Task-Specific Information\n: The upper (or top) layers of a model generally capture task-specific features, while lower layers handle more general, reusable features. LoRA’s selective adaptation focuses on adjusting only those layers where task-specific learning is most beneficial.\n\n    \n\n  \n\n\n\n\n\nDoes LoRA impact lower attention layers less than higher attention layers?\n\n\n\n\n  \n\n    \nYes, in practice, LoRA impacts higher attention layers more than lower ones. This is because LoRA selectively adapts layers, targeting the task-specific adaptability of higher layers while preserving the general-purpose features in lower layers. This design enables effective task adaptation with minimal overfitting, allowing the model to retain broad applicability.\n\n  \n\n  \n\n    \nWhy higher attention layers are more affected:\n\n\n    \n\n      \n\n        \nFunction of Higher Attention Layers\n: Higher attention layers (those closer to the output) tend to capture more task-specific, abstract information. During fine-tuning, LoRA modifies these layers to incorporate new task-related features. Adjustments here have a greater impact on the model’s output because these layers process information in a way that directly influences final predictions.\n\n      \n\n      \n\n        \nLess Impact on Lower Layers\n: Lower layers (closer to the input) generally focus on extracting basic, general features. For example, in language models, they capture fundamental linguistic structures like syntax and word relationships. Since these lower layers capture foundational patterns, they benefit less from task-specific adaptations. Fine-tuning these lower layers with LoRA could lead to a loss of generalizable features, which would reduce the model’s ability to transfer across tasks.\n\n      \n\n      \n\n        \nLoRA’s Selective Impact\n: LoRA is typically implemented on a subset of attention heads or specific projections within the attention mechanism (e.g., the query and value projections). Even when applied across all layers, the task-specific nature of fine-tuning tends to have a more pronounced effect on the higher layers, which adapt more flexibly to new data patterns.\n\n      \n\n      \n\n        \nRegularization Effect in Lower Layers\n: Because LoRA introduces a low-rank constraint, it inherently acts as a form of regularization. Lower layers, which are already constrained to represent general features, become even more regularized. This further reduces the likelihood of significant changes in these layers and minimizes the effect of LoRA on them.\n\n      \n\n    \n\n  \n\n  \n\n    \nPractical Implications:\n\n  \n\n  \n\n    \nIn many cases, fine-tuning with LoRA results in:\n\n    \n\n      \nMajor adjustments\n to higher layers, allowing the model to learn specific features of the fine-tuning task.\n\n      \nMinimal impact\n on lower layers, preserving general knowledge from pre-training and preventing overfitting.\n\n    \n\n  \n\n\n\n\n\nQuantized Low-Rank Adaptation (QLoRA)\n\n\n\n\n  \nProposed in \nQLoRA: Efficient Finetuning of Quantized LLMs\n.\n\n  \nThis paper by Dettmers et al. from UW presents QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. Put simply, QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\n\n  \nPut simply, QLoRA is a method designed to efficiently fine-tune large pre-trained language models (LLMs), like a 65B parameter model, on limited GPU memory without sacrificing performance. It combines the principles of Low-Rank Adaptation (LoRA) with innovative 4-bit NormalFloat (NF4) quantization and Double Quantization techniques, optimizing parameter efficiency and computational resource utilization.\n\n  \nAt a top-level, QLoRA operates based on the following steps:\n    \n\n      \nQuantize the pre-trained model to 4 bits and freeze it.\n\n      \nAttach small, trainable adapter layers (similar to LoRA).\n\n      \nFinetune only the adapter layers while using the frozen quantized model for context.\n\n    \n\n  \n\n  \nKey Components:\n\n    \n\n      \nLow-Rank Adaptation:\n QLoRA follows LoRA’s strategy of injecting trainable low-rank matrices into the architecture of pretrained LLMs, specifically targeting Transformer layers. This selective fine-tuning strategy focuses on optimizing these low-rank matrices rather than the entire model, reducing the number of trainable parameters and computational costs.\n\n      \nQuantization:\n The distinguishing aspect of QLoRA lies in its quantization approach, which includes:\n        \n\n          \nNF4 Quantization: This technique involves quantizing the model weights to 4-bit NormalFloat (NF4), efficiently compressing them to fit a specific distribution suitable for NF4 without complex algorithms.\n\n          \nDouble Quantization: This secondary quantization further reduces memory overhead by quantizing the quantization constants themselves, using 8-bit floats with a 256 block size, achieving significant memory savings without affecting model performance.\n\n        \n\n      \n\n    \n\n  \n\n  \nOperation:\n\n    \n\n      \nQLoRA employs a frozen, 4-bit quantized pretrained language model and fine-tunes it by backpropagating gradients into the low rank adapters. This method optimizes computation through low-bit quantization and reduces the number of parameters by using low-rank structures, striking a balance between efficiency and performance.\n\n    \n\n  \n\n  \nTheir best model family, which they name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes.\n\n  \nThey use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models).\n\n  \nTheir results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. They provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, they find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT.\n\n  \nThe figure below from the paper shows different finetuning methods and their memory requirements. QLORA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\n\n\n\n\n\n\n\n\n\n  \nIn the QLoRA approach, it is the original model’s weights that are quantized to 4-bit precision. The newly added Low-rank Adapter (LoRA) weights are not quantized; they remain at a higher precision and are fine-tuned during the training process. This strategy allows for efficient memory use while maintaining the performance of large language models during finetuning.\n\n\n\n\n\n\n  \nTo learn more about QLoRA and how it works, \nthis\n Hugging Face blog post is highly recommended.\n\n\n\n\n\nQuantization-Aware Low-Rank Adaptation (QA-LoRA)\n\n\n\n\n  \nProposed in \nQA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models\n.\n\n  \nRecent advancements in large language models (LLMs) have significantly improved their capabilities in various language-understanding tasks. However, the deployment of these models, especially on edge devices, is hampered by their substantial computational requirements.\n\n  \nThis paper by Xu et al. from Huawei seeks to address the aforementioned issue and proposes a quantization-aware low-rank adaptation (QA-LoRA) algorithm, a technique that aims to mitigate the computational burden by efficiently fine-tuning low-bit diffusion models without compromising accuracy. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM’s weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy.\n\n  \nPut simply, QA-LoRA stands out by incorporating a quantization-aware approach that merges and quantizes the weights of Low-Rank Adaptation (LoRA) with the full-precision model weights. This process not only optimizes memory usage and computational efficiency during inference but also ensures the seamless integration of LoRA and auxiliary weights into a quantized model. Notably, QA-LoRA allows for the reduction of weight precision (e.g., to INT4, INT3, and INT2) during fine-tuning, significantly decreasing time and memory usage while maintaining accuracy levels, as it eliminates the need for post-training quantization.\n\n  \nThe algorithm operates through several key steps:\n    \n\n      \nAdding LoRA Weights\n: LoRA weights are introduced to the pre-trained model, enhancing its adaptability.\n\n      \nFine-Tuning LoRA Weights\n: These weights are then specifically fine-tuned, which involves updating the LoRA weights while the original model’s weights remain unchanged.\n\n      \nMerging LoRA and Original Model Weights\n: Subsequently, the fine-tuned LoRA weights are merged with the model’s original weights.\n\n      \nQuantization\n: Finally, the combined weight set is quantized to a lower-bit format, which is essential for reducing both memory and computational costs.\n\n    \n\n  \n\n  \nThe following figure from the paper shows an illustration of the goal of QA-LoRA. Compared to prior adaptation methods, LoRA and QLoRA, QA-LoRA is computationally efficient in both the fine-tuning and inference stages. More importantly, it does not suffer an accuracy loss because post-training quantization is not required. They display INT4 quantization in the figure, but QA-LoRA can be generalized to INT3 and INT2.\n\n\n\n\n\n\n\n\n\n  \nQA-LoRA’s effectiveness has been validated across different fine-tuning datasets and downstream scenarios, particularly with the LLaMA and LLaMA2 model families. Its unique integration of quantization-aware techniques with low-rank adaptation principles marks a significant advancement in the fine-tuning of diffusion models for low-bit settings. This approach not only addresses the challenges posed by the computational demands of LLMs but also opens up new possibilities for deploying these models more efficiently and effectively on a wider range of devices.\n\n  \nThe implementation of QA-LoRA is straightforward and can be achieved with a minimal addition to the existing codebase, showcasing its practicality for widespread adoption. Further details, including code examples, are available on their GitHub repository, making it accessible for researchers and practitioners aiming to leverage the benefits of this innovative adaptation technique.\n\n  \nCode\n\n\n\n\n\nRefined Low-Rank Adaptation (ReLoRA)\n\n\n\n\n  \nProposed in \nStack More Layers Differently: High-Rank Training Through Low-Rank Updates\n by Lialin et al. from UMass Lowell.\n\n  \nRefined Low-Rank Adaptation (ReLoRA) is a low-rank training technique as an alternative approach to training large neural networks. ReLoRA utilizes low-rank updates to train high-rank networks. Put simply, they explore whether LoRA can be used for pretraining (as opposed to finetuning) LLMs in a parameter-efficient manner.\n\n  \nThey apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training.\n\n  \nFurthermore, they observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Their findings shed light on the potential of low-rank training techniques and their implications for scaling laws.\n\n  \nA caveat worth mentioning is that the researchers only pretrained models up to 350 M parameters for now (the smallest Llama 2 model is 7B parameters, for comparison).\n\n  \nThe following figure \n(source)\n presents an overview of their results:\n\n\n\n\n\n\n\n\nS-LoRA: Serving Thousands of Concurrent LoRA Adapters\n\n\n\n\n  \nThis paper by Sheng et al. from UC Berkeley, Stanford, and Shanghai Jiao Tong focuses on the scalable serving of LoRA (Low-Rank Adaptation) adapters for large language models (LLMs).\n\n  \nThe “pretrain-then-finetune” paradigm, widely adopted in deploying LLMs, leads to numerous fine-tuned variants, presenting significant opportunities for batched inference during serving. The paper introduces S-LoRA, a system designed for this purpose.\n\n  \nS-LoRA addresses memory management challenges by storing all adapters in main memory and fetching them to GPU memory as needed. The system employs Unified Paging, a unified memory pool managing dynamic adapter weights and KV cache tensors, to reduce memory fragmentation and I/O overhead.\n\n  \nThe paper presents a novel tensor parallelism strategy and customized CUDA kernels for efficient heterogeneous batching of LoRA computations, enabling the serving of thousands of adapters on a single or multiple GPUs with minimal overhead.\n\n  \nThe following image from the paper shows separated batched computation for the base model and LoRA computation. The batched computation of the base model is implemented by GEMM. The batched computation for LoRA adapters is implemented by custom CUDA kernels which support batching various sequence lengths and adapter ranks.\n\n\n\n\n\n\n\n\n\n  \nThe following image from the paper shows an overview of memory allocation in S-LoRA. S-LoRA stores all adapters in the main memory and fetches the active adapters for the current batch to the GPU memory. The GPU memory is used to store the KV cache, adapter weights, base model weights, and other temporary tensors.\n\n\n\n\n\n\n\n\n\n  \nS-LoRA’s performance is evaluated against state-of-the-art libraries like Weights PEFT and vLLM, showing up to 4 times higher throughput and the capability to serve significantly more adapters.\n\n  \nThe system is effective in reducing the training and communication costs in Federated Learning, making it a promising approach for deploying large language models in resource-constrained environments.\n\n  \nThis paper contributes significantly to the field of machine learning by presenting a novel and efficient method for serving a large number of LoRA adapters, a crucial aspect in the deployment of large-scale language models.\n\n  \nCode\n\n\n\n\n\nPredibase\n\n\n\n\n  \nSimilar to S-LoRA, \nPredibase\n, a startup, offers a unique serving infrastructure – \nLoRAX\n – which lets you cost-effectively serve many fine-tuned adapters on a single GPU in dedicated deployments.\n\n\n\n\n\nWeight-Decomposed Low-Rank Adaptation (DoRA)\n\n\n\n\n  \nProposed in \nDoRA: Weight-Decomposed Low-Rank Adaptation\n by Liu et al. from  NVIDIA and HKUST.\n\n  \nWeight-Decomposed Low-Rank Adaptation (DoRA) is a novel Parameter-Efficient Fine-Tuning (PEFT) method that surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs.\n\n  \nThe authors’ weight decomposition analysis reveals fundamental differences between full fine-tuning and LoRA, showing that directional updates play a crucial role in learning capability. DoRA employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.\n\n  \nDoRA demonstrates superior performance across a range of tasks, including commonsense reasoning, visual instruction tuning, and image/video-text understanding, across models like LLaMA, LLaVA, and VL-BART. It achieves this by effectively managing the trade-off between the number of trainable parameters and learning capacity, without adding inference overhead.\n\n  \nThe following figure from the paper illustrates an overview of DoRA, which decomposes the pre-trained weight into magnitude and direction components for fine-tuning, especially with LoRA to efficiently update the direction component. Note that \\(\\|\\cdot\\|_c\\) denotes the vector-wise norm of a matrix across each column vector.\n\n\n\n\n\n\n\n\n\n  \nExperiments show that DoRA not only outperforms LoRA but also matches or exceeds the performance of full fine-tuning across different tasks, with significant improvements in commonsense reasoning tasks and multimodal understanding, illustrating its effectiveness and efficiency.\n\n  \nThe paper also explores DoRA’s compatibility with other LoRA variants, such as VeRA, and demonstrates its adaptability across different training sizes and rank settings, further establishing its utility as a versatile and powerful fine-tuning method.\n\n  \nBlog\n\n\n\n\n\nSummary of LoRA Techniques\n\n\n\n\n  \nThe following section is inspired from Cameron Woulfe’s \n(source)\n post.\n\n  \nHere’s an overview of some prevalent variants of LoRA techniques:\n    \n\n      \nLoRA\n models the update derived for a model’s weights during finetuning with a low rank decomposition, implemented in practice as a pair of linear projections. LoRA leaves the pretrained layers of the LLM fixed and injects a trainable rank decomposition matrix into each layer of the model.\n\n      \nQLoRA\n is (arguably) the most popular LoRA variant and uses model quantization techniques to reduce memory usage during finetuning while maintaining (roughly) equal levels of performance. QLoRA uses 4-bit quantization on the pretrained model weights and trains LoRA modules on top of this. In practice, QLoRA saves memory at the cost of slightly-reduced training speed.\n\n      \nQA-LoRA\n is an extension of LoRA/QLoRA that further reduces the computational burden of training and deploying LLMs. It does this by combining parameter-efficient finetuning with quantization (i.e., group-wise quantization applied during training/inference).\n\n      \nLoftQ\n studies a similar idea to QA-LoRA – applying quantization and LoRA finetuning on a pretrained model simultaneously.\n\n      \nLongLoRA\n attempts to cheaply adapt LLMs to longer context lengths using a parameter-efficient (LoRA-based) finetuning scheme. In particular, we start with a pretrained model and finetune it to have a longer context length. This finetuning is made efficient by:\n        \n\n          \nUsing sparse local attention instead of dense global attention (optional at inference time).\n\n          \nUsing LoRA (authors find that this works well for context extension).\n\n        \n\n      \n\n      \nS-LoRA\n aims to solve the problem of deploying multiple LoRA modules that are used to adapt the same pretrained model to a variety of different tasks. Put simply, S-LoRA does the following to serve thousands of LoRA modules on a single GPU (or across GPUs):\n        \n\n          \nStores all LoRA modules in main memory.\n\n          \nPuts modules being used to run the current query into GPU memory.\n\n          \nUses unified paging to allocate GPU memory and avoid fragmentation.\n\n          \nProposes a new tensor parallelism strategy to batch LoRA computations.\n\n        \n\n      \n\n      \n**ReLoRA\n refines neural network training by iteratively applying low-rank updates to achieve high-rank performance, streamlining the process for large models.\n\n      \nDoRA\n surpasses existing techniques like LoRA by decomposing pre-trained weights into magnitude and directional components for efficient fine-tuning. This method is designed to bridge the accuracy gap between LoRA-based methods and full fine-tuning, without increasing inference costs. It employs LoRA for directional updates and introduces trainable magnitude components, enhancing learning capacity and stability.\n\n      \nMany other LoRA variants exist as well:\n        \n\n          \nLQ-LoRA:\n uses a more sophisticated quantization scheme within QLoRA that performs better and can be adapted to a target memory budget.\n\n          \nMultiLoRA:\n extension of LoRA that better handles complex multi-task learning scenarios.\n\n          \nLoRA-FA:\n freezes half of the low-rank decomposition matrix (i.e., the A matrix within the product AB) to further reduce memory overhead.\n\n          \nTied-LoRA:\n leverages weight tying to further improve the parameter efficiency of LoRA.\n\n          \nGLoRA:\n extends LoRA to adapt pretrained model weights and activations to each task in addition to an adapter for each layer.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\n\n\n\nLow-rank Linear Subspace ReFT (LoReFT)\n\n\n\n\n  \nProposed in \nReFT: Representation Finetuning for Language Models\n by Wu et al. from Stanford and the Pr(Ai)2R Group.\n\n  \nRepresentation Finetuning (ReFT) is a suite of methods to modify the hidden representations of language models (LMs) for task-specific adaptation. Unlike traditional parameter-efficient finetuning (PEFT) methods that adapt by modifying weights, ReFT manipulates a small fraction of model representations, enhancing the interpretability and flexibility of the interventions.\n\n  \nA key variant within ReFT, named Low-rank Linear Subspace ReFT (LoReFT), leverages a low-rank projection matrix to edit representations in a linear subspace. This approach is demonstrated to be 10\\(\\times\\)–50\\(\\times\\) more parameter-efficient compared to existing state-of-the-art PEFTs like LoRA.\n\n  \nThe ReFT methodology, specifically Low-rank Linear Subspace ReFT (LoReFT), operates by editing hidden representations in a linear subspace. LoReFT modifies these representations using a projection matrix \\(R\\), which redefines them in a low-dimensional subspace for efficiency. The matrix \\(R\\) has orthonormal rows, which are crucial for maintaining the quality of the intervention without adding much complexity.\n\n  \nThe core intervention of LoReFT, as per the distributed interchange intervention (DII) formula \\(DII(b, s, R) = b + R^\\top(Rs - Rb)\\), leverages the projection matrix to adjust the hidden states \\(b\\) towards a target state \\(s\\) by the application of \\(R\\). This intervention is designed to manipulate the model output towards desired behaviors or answers subtly and effectively.\n\n  \nLoReFT employs a linear transformation defined by the parameters \\(W\\) and \\(b\\) (not to be confused with the bias term), which projects the representation into the subspace before it is edited. This transformation helps in aligning the representation more closely with the task-specific features that are crucial for performance.\n\n  \nPractically, LoReFT is implemented as a set of non-overlapping interventions across multiple layers of a Transformer-based model. These interventions are strategically placed to modify the behavior of the model without extensive retraining of the underlying parameters.\n\n  \nEach intervention is applied after the computation of layer \\(L\\) representations, meaning it directly affects the computation of subsequent layers \\(L+1\\) to \\(L+m\\). This placement ensures that the interventions have a cascading effect, enhancing their impact on the final model output.\n\n  \nThe hyperparameter tuning for LoReFT focuses on the number and placement of interventions across the layers, optimizing both the effectiveness of each intervention and the overall computational overhead. This involves selecting the appropriate number of prefix and suffix positions in the input where interventions are most beneficial, as well as deciding on the layers where these modifications will have the most impact.\n\n  \nThe figure below from the paper shows an illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function \\(\\Phi\\) is applied to hidden representations at positions \\(P\\) in layer \\(L\\). (2) The right panel depicts the hyperparameters we tune when experimenting with LoReFT. Specifically, the figure depicts application of LoReFT at all layers with prefix length \\(p\\) = 2 and suffix length \\(s\\) = 2. When not tying layer weights, we train separate intervention parameters at each position and layer, resulting in 16 interventions with unique parameters in this example.\n\n\n\n\n\n\n\n\n\n  \nThe authors evaluate LoReFT across multiple domains, including commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding. It is shown that LoReFT achieves competitive or superior performance on all tasks, especially shining in commonsense reasoning benchmarks.\n\n  \nImplementation details reveal that LoReFT interventions are applied at selected layers and positions within the LM, optimizing both the number of interventions and their locations through hyperparameter tuning. This targeted approach allows for minimal additional computational overhead at inference.\n\n  \nLoReFT is implemented in a publicly available Python library, \npyreft\n, which facilitates the adoption of ReFT methods by providing tools to apply these interventions on any pretrained LM from the HuggingFace model hub.\n\n  \nThe paper establishes the potential of representation-focused finetuning as a more effective alternative to weight-based methods, setting new standards for efficiency and performance in adapting large-scale LMs to diverse tasks.\n\n\n\n\n\nStratified Progressive Adaptation Fine-tuning (SPAFIT)\n\n\n\n\n  \nProposed in \nSPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models\n by Arora and Wang from Simon Fraser University, Stratified Progressive Adaptation Fine-tuning (SPAFIT) is a novel Parameter-Efficient Fine-Tuning (PEFT) method aimed at optimizing the fine-tuning process of Transformer-based large language models by localizing the fine-tuning to specific layers according to their linguistic knowledge importance. This addresses issues like catastrophic forgetting and computational inefficiency common in full fine-tuning methods.\n\n  \nSPAFIT organizes the model into three groups of layers, with increasing complexity of fine-tuning allowed as the layers progress from basic linguistic processing to more task-specific functions. Group 1 layers remain completely frozen, Group 2 layers undergo fine-tuning only on bias terms, and Group 3 layers are fine-tuned using both BitFit for simple parameters and Low-Rank Adaptation (LoRA) for more significant weight matrices.\n\n  \nThe authors conducted experiments using the BERT-large-cased model across nine tasks from the GLUE benchmark. Their results demonstrate that SPAFIT can achieve or exceed the performance of full fine-tuning and other PEFT methods like Full BitFit and Full LoRA while fine-tuning significantly fewer parameters.\n\n  \nThe figure below from the paper illustrates an example implementation of SPAFIT on BERT.\n\n\n\n\n\n\n\n\n\n  \nNotable results include SPAFIT models achieving the best performance on tasks involving sentence similarity, like MRPC and STS-B, and showing a substantial reduction in the number of parameters fine-tuned—highlighting SPAFIT’s efficiency.\n\n  \nThe research suggests that different types of linguistic knowledge can indeed be localized to specific layers of a language model, potentially leading to more targeted and efficient fine-tuning strategies.\n\n  \nThe paper raises points for future investigation, including the application of SPAFIT to more complex tasks like summarization and to models that contain both encoder and decoder architectures. The study also acknowledges the need for further analysis on the optimal balance of parameter efficiency against task performance and the extent of adaptation necessary at different layers.\n\n\n\n\n\nBitFit\n\n\n\n\n  \nProposed in \nBitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models\n by Ben-Zaken et al.  from Yoav Goldberg’s group at Bar Ilan University and the Allen Institute for Artificial Intelligence introduces BitFit, a fine-tuning method for pre-trained BERT models. \nBitFit focuses on updating only the bias-terms of the model, which are a minimal fraction of the model’s parameters, effectively reducing the memory footprint and computational demands typically associated with full model fine-tuning.\n\n  \nBitFit’s methodology leverages the observation that fine-tuning often doesn’t require extensive retraining of all parameters. Instead, fine-tuning only the bias terms achieves competitive results compared to full model fine-tuning, especially with small to medium-sized datasets. In scenarios permitting slight performance degradation, the method can be constrained to adjust only two specific types of bias terms, representing just 0.04% of the total model parameters.\n\n  \nImplementation details include freezing the transformer-encoder’s main weights and training only the bias terms along with a task-specific classification layer. This approach allows the model to handle multiple tasks efficiently in a streaming fashion without requiring simultaneous access to all task datasets.\n\n  \nExperimental results on the GLUE benchmark show that BitFit is comparable or superior to full fine-tuning in several NLP tasks. It also outperforms other parameter-efficient methods like Diff-Pruning and Adapters in terms of the number of parameters modified, showcasing its effectiveness in achieving high performance with significantly fewer trainable parameters.\n\n  \nThe findings underscore the potential of focusing fine-tuning efforts on a small subset of parameters, specifically bias terms, to maintain or even enhance performance while minimizing computational costs. This approach also prompts further exploration of the role of bias terms in neural networks and their impact on model behavior and task transferability.\n\n\n\n\n\nNOLA\n\n\n\n\n  \nProposed in \nNOLA: Compressing LoRA Using Linear Combination of Random Basis\n by Koohpayegani et al. in ICLR 2024, NOLA is a novel method for compressing large language models (LLMs) that addresses the limitations of Low-Rank Adaptation (LoRA). NOLA reparameterizes the rank-decomposition matrices used in LoRA through linear combinations of randomly generated basis matrices, significantly reducing the parameter count by optimizing only the mixture coefficients.\n\n  \nNOLA decouples the number of trainable parameters from both the rank choice and network architecture, unlike LoRA, where parameters are inherently dependent on the matrix dimensions and rank, which must be an integer. This method not only preserves the adaptation quality but also allows for extreme compression, achieving up to 20 times fewer parameters than the most compressed LoRA models without loss of performance.\n\n  \nThe method’s implementation includes using a pseudo-random number generator for creating basis matrices, where the generator’s seed and the linear coefficients are stored, greatly reducing storage requirements. Quantization of these coefficients further minimizes storage needs without impacting model performance.\n\n  \nThe figure below from the paper shows the process that NOLA follows. After constraining the rank of \\(\\Delta W\\) by decomposing it to \\(A \\times B\\), we reparametrize A and B to be a linear combination of several random basis matrices. We freeze the basis and W and learn the combination coefficients. To reconstruct the model, we store the coefficients and the seed of the random generator which is a single scalar. NOLA results in more compression compared to LoRA and more importantly decouples the compression ratio from the rank and dimensions of W. One can reduce the number of parameters to 4 times smaller than rank=1 of LoRA which is not possible with LoRA due to rank being an integer number.\n\n\n\n\n\n\n\n\n\n  \nDetailed experimental evaluations across several tasks and models, including GPT-2 and LLaMA-2, showcase NOLA’s effectiveness. It maintains or exceeds benchmark metrics such as BLEU and ROUGE-L while using significantly fewer parameters compared to both LoRA and full model fine-tuning.\n\n  \nThe approach’s versatility is demonstrated through its application not only in natural language processing tasks but also in adapting Vision Transformer (ViT) models for image classification, indicating its potential widespread applicability across different types of deep learning architectures.\n\n  \nCode\n\n\n\n\n\nMatrix of Rank Adaptation (MoRA)\n\n\n\n\n  \nProposed in \nMoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning\n by Jiang et al. from Beihang University and Microsoft introduces a novel method, MoRA (Matrix of Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique for LLMs. The authors identify limitations in existing PEFT methods, particularly Low-Rank Adaptation (LoRA), which may restrict LLMs’ ability to learn and retain new knowledge. To address these issues, MoRA employs a high-rank updating mechanism using a square matrix to achieve greater flexibility and effectiveness without increasing the number of trainable parameters.\n\n  \nMoRA utilizes non-parameterized operators to adjust input and output dimensions, ensuring the weight can be integrated back into LLMs like LoRA. The method involves the following steps:\n    \n\n      \nReduction of Input Dimension\n: Non-parameter operators reduce the input dimension for the square matrix.\n\n      \nIncrease of Output Dimension\n: Corresponding operators increase the output dimension, maintaining the number of trainable parameters while achieving high-rank updates.\n\n    \n\n  \n\n  \nThe figure below from the paper illustrates an overview of our method compared to LoRA under same number of trainable parameters. \\(W\\) is the frozen weight from model. \\(A\\) and \\(B\\) are trainable low-rank matrices in LoRA. \\(M\\) is the trainable matrix in our method. Gray parts are non-parameter operators to reducing the input dimension and increasing the output dimension. \\(r\\) represents the rank in two methods.\n\n\n\n\n\n\n\n\n\n  \nThe authors comprehensively evaluate MoRA across five tasks—instruction tuning, mathematical reasoning, continual pretraining, memory, and pretraining—demonstrating that MoRA outperforms LoRA in memory-intensive tasks and achieves comparable performance in other areas.\n\n  \nTechnical Details and Implementation:\n\n    \n\n      \nLow-Rank Limitation in LoRA\n: LoRA uses low-rank matrices to approximate full-rank updates, limiting its capacity to store new information, especially in memory-intensive tasks. The low-rank matrices A and B in LoRA struggle to fully capture the complexity needed for tasks requiring substantial knowledge enhancement.\n\n      \nHigh-Rank Updating in MoRA\n: MoRA replaces the low-rank matrices with a square matrix, significantly increasing the rank and thus the capacity for updates. For example, LoRA with rank 8 employs matrices \\(A \\in \\mathbb{R}^{4096 \\times 8}\\) and \\(B \\in \\mathbb{R}^{8 \\times 4096}\\), while MoRA uses a square matrix \\(M \\in \\mathbb{R}^{256 \\times 256}\\), achieving a higher rank with the same number of parameters.\n\n      \nCompression and Decompression Functions\n: MoRA employs various methods to implement compression and decompression functions, including truncation, sharing rows/columns, reshaping, and rotation. These methods help reduce the input dimension and increase the output dimension effectively.\n\n      \nRotation Operators\n: Inspired by RoPE (Rotary Position Embedding), MoRA introduces rotation operators to differentiate inputs, enhancing the expressiveness of the square matrix.\n\n    \n\n  \n\n  \nEvaluation and Results:\n\n    \n\n      \nMemory Task\n: In memorizing UUID pairs, MoRA showed significant improvements over LoRA with the same number of trainable parameters. MoRA required fewer training steps to achieve high accuracy compared to LoRA, demonstrating its effectiveness in memory-intensive tasks.\n\n      \nFine-Tuning Tasks\n: MoRA was evaluated on instruction tuning (using Tülu v2 dataset), mathematical reasoning (using MetaMath, GSM8K, MATH), and continual pretraining (in biomedical and financial domains). It matched LoRA’s performance in instruction tuning and mathematical reasoning but outperformed LoRA in continual pretraining tasks, benefiting from high-rank updating.\n\n      \nPretraining\n: MoRA and a variant, ReMoRA (which merges updates back into the model during training), were evaluated on pretraining transformers from scratch on the C4 dataset. MoRA showed better pretraining loss and perplexity metrics compared to LoRA and ReLoRA, further validating the advantages of high-rank updating.\n\n    \n\n  \n\n  \nMoRA addresses the limitations of low-rank updates in LoRA by employing high-rank matrices, significantly enhancing the model’s capacity to learn and memorize new knowledge. This method shows promise for improving parameter-efficient fine-tuning of LLMs, especially in memory-intensive and domain-specific tasks. The authors provide comprehensive implementation details and empirical evaluations, establishing MoRA as an effective advancement in the field of PEFT.\n\n\n\n\n\nWhich PEFT Technique to Choose: A Mental Model\n\n\n\n\n  \nChoosing a PEFT involves simply matching them with your objectives as shown in the figure below.\n\n\n\n\n\n\n\n\nSoft Prompt Tuning\n\n\n\n\n  \n\n    \nWhat:\n Soft Prompt tuning involves adding a small trainable prefix to the input of the pre-trained LLM during fine-tuning, which modifies the representation learned by the pre-trained model to better suit the downstream task.\n\n  \n\n  \n\n    \nWhen to use:\n Prompt Tuning is a good choice when you have a large pre-trained LLM but want to fine-tune it for multiple different downstream tasks at inference time with minimal computational resources. It is also useful when you want to generate diverse and high-quality text outputs based on specific prompts.\n\n  \n\n\n\n\n\nPrefix Tuning\n\n\n\n\n  \n\n    \nWhat:\n Prefix Tuning involves learning a set of trainable parameters that modify the pre-trained LLM’s hidden states in response to task-specific prompts during inference, effectively fine-tuning the model at inference time.\n\n  \n\n  \n\n    \nWhen to use:\n When you want to fine-tune a pre-trained LLM for a specific downstream task and have limited computational resources when you want to modify the representation learned by the pre-trained model for a particular task.\n\n  \n\n\n\n\n\nAdapters\n\n\n\n\n  \n\n    \nWhat:\n Adapters are tiny modules that are added to pre-trained LLMs, typically between the pre-trained layers, to adapt the model to new downstream tasks. During fine-tuning, only the weights of the adapter are learned, while the pre-trained model’s parameters remain fixed.\n\n  \n\n  \n\n    \nWhen to use:\n When you need to fine-tune multiple downstream tasks on the same pre-trained model. Additionally, Adapters are flexible and can be quickly and easily plugged into different parts of the pre-trained model without requiring major modifications.\n\n  \n\n\n\n\n\nBitFit\n\n\n\n\n  \n\n    \nWhat:\n BitFit simplifies the fine-tuning process by only updating the bias terms of the model, reducing the number of parameters that need to be modified.\n\n  \n\n  \n\n    \nWhen to use:\n BitFit is an excellent choice when computational resources are a constraint or when working with smaller datasets. It’s especially suited for tasks where slight performance compromises are acceptable in exchange for greater efficiency.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nBias-Only Training:\n By focusing on updating only the bias terms, BitFit significantly lowers the computational demands and memory usage.\n\n      \nEfficient Adaptability:\n This method achieves comparable results to more extensive fine-tuning methods with far fewer parameter updates, making it ideal for rapid deployment and iterative development.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nFreezing Main Weights:\n The main weights of the Transformer encoder are frozen, preserving the pre-trained knowledge.\n\n      \nBias Term Training:\n Only the bias terms are fine-tuned along with a task-specific classification layer, providing an efficient way to adapt the model to new tasks.\n\n      \nEvaluation Across Tasks:\n BitFit’s efficacy is tested on various NLP tasks, showing its capability to maintain high performance with minimal parameter adjustments.\n\n    \n\n  \n\n\n\n\n\nLoRA\n\n\n\n\n  \n\n    \nWhat:\n LoRA (Low-Rank Adaptation) is a technique that modifies the pre-trained LLM’s attention mechanism during fine-tuning by introducing a low-rank matrix factorization that learns task-specific attention patterns.\n\n  \n\n  \n\n    \nWhen to use:\n LoRA is a good choice when you want to fine-tune a pre-trained LLM for a specific downstream task that requires task-specific attention patterns. It is also useful when you have limited computational resources and want to reduce the number of trainable parameters in the model. Specifically:\n\n    \n\n      \nMemory Efficiency is Desired but Not Critical: LoRA offers substantial savings in terms of parameters and computational requirements. If you’re looking to achieve a balanced reduction in trainable parameters without diving into the complexities of quantization, LoRA is an ideal choice.\n\n      \nReal-time Application: LoRA ensures no added inference latency, making it suitable for real-time applications.\n\n      \nTask-Switching is Required: LoRA can share the pretrained model across multiple tasks, reducing the need for maintaining separate models for each task.\n\n    \n\n  \n\n\n\n\n\nQLoRA\n\n\n\n\n  \n\n    \nWhat:\n QLoRA (Quantized Low-Rank Adaptation) is an advanced fine-tuning technique that integrates quantization with low-rank adaptation, allowing for efficient fine-tuning of large language models with significantly reduced memory usage.\n\n  \n\n  \n\n    \nWhen to use:\n QLoRA is ideal for scenarios where memory and computational efficiency are paramount, particularly when fine-tuning very large models on limited hardware. It is especially useful when working with low-bit model environments or when full 16-bit fine-tuning would be prohibitively expensive.\n\n    \n\n      \nKey Features:\n\n        \n\n          \n4-bit Quantization:\n QLoRA uses a novel 4-bit NormalFloat (NF4) quantization, optimized for normally distributed weights, to reduce the memory footprint.\n\n          \nDouble Quantization:\n This technique further reduces memory usage by quantizing the quantization constants.\n\n          \nPaged Optimizers:\n These manage memory spikes during gradient checkpointing, enabling stable fine-tuning on a single GPU.\n\n        \n\n      \n\n      \nProcess:\n\n        \n\n          \nModel Quantization:\n The pre-trained model is quantized to 4-bit precision using NF4.\n\n          \nAdding LoRA Weights:\n LoRA weights are integrated into the quantized model.\n\n          \nFine-Tuning:\n The LoRA weights are fine-tuned, with gradients backpropagated through the frozen quantized model.\n\n          \nDouble Quantization:\n Quantization constants are further quantized to minimize memory usage.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nQA-LoRA\n\n\n\n\n  \n\n    \nWhat:\n QA-LoRA is a specialized technique for fine-tuning low-bit diffusion models. It integrates quantization-aware strategies with Low-Rank Adaptation (LoRA) principles, providing an efficient way to handle low-bit model environments.\n\n  \n\n  \n\n    \nWhen to use:\n Ideal for scenarios where the primary goal is to optimize memory usage and computational efficiency in low-bit settings. This method is particularly effective when traditional fine-tuning approaches fall short due to the constraints of low-bit environments.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nQuantization-Aware Approach:\n QA-LoRA uniquely combines LoRA weights with full-precision model weights, then jointly quantizes them, enhancing memory and computational efficiency during inference.\n\n      \nEfficient for Low-Bit Models:\n Tailored for low-bit diffusion models, it addresses the specific challenges posed by these environments, making it a standout choice in such contexts.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nAdding LoRA Weights:\n QA-LoRA begins by integrating LoRA weights into the pre-trained model.\n\n      \nFine-Tuning LoRA Weights:\n These weights are then fine-tuned, focusing solely on the LoRA weights while keeping the original model weights unchanged.\n\n      \nMerging Weights:\n Post-fine-tuning, the LoRA and original model weights are merged.\n\n      \nQuantization:\n The merged weights undergo quantization to a lower-bit format, crucial for reducing memory and computational costs.\n\n    \n\n  \n\n\n\n\n\nReLoRA\n\n\n\n\n  \n\n    \nWhat:\n ReLoRA is an innovative approach for training high-rank networks efficiently. It revises the Low-Rank Adaptation method by iteratively applying low-rank updates to gradually increase the model’s effective rank.\n\n  \n\n  \n\n    \nWhen to use:\n Best suited for training large-scale models, particularly when the objective is to achieve high-rank training outcomes with less computational expenditure. ReLoRA is especially valuable for large transformer language models where resource efficiency is critical.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nIterative Low-Rank Updates:\n Unlike traditional low-rank methods, ReLoRA applies updates in an iterative manner, each time incrementally enhancing the model’s rank, leading to more efficient high-rank network training.\n\n      \nResource Efficiency:\n Allows for training of large, high-performing models while significantly reducing computational demands.\n\n    \n\n  \n\n  \nDifferentiation from Other Techniques:\n\n    \n\n      \nReLoRA stands out from previous techniques like standard LoRA by its unique iterative process. This method incrementally increases the rank of the model through successive low-rank updates, enabling more dynamic and refined training for large-scale models.\n\n    \n\n  \n\n\n\n\n\nS-LoRA\n\n\n\n\n  \n\n    \nWhat:\n S-LoRA is a scalable system for serving multiple LoRA (Low-Rank Adaptation) adapters concurrently in large language models (LLMs). It manages memory efficiently by storing all adapters in main memory and dynamically fetching them to GPU memory. The system uses customized CUDA kernels for batch processing, optimizing both memory usage and computational efficiency.\n\n  \n\n  \n\n    \nWhen to use:\n S-LoRA is ideal for scenarios where many fine-tuned variants of LLMs need to be served simultaneously with high throughput. It significantly reduces memory fragmentation and I/O overhead, making it suitable for large-scale deployments in resource-constrained environments.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nEfficient Memory Management:\n Utilizes a unified memory pool to manage adapter weights dynamically, reducing memory fragmentation.\n\n      \nHigh Throughput Serving:\n Custom CUDA kernels enable efficient heterogeneous batching of LoRA computations, allowing the serving of thousands of adapters with minimal overhead.\n\n      \nReduced Training and Communication Costs:\n Offers an effective solution in federated learning scenarios by lowering the costs associated with training and data communication.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nStorage of Adapters:\n All adapters are stored in the main memory, ready for dynamic retrieval.\n\n      \nDynamic Fetching:\n Adapters required for current computations are fetched into GPU memory as needed.\n\n      \nBatch Processing:\n Customized CUDA kernels facilitate batch processing, ensuring efficient computation across various sequence lengths and adapter ranks.\n\n    \n\n  \n\n\n\n\n\nDoRA\n\n\n\n\n  \n\n    \nWhat:\n DoRA is an advanced fine-tuning method that decomposes pre-trained model weights into magnitude and directional components. This decomposition facilitates efficient fine-tuning by employing LoRA for directional updates and introducing trainable magnitude components to enhance the learning capacity and stability.\n\n  \n\n  \n\n    \nWhen to use:\n DoRA is particularly effective when there is a need to bridge the performance gap between LoRA-based methods and full fine-tuning without increasing inference costs. It’s suitable for tasks that require high performance, such as commonsense reasoning, visual instruction tuning, and multimodal understanding.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nWeight Decomposition:\n Separates weights into magnitude and direction, allowing for targeted updates that enhance learning capability without additional inference overhead.\n\n      \nEnhanced Learning Capacity:\n Integrates trainable magnitude components with directional updates, providing a balanced approach to fine-tuning that improves both stability and learning capacity.\n\n      \nVersatility Across Tasks:\n Demonstrates superior performance across various tasks and models, proving its adaptability and effectiveness in different settings.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nDecomposition of Weights:\n Begins with the decomposition of pre-trained model weights into their magnitude and directional components.\n\n      \nDirectional Updates Using LoRA:\n Employs LoRA specifically for updating directional components during fine-tuning.\n\n      \nTraining of Magnitude Components:\n Trainable magnitude components are fine-tuned separately, enhancing the overall learning capacity of the model.\n\n      \nPerformance Evaluation:\n The effectiveness of DoRA is validated across multiple tasks, showcasing significant performance improvements compared to other fine-tuning methods.\n\n    \n\n  \n\n\n\n\n\nSPAFIT\n\n\n\n\n  \n\n    \nWhat:\n SPAFIT (Stratified Progressive Adaptation Fine-tuning) is a Parameter-Efficient Fine-Tuning (PEFT) method that targets specific layers of a Transformer-based large language model according to their contribution to linguistic knowledge.\n\n  \n\n  \n\n    \nWhen to use:\n SPAFIT is effective when you want to avoid the pitfalls of catastrophic forgetting and computational inefficiency typical in full model fine-tuning. It’s particularly useful for tasks that require different levels of linguistic processing, allowing for tailored adaptation.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nLayer-Specific Fine-Tuning:\n SPAFIT divides the model into three groups, allowing each group of layers to be fine-tuned to varying extents based on their importance to task performance.\n\n      \nEfficiency and Performance:\n By fine-tuning fewer parameters, SPAFIT achieves competitive or superior results compared to full fine-tuning, particularly on tasks involving sentence similarity.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nLayer Grouping:\n Model layers are categorized into three groups based on their function and linguistic contribution.\n\n      \nAdaptive Fine-Tuning:\n Group 1 layers remain frozen, Group 2 layers are fine-tuned only on bias terms, and Group 3 layers undergo a more comprehensive fine-tuning using BitFit and LoRA for different components.\n\n      \nPerformance Evaluation:\n SPAFIT’s effectiveness is validated across multiple NLP tasks, showing strong results with fewer fine-tuned parameters.\n\n    \n\n  \n\n\n\n\n\nNOLA\n\n\n\n\n  \n\n    \nWhat:\n NOLA is a novel method for compressing large language models that reparameterizes the matrices used in Low-Rank Adaptation (LoRA) through linear combinations of randomly generated basis matrices, drastically reducing the parameter count.\n\n  \n\n  \n\n    \nWhen to use:\n Ideal for situations where extreme model compression is necessary without sacrificing performance, making it suitable for deployment in resource-constrained environments or when model storage costs need to be minimized.\n\n  \n\n  \nKey Features:\n\n    \n\n      \nParameter Compression:\n Achieves up to 20 times fewer parameters than the most compressed LoRA models.\n\n      \nDecoupling Parameter Count:\n Separates the number of trainable parameters from the rank choice and network architecture, allowing for more flexible and efficient model compression.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nMatrix Reparameterization:\n Decomposes weight changes into two matrices, \\(A\\) and \\(B\\), which are then reparameterized using a linear combination of random basis matrices.\n\n      \nLearning Combination Coefficients:\n Focuses on optimizing the mixture coefficients for these basis matrices while keeping the original matrices frozen.\n\n      \nStorage Optimization:\n Stores only the coefficients and the seed of the random number generator used for creating the basis matrices, significantly reducing storage requirements.\n\n      \nEvaluation on Multiple Tasks:\n Demonstrates effectiveness across various tasks and models, maintaining or exceeding benchmark metrics while significantly reducing the parameter count.\n\n    \n\n  \n\n\n\n\n\nMoRA\n\n\n\n\n  \n\n    \nWhat:\n MoRA (Matrix of Rank Adaptation) is an advanced fine-tuning technique designed to enhance the capacity of large language models (LLMs) to learn and retain new knowledge. It replaces the low-rank matrices used in LoRA with a high-rank square matrix, significantly increasing the model’s update capacity without increasing the number of trainable parameters. This method introduces non-parameterized operators to adjust the input and output dimensions, ensuring efficient integration with existing LLMs.\n\n  \n\n  \nWhen to use:\n MoRA is particularly effective for tasks that require substantial knowledge enhancement and memory capacity. It is well-suited for scenarios where:\n    \n\n      \nMemory-Intensive Tasks:\n The task demands significant memorization and the retention of new knowledge, such as continual pretraining and memory tasks.\n\n      \nLimited Resources:\n You need to maximize performance while maintaining low computational and memory overheads.\n\n      \nPerformance Matching or Exceeding LoRA:\n The method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks, making it a versatile choice across various applications.\n\n    \n\n  \n\n  \nKey Features:\n\n    \n\n      \nHigh-Rank Updates:\n Utilizes a square matrix to achieve high-rank updates, significantly increasing the model’s capacity to learn and retain new information.\n\n      \nEfficient Parameter Use:\n Maintains the same number of trainable parameters as LoRA by employing non-parameterized operators for input and output dimension adjustments.\n\n      \nVersatility Across Tasks:\n Demonstrates superior performance in memory-intensive tasks and matches performance in other fine-tuning scenarios, proving its effectiveness across diverse applications.\n\n    \n\n  \n\n  \nProcess:\n\n    \n\n      \nInput Dimension Reduction:\n Non-parameterized operators reduce the input dimension for the high-rank square matrix.\n\n      \nOutput Dimension Increase:\n Corresponding operators increase the output dimension, maintaining parameter efficiency.\n\n      \nIntegration with LLMs:\n The high-rank matrix and operators can be integrated back into the LLM, similar to LoRA, ensuring seamless deployment.\n\n      \nEmpirical Evaluation:\n Comprehensive evaluation across multiple tasks, including instruction tuning, mathematical reasoning, and continual pretraining, demonstrating significant improvements in memory-intensive tasks and comparable performance in others.\n\n    \n\n  \n\n\n\n\n\nComparative Analysis of Popular PEFT Methods\n\n\n\n\n    \n\n        \n\n            \n\n                \nPEFT Methods\n\n                \nDescription\n\n                \nWhen to Use\n\n                \nComputational Overhead\n\n                \nMemory Efficiency\n\n                \nVersatility across Tasks\n\n                \nPerformance Impact\n\n            \n\n        \n\n        \n\n            \n\n                \nPrompt Tuning\n\n                \nModifies LLM's hidden states with trainable parameters in response to task-specific prompts.\n\n                \nLarge pre-trained LLM.\nAdaptation to multiple tasks.\n\n                \nLow\n\n                \nModerate\n\n                \nHigh\n\n                \nDepends on prompt quality\n\n            \n\n            \n\n                \nPrefix Tuning\n\n                \nAdds a trainable prefix to modify LLM's learned representation.\n\n                \nTask-specific adaptation.\nLimited resources.\n\n                \nLow\n\n                \nModerate\n\n                \nModerate\n\n                \nCan vary, but usually positive with proper tuning\n\n            \n\n            \n\n                \nAdapters\n\n                \nInserts neural modules between LLM layers; only adapter weights are updated during fine-tuning.\n\n                \nMultiple tasks on one LLM.\nFlexibility required.\n\n                \nModerate\n\n                \nGood (only adapters are fine-tuned)\n\n                \nHigh (can be added for multiple tasks)\n\n                \nTypically positive if adapters are well-tuned\n\n            \n\n            \n\n                \nLoRA\n\n                \nIntroduces a low-rank matrix into the attention mechanism to learn task-specific patterns.\n\n                \nTasks with specialized attention requirements.\nLimited resources.\n\n                \nLow-Moderate\n\n                \nGood\n\n                \nModerate\n\n                \nGenerally positive with good training\n\n            \n\n            \n\n                \nQLoRA\n\n                \nBuilds on LoRA with quantization for enhanced memory efficiency.\n\n                \nStrict memory constraints.\nEmphasis on performance & efficiency.\n\n                \nLow\n\n                \nExcellent\n\n                \nHigh\n\n                \nComparable or better than full fine-tuning\n\n            \n\n            \n\n                \nQA-LoRA\n\n                \nEnhances LoRA with quantization-aware techniques for fine-tuning low-bit diffusion models.\n\n                \nOptimizing efficiency in low-bit settings.\nResource-constrained environments.\n\n                \nLow\n\n                \nExcellent\n\n                \nModerate\n\n                \nEnhanced efficiency and effectiveness in specific settings\n\n            \n\n            \n\n                \nReLoRA\n\n                \nIteratively applies low-rank updates for efficient training of high-rank networks.\n\n                \nLarge-scale models requiring high-rank training with reduced resources.\n\n                \nModerate\n\n                \nGood\n\n                \nModerate\n\n                \nAchieves high-rank training efficiency and performance\n\n            \n  \n           \n\n                \nS-LoRA\n\n                \nSystem for scalable serving of LoRA adapters in LLMs, using a unified memory management system and custom CUDA kernels for batch processing.\n\n                \nDeploying multiple LLM variants efficiently.\nHigh throughput needs in serving.\n\n                \nModerate\n\n                \nGood (efficient memory management)\n\n                \nHigh (supports thousands of concurrent adapters)\n\n                \nIncreases throughput, reduces costs in federated settings\n\n            \n\n            \n\n                \nDoRA\n\n                \nDecomposes pre-trained weights into magnitude and directional components for fine-tuning, employing LoRA for directional updates to enhance learning capacity and stability.\n\n                \nImproving learning capacity without adding inference overhead.\nHigh performance across diverse tasks.\n\n                \nLow\n\n                \nGood\n\n                \nHigh (adaptable across various models and tasks)\n\n                \nMatches or exceeds full fine-tuning performance\n\n            \n                                      \n           \n\n                \nSPAFIT\n\n                \nStratifies layer fine-tuning by linguistic importance, selectively applying adaptations.\n\n                \nOptimal resource allocation.\nHigh performance with reduced parameter tuning.\n\n                \nLow to moderate\n\n                \nHigh (fine-tunes fewer parameters)\n\n                \nHigh (effective across multiple tasks)\n\n                \nMatches or exceeds full model tuning\n\n            \n\n            \n\n                \nBitFit\n\n                \nUpdates only bias terms of pre-trained BERT models, reducing the fine-tuning overhead.\n\n                \nSmall to medium datasets.\nMinimal performance degradation acceptable.\n\n                \nLow\n\n                \nHigh (minimal parameters are updated)\n\n                \nModerate (depends on the importance of bias terms)\n\n                \nComparable or superior to full fine-tuning\n\n            \n\n            \n\n                \nNOLA\n\n                \nCompresses LoRA using a linear combination of random basis matrices, minimizing parameter counts.\n\n                \nExtreme model compression without losing performance.\nResource-constrained environments.\n\n                \nLow\n\n                \nExcellent (up to 20 times fewer parameters)\n\n                \nHigh (effective across NLP and Vision tasks)\n\n                \nMaintains or exceeds benchmark metrics\n\n            \n     \n            \n\n                \nMoRA\n\n                \nEmploys a high-rank square matrix for updates, enhancing the model's capacity to learn and retain new knowledge while maintaining parameter efficiency.\n\n                \nTasks requiring substantial knowledge enhancement and memory capacity.\nLimited resources.\n\n                \nLow-Moderate\n\n                \nGood\n\n                \nHigh\n\n                \nOutperforms LoRA on memory-intensive tasks and matches performance on others\n\n            \n                       \n        \n\n    \n\n\n\n\n\nPractical Tips for Finetuning LLMs Using LoRA\n\n\n\n\n  \n\n    \nThis section is inspired by the findings of \nSebastian Raschka’s blog\n talking about practical tips for finetuning.\n\n\n    \n\n      \n\n        \nConsistency in LLM Training\n: Despite the inherent randomness in training models on GPUs, the outcomes of LoRA experiments remain consistent across multiple runs, which is promising for comparative studies.\n\n      \n\n      \n\n        \nQLoRA Compute-Memory Trade-offs\n: Quantized LoRA (QLoRA) offers a 33% reduction in GPU memory usage at the cost of a 33% increase in runtime, proving to be a viable alternative to regular LoRA when facing GPU memory constraints.\n\n      \n\n      \n\n        \nLearning Rate Schedulers\n: Using learning rate schedulers like cosine annealing can optimize convergence during training and avoid overshooting the loss minima. While it has a notable impact on SGD optimizer performance, it makes less difference when using Adam or AdamW optimizers.\n\n      \n\n      \n\n        \nChoice of Optimizers\n: The optimizer choice (Adam vs. SGD) doesn’t significantly impact the peak memory demands of LLM training, and swapping Adam for SGD may not provide substantial memory savings, especially with a small LoRA rank (r).\n\n      \n\n      \n\n        \nImpact of Multiple Training Epochs\n: Iterating multiple times over a static dataset in multi-epoch training may not be beneficial and could deteriorate model performance, possibly due to overfitting.\n\n      \n\n      \n\n        \nApplying LoRA Across Layers\n: Enabling LoRA across all layers, not just the Key and Value matrices, can significantly increase model performance, though it also increases the number of trainable parameters and memory requirements.\n\n      \n\n      \n\n        \nLoRA Hyperparameters\n: Adjusting the LoRA rank (r) and selecting an appropriate alpha value are crucial. A heuristic that yielded good results was setting alpha at twice the rank’s value, with r=256 and alpha=512 being the best setting in one particular case.\n\n      \n\n      \n\n        \nFine-tuning Large Models\n: LoRA allows for fine-tuning 7 billion parameter LLMs on a single GPU with 14 GB of RAM within a few hours. However, optimizing an LLM to excel across all benchmark tasks may be unattainable with a static dataset.\n\n      \n\n    \n\n  \n\n  \n\n    \nAdditionally, the article addresses common questions related to LoRA:\n\n\n    \n\n      \n\n        \nImportance of Dataset\n: The dataset used for fine-tuning is critical, and data quality is very important. Experiments showed that a curated dataset with fewer examples (like LIMA) could yield better performance than larger datasets (like Alpaca).\n\n      \n\n      \n\n        \nLoRA for Domain Adaptation\n: LoRA’s effectiveness for domain adaptation requires further investigation. Including task-specific examples in the fine-tuning process is recommended.\n\n      \n\n      \n\n        \nSelecting the Best Rank\n: Choosing the best rank for LoRA is a hyperparameter that needs to be explored for each LLM and dataset. A larger rank could lead to overfitting, while a smaller rank may not capture diverse tasks within a dataset.\n\n      \n\n      \n\n        \nEnabling LoRA for All Layers\n: Exploring the impact of enabling LoRA for different combinations of layers is suggested for future experiments.\n\n      \n\n      \n\n        \nAvoiding Overfitting\n: To prevent overfitting, one could decrease the rank or increase the dataset size, adjust the weight decay rate, or consider increasing the dropout value for LoRA layers.\n\n      \n\n      \n\n        \nOther Optimizers\n: Exploring other optimizers, such as Sophia, which promises faster training and better performance than Adam, is suggested for future research.\n\n      \n\n      \n\n        \nFactors Influencing Memory Usage\n: Model size, batch size, the number of trainable LoRA parameters, and dataset size can influence memory usage. Shorter training sequences can lead to substantial memory savings.\n\n      \n\n    \n\n  \n\n\n\n\n\nRelated: Surgical fine-tuning\n\n\n\n\n  \nWhile not exactly a PEFT method, \nSurgical fine-tuning\n by Lee et al. from Finn’s group at Stanford is a method of selectively updating specific layers in a neural network based on how a fine-tuning dataset differs from the original pretraining dataset, rather than retraining every layer.\n\n  \nMotivation:\n\n    \n\n      \nLayer Specificity:\n Early layers in a neural network capture fundamental features of inputs (e.g., edges or shapes in images), while deeper layers combine these features for predictions (e.g., classifying images).\n\n      \nEfficiency:\n Rather than universally fine-tuning every layer, selectively updating specific layers can achieve better performance, especially when the fine-tuning dataset has notable differences from the pretraining dataset.\n\n    \n\n  \n\n  \nApproaches:\n\n    \n\n      \nManual Approach:\n\n        \n\n          \nFine-tune each layer individually and create a distinct model for each layer.\n\n          \nCompare the performance of each model to identify the best layers for fine-tuning.\n\n        \n\n      \n\n      \nAutomated Approach:\n\n        \n\n          \nCalculate gradients for each layer.\n\n          \nDerive relative gradients by dividing the layer’s gradient by its weight magnitude.\n\n          \nNormalize these relative gradients across layers, ranking them between 0 to 1.\n\n          \nAssign learning rates for layers based on their normalized relative gradient value during training.\n\n        \n\n      \n\n      \nBased on the findings in this paper, here are some tips for determining which layers to fine-tune when adapting a pretrained model to a new target distribution:\n        \n\n          \nConsider the type of distribution shift between the source and target data:\n            \n\n              \nFor input-level shifts like image corruptions, fine-tuning earlier layers (first conv block) tends to work best. This allows the model to adapt to changes in the input while preserving higher-level features.\n\n              \nFor feature-level shifts where the feature representations differ between source and target, fine-tuning middle layers (middle conv blocks) tends to work well. This tunes the mid-level features without distorting low-level or high-level representations.\n\n              \nFor output-level shifts like label distribution changes, fine-tuning later layers (fully connected classifier) tends to be most effective. This keeps the feature hierarchy intact and only adapts the output mapping.\n\n            \n\n          \n\n          \nTry fine-tuning only a single contiguous block of layers while freezing others. Systematically test first, middle, and last blocks to find the best one.\n\n          \nUse criteria like relative gradient norms to automatically identify layers that change the most for the target data. Fine-tuning those with higher relative gradients can work better than full fine-tuning.\n\n          \nWhen in doubt, fine-tuning only the classifier head is a solid default that outperforms no fine-tuning. But for shifts related to inputs or features, surgical fine-tuning of earlier layers can improve over this default.\n\n          \nIf possible, do some quick validation experiments to directly compare different surgical fine-tuning choices on a small held-out set of target data.\n\n          \nThe key insight is that different parts of the network are best suited for adapting to different types of distribution shifts between the source and target data.\n\n        \n\n      \n\n    \n\n  \n\n  \nResults:\n\n    \n\n      \nCIFAR-C Dataset:\n\n        \n\n          \nManual approach yielded an accuracy of 82.8%.\n\n          \nFine-tuning the entire network resulted in 79.9% accuracy.\n\n          \nThe automated approach achieved an accuracy of 81.4%.\n\n        \n\n      \n\n    \n\n  \n\n  \nSignificance:\n Surgical fine-tuning is rooted in understanding how neural networks process input. This enhanced understanding can drive the discovery of more efficient methods to improve machine learning models.\n\n  \nConsideration:\n For more complex datasets, discerning differences between pretraining and fine-tuning datasets can be challenging. This complexity might make automated approaches like the one proposed more valuable, even if it didn’t yield the best performance on CIFAR-C.\n\n\n\n\n\n\n\n\nLoRA vs. QLoRA experimentation by \nSebastian Raschka\n\n\n\n  \nThis section is taken from \nSebastian Raschka’s\n post on LoRA & QLoRA experiments to finetune open-source LLMs, and presents his learnings:\n    \n\n      \nDespite embracing the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.\n\n      \nQLoRA presents a trade-off that might be worthwhile if you’re constrained by GPU memory. It offers 33% memory savings at the cost of a 33% increase in runtime.\n\n      \nWhen finetuning LLMs, the choice of optimizer shouldn’t be a major concern. While SGD on its own is suboptimal, there’s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.\n\n      \nWhile Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn’t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.\n\n      \nFor static datasets, iterating multiple times as done in multi-epoch training might not be beneficial. It often deteriorates the results, probably due to overfitting.\n\n      \nIf you’re incorporating LoRA, ensure it’s applied across all layers, not just to the Key and Value matrices, to maximize model performance.\n\n      \nAdjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank’s value.\n\n      \n7B models can be finetuned efficiently within a few hours on a single GPU possessing 14 Gb of RAM.\n\n    \n\n  \n\n  \nWith a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\n  \nFinetuning LLMs Efficiently with Adapters\n\n  \nSrishti Gureja on LinkedIn\n\n  \nSebastian Raschka on LinkedIn\n\n  \nPrithivi Da on LinkedIn\n\n  \n🤗 PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware\n\n  \nHugging Face: PEFT\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledPEFT,\n  title   = {Parameter Efficient Fine-Tuning (PEFT)},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/parameter-efficient-fine-tuning/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Distributed Training Parallelism\n\n  \n\n\n  \n\n  \n\n  \nBackground\n\n  \nOverview: Types of Parallelism\n    \n\n      \nModel Parallelism\n        \n\n          \nConcept\n\n          \nMechanism\n\n          \nPros and Cons\n\n          \nUse Cases\n\n          \nImplementation in PyTorch\n\n          \nConclusion\n\n          \nSummary\n\n        \n\n      \n\n      \nData Parallelism\n        \n\n          \nConcept\n\n          \nMechanism\n\n          \nPros and Cons\n\n          \nUse Cases\n\n          \nImplementation in PyTorch\n\n          \nConclusion\n\n          \nSummary\n\n        \n\n      \n\n      \nPipeline Parallelism\n\n      \nTensor Parallelism\n\n      \nChoosing the right strategy: Data v/s Model v/s Pipeline v/s Tensor Parallelism\n\n    \n\n  \n\n  \nData Parallelism\n    \n\n      \nDataParallel (DP)\n        \n\n          \nHow DP Works\n\n          \nKey Steps in Using DP\n\n          \nCode Sample\n\n          \nExplanation of the Code\n\n        \n\n      \n\n      \nDistributed Data Parallel (DDP)\n        \n\n          \nKey Features of DDP\n\n          \nTechnical Steps to Use DDP\n\n          \nCode Sample\n\n          \nExplanation of the Code\n\n        \n\n      \n\n    \n\n  \n\n  \nModel Parallelism\n    \n\n      \nLayer-wise Parallelism\n\n      \nTensor-wise Parallelism\n\n      \nOperator-wise Parallelism\n\n      \nSummary\n\n      \nComparative Analysis: Types of Model Parallelism\n        \n\n          \nCriteria for Comparison\n\n          \nLayer-wise Parallelism\n\n          \nTensor-wise Parallelism\n\n          \nOperator-wise Parallelism\n\n        \n\n      \n\n      \nSummary Table\n\n      \nChoosing the Right Type\n\n    \n\n  \n\n  \nHybrid (Model and Data) Parallelism\n    \n\n      \nFully Sharded Data Parallel (FSDP)\n        \n\n          \nKey Features of FSDP\n\n          \nTechnical Details\n\n          \nCode Sample\n\n          \nExplanation of the Code\n\n          \nBenefits of FSDP\n\n        \n\n      \n\n    \n\n  \n\n  \nTensor Parallelism\n    \n\n      \nConcept\n\n      \nMechanism\n\n      \nTypes of Tensor Parallelism\n\n      \nPros and Cons\n\n      \nUse Cases\n\n      \nImplementation in PyTorch\n\n      \nConclusion\n\n      \nSummary\n\n    \n\n  \n\n  \nPipeline Parallelism\n    \n\n      \nConcept\n\n      \nMechanism\n\n      \nTypes of Pipeline Parallelism\n\n      \nPros and Cons\n\n      \nUse Cases\n\n      \nImplementation in PyTorch\n\n      \nConclusion\n\n      \nSummary\n\n      \nDeepSpeed\n        \n\n          \nKey Features of DeepSpeed\n\n          \nTechnical Details\n\n          \nCode Sample\n\n          \nExplanation of the Code\n\n          \nBenefits of DeepSpeed\n\n        \n\n      \n\n      \nDeepSpeed ZeRO\n        \n\n          \nKey Features of DeepSpeed ZeRO\n\n          \nTechnical Details\n\n          \nBenefits\n\n          \nCode Sample\n\n          \nExplanation of the Code\n\n          \nComparison of ZeRO Stages\n\n        \n\n      \n\n    \n\n  \n\n  \nComparative Analysis: DP, DDP, FSDP, DeepSpeed, and DeepSpeed ZeRO\n    \n\n      \nData Parallel (DP)\n        \n\n          \nOverview\n\n          \nPros\n\n          \nCons\n\n          \nCode Sample\n\n        \n\n      \n\n      \nDistributed Data Parallel (DDP)\n        \n\n          \nOverview\n\n          \nPros\n\n          \nCons\n\n          \nCode Sample\n\n        \n\n      \n\n      \nFully Sharded Data Parallel (FSDP)\n        \n\n          \nOverview\n\n          \nPros\n\n          \nCons\n\n          \nCode Sample\n\n        \n\n      \n\n      \nDeepSpeed\n        \n\n          \nOverview\n\n          \nPros\n\n          \nCons\n\n          \nCode Sample\n\n        \n\n      \n\n      \nDeepSpeed ZeRO\n        \n\n          \nOverview\n\n          \nPros\n\n          \nCons\n\n          \nCode Sample\n\n        \n\n      \n\n      \nComparative Summary\n\n      \nUse Cases\n\n    \n\n  \n\n  \nCitation\n\n\n\n\n\nBackground\n\n\n\n\n  \nDistributed training parallelism is crucial for efficiently training large-scale deep learning models that require extensive computational resources. This approach leverages multiple GPUs or machines to perform computations in parallel, significantly reducing training time and enabling the handling of larger datasets and models.\n\n  \nThere are four main strategies for parallelism in distributed training: model, data, pipeline, and tensor parallelism. Each has its own mechanisms, advantages, and challenges, and understanding them is essential for optimizing training performance in different scenarios.\n\n\n\n\n\nOverview: Types of Parallelism\n\n\n\nModel Parallelism\n\n\n\n\n  \nModel parallelism is a strategy for distributing the computation of a deep learning model across multiple GPUs or other computing devices. This approach is particularly beneficial when the model is too large to fit into the memory of a single GPU. Instead of duplicating the entire model on each GPU (as in data parallelism), different parts of the model are placed on different GPUs, allowing for the training of very large models.\n\n\n\n\n\nConcept\n\n\n\n\n  \nIn model parallelism, the model is partitioned such that each GPU is responsible for a subset of the model parameters. During the forward pass, data flows through these partitions sequentially, with intermediate results transferred between GPUs as needed. The backward pass similarly computes gradients for each partition, which are then used to update the parameters locally on each GPU.\n\n\n\n\n\nMechanism\n\n\n\n\n  \nModel Partitioning\n:\n    \n\n      \nThe model is divided into distinct segments, each assigned to a different GPU. For example, in a neural network, different layers or groups of layers might be placed on different GPUs.\n\n    \n\n  \n\n  \nForward Pass\n:\n    \n\n      \nInput data is fed into the first segment of the model on the first GPU. The output of this segment is transferred to the next GPU, which processes the next segment of the model, and so on until the final segment.\n\n    \n\n  \n\n  \nBackward Pass\n:\n    \n\n      \nDuring backpropagation, gradients are computed in the reverse order. Each GPU computes the gradients for its segment and passes the necessary information back to the previous GPU to continue the gradient computation.\n\n    \n\n  \n\n  \nParameter Update\n:\n    \n\n      \nEach GPU updates the parameters of its segment locally. If needed, synchronization steps can ensure consistency across GPUs.\n\n    \n\n  \n\n\n\n\n\nPros and Cons\n\n\n\n\n  \nPros\n:\n    \n\n      \nMemory Efficiency\n: Allows for training models that are too large to fit into the memory of a single GPU by distributing the model’s parameters and intermediate activations across multiple GPUs.\n\n      \nScalability\n: Enables the use of large-scale models in research and production, leveraging multiple GPUs to handle complex computations.\n\n      \nResource Utilization\n: Can optimize the utilization of available computational resources by balancing the load across multiple GPUs.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nComplexity\n: Implementing model parallelism can be challenging due to the need to carefully manage data transfers and dependencies between different parts of the model.\n\n      \nCommunication Overhead\n: Transferring intermediate results between GPUs introduces latency, which can become a bottleneck, especially if the network bandwidth is limited.\n\n      \nSynchronization Issues\n: Ensuring that all parts of the model remain synchronized during training can be difficult, particularly in distributed environments.\n\n    \n\n  \n\n\n\n\n\nUse Cases\n\n\n\n\n  \nModel parallelism is especially useful in scenarios where the model size exceeds the memory capacity of a single GPU. Some common use cases include:\n    \n\n      \nLarge Language Models\n: Models like GPT-3, which have billions of parameters, often require model parallelism to be trained effectively.\n\n      \nDeep Convolutional Networks\n: Very deep networks with many layers can benefit from model parallelism by distributing layers across multiple GPUs.\n\n      \n3D Convolutional Networks\n: Used in applications like video processing and 3D object recognition, where the model size can be substantial.\n\n    \n\n  \n\n\n\n\n\nImplementation in PyTorch\n\n\n\n\n  \nIn PyTorch, model parallelism can be implemented by manually assigning different parts of the model to different GPUs. Here’s a basic example:\n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\n\n# Define a simple model with two linear layers\n\nclass\n \nSimpleModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nSimpleModel\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nlayer1\n \n=\n \nnn\n.\nLinear\n(\n10\n,\n \n10\n).\nto\n(\n'cuda:0'\n)\n  \n# Place on GPU 0\n\n        \nself\n.\nlayer2\n \n=\n \nnn\n.\nLinear\n(\n10\n,\n \n10\n).\nto\n(\n'cuda:1'\n)\n  \n# Place on GPU 1\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nself\n.\nlayer1\n(\nx\n)\n\n        \nx\n \n=\n \nx\n.\nto\n(\n'cuda:1'\n)\n  \n# Move intermediate output to GPU 1\n\n        \nx\n \n=\n \nself\n.\nlayer2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\n# Initialize model, loss, and optimizer\n\nmodel\n \n=\n \nSimpleModel\n()\n\n\ncriterion\n \n=\n \nnn\n.\nMSELoss\n()\n\n\noptimizer\n \n=\n \noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.01\n)\n\n\n\n# Dummy input and target\n\ninput_data\n \n=\n \ntorch\n.\nrandn\n(\n5\n,\n \n10\n).\nto\n(\n'cuda:0'\n)\n\n\ntarget\n \n=\n \ntorch\n.\nrandn\n(\n5\n,\n \n10\n).\nto\n(\n'cuda:1'\n)\n\n\n\n# Forward pass\n\noutput\n \n=\n \nmodel\n(\ninput_data\n)\n\n\nloss\n \n=\n \ncriterion\n(\noutput\n,\n \ntarget\n)\n\n\n\n# Backward pass\n\nloss\n.\nbackward\n()\n\n\n\n# Optimizer step\n\noptimizer\n.\nstep\n()\n\n\n\n\n\n\n  \nIn this example, the model has two linear layers, each placed on a different GPU. During the forward pass, the input is moved between GPUs as needed. The backward pass and optimizer step handle the parameter updates for each layer separately.\n\n\n\n\n\nConclusion\n\n\n\n\n  \nModel parallelism is a powerful technique for training large models that cannot fit into the memory of a single GPU. While it introduces complexity in terms of implementation and communication overhead, it enables the training of state-of-the-art models in various fields, from natural language processing to computer vision. Properly balancing the model segments and managing the data flow are key challenges, but with frameworks like PyTorch providing the necessary tools, model parallelism can be effectively leveraged to push the boundaries of deep learning research and applications.\n\n\n\n\n\nSummary\n\n\n\n\n  \nConcept\n:\n    \n\n      \nModel parallelism involves splitting the model itself across multiple GPUs. Different layers or parts of the model are assigned to different GPUs. This is particularly useful when a model is too large to fit into the memory of a single GPU.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nData flows sequentially through the model parts across different GPUs.\n\n      \nEach GPU computes its assigned part of the forward and backward passes.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nEnables training of very large models that do not fit into a single GPU’s memory.\n\n      \nCan optimize memory usage and computational load distribution.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nMore complex to implement due to the need to manage dependencies and data transfers between GPUs.\n\n      \nMay introduce latency due to data transfer between GPUs.\n\n    \n\n  \n\n\n\n\n\nData Parallelism\n\n\n\n\n  \nData parallelism is a technique in parallel computing where data is distributed across multiple processors or machines, and each processor performs the same operation on different parts of the data simultaneously. This approach is particularly effective in scenarios where the same set of operations need to be applied to large datasets.\n\n\n\n\n\nConcept\n\n\n\n\n  \nThe fundamental concept of data parallelism is to break down a large dataset into smaller chunks and process these chunks in parallel. Each processor (or core) receives a portion of the data and applies the same computational operations. After processing, the results are aggregated to form the final output.\n\n\n\n\n\nMechanism\n\n\n\n\n  \n\n    \nData Splitting\n: The dataset is divided into smaller, equally-sized chunks. Each chunk is assigned to a different processor or computing unit.\n\n  \n\n  \n\n    \nParallel Execution\n: Each processor performs the same operation on its assigned chunk of data. This step happens concurrently, leveraging the parallel nature of the computing environment.\n\n  \n\n  \n\n    \nSynchronization\n: After processing, the results from each processor are synchronized. This step may involve combining or aggregating the processed data to produce the final result.\n\n  \n\n  \n\n    \nResult Aggregation\n: The partial results from each processor are combined to form the complete output. This step may require communication between processors to gather and combine the data.\n\n  \n\n\n\n\n\nPros and Cons\n\n\n\nPros:\n\n\n\n  \nIncreased Efficiency\n: By processing data in parallel, the overall computational time is reduced significantly.\n\n  \nScalability\n: Data parallelism can scale with the number of processors, making it suitable for large-scale data processing tasks.\n\n  \nResource Utilization\n: Efficiently utilizes available computing resources by distributing the workload.\n\n\n\n\n\nCons:\n\n\n\n  \nCommunication Overhead\n: Synchronizing and aggregating results can introduce overhead, especially in distributed systems.\n\n  \nComplexity\n: Implementing data parallelism can be complex, requiring careful management of data distribution and synchronization.\n\n  \nLimited by Data Size\n: The size of the dataset must be sufficiently large to justify the overhead of parallelization.\n\n\n\n\n\nUse Cases\n\n\n\n\n  \nDeep Learning\n: Training large neural networks where massive datasets are divided and processed in parallel across multiple GPUs.\n\n  \nBig Data Analytics\n: Processing large volumes of data in parallel to extract insights and perform analytics.\n\n  \nScientific Simulations\n: Running simulations that require processing large datasets, such as weather forecasting or molecular dynamics.\n\n\n\n\n\nImplementation in PyTorch\n\n\n\n\n  \nPyTorch provides several utilities to implement data parallelism easily:\n\n\n\n\n\n\n  \n\n    \ntorch.nn.DataParallel\n: A simple way to wrap a model to enable parallel processing across multiple GPUs.\n\n\n    \nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\n\n# Define the model\n\nmodel\n \n=\n \nnn\n.\nDataParallel\n(\nMyModel\n())\n\n\n\n# Move the model to GPU\n\nmodel\n.\nto\n(\n'cuda'\n)\n\n\n\n# Define the optimizer and loss function\n\noptimizer\n \n=\n \noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.01\n)\n\n\ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n\n\n# Training loop\n\nfor\n \ndata\n,\n \ntarget\n \nin\n \ndataloader\n:\n\n    \ndata\n,\n \ntarget\n \n=\n \ndata\n.\nto\n(\n'cuda'\n),\n \ntarget\n.\nto\n(\n'cuda'\n)\n\n    \noptimizer\n.\nzero_grad\n()\n\n    \noutput\n \n=\n \nmodel\n(\ndata\n)\n\n    \nloss\n \n=\n \ncriterion\n(\noutput\n,\n \ntarget\n)\n\n    \nloss\n.\nbackward\n()\n\n    \noptimizer\n.\nstep\n()\n\n\n    \n\n  \n\n  \n\n    \nDistributed Data Parallel (DDP)\n: For more advanced and scalable parallelism, PyTorch offers \ntorch.nn.parallel.DistributedDataParallel\n. This approach is more efficient and scalable for large-scale applications.\n\n\n    \nimport\n \ntorch\n\n\nimport\n \ntorch.distributed\n \nas\n \ndist\n\n\nimport\n \ntorch.multiprocessing\n \nas\n \nmp\n\n\nfrom\n \ntorch.nn.parallel\n \nimport\n \nDistributedDataParallel\n \nas\n \nDDP\n\n\n\ndef\n \ntrain\n(\nrank\n,\n \nworld_size\n):\n\n    \n# Initialize the process group\n\n    \ndist\n.\ninit_process_group\n(\n\"gloo\"\n,\n \nrank\n=\nrank\n,\n \nworld_size\n=\nworld_size\n)\n\n\n    \n# Create model and move it to GPU with rank\n\n    \nmodel\n \n=\n \nMyModel\n().\nto\n(\nrank\n)\n\n    \nddp_model\n \n=\n \nDDP\n(\nmodel\n,\n \ndevice_ids\n=\n[\nrank\n])\n\n\n    \n# Define loss function and optimizer\n\n    \nloss_fn\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n    \noptimizer\n \n=\n \noptim\n.\nSGD\n(\nddp_model\n.\nparameters\n(),\n \nlr\n=\n0.01\n)\n\n\n    \n# Training loop\n\n    \nfor\n \ndata\n,\n \ntarget\n \nin\n \ndataloader\n:\n\n        \ndata\n,\n \ntarget\n \n=\n \ndata\n.\nto\n(\nrank\n),\n \ntarget\n.\nto\n(\nrank\n)\n\n        \noptimizer\n.\nzero_grad\n()\n\n        \noutputs\n \n=\n \nddp_model\n(\ndata\n)\n\n        \nloss\n \n=\n \nloss_fn\n(\noutputs\n,\n \ntarget\n)\n\n        \nloss\n.\nbackward\n()\n\n        \noptimizer\n.\nstep\n()\n\n\n    \n# Clean up\n\n    \ndist\n.\ndestroy_process_group\n()\n\n\n\ndef\n \nmain\n():\n\n    \nworld_size\n \n=\n \n4\n\n    \nmp\n.\nspawn\n(\ntrain\n,\n \nargs\n=\n(\nworld_size\n,),\n \nnprocs\n=\nworld_size\n,\n \njoin\n=\nTrue\n)\n\n\n\nif\n \n__name__\n \n==\n \n\"__main__\"\n:\n\n    \nmain\n()\n\n\n    \n\n  \n\n\n\n\n\nConclusion\n\n\n\n\n  \nData parallelism is a powerful technique to enhance computational efficiency by distributing data across multiple processors or machines. It is highly beneficial in scenarios involving large datasets and complex computations, such as deep learning and big data analytics. While data parallelism offers significant advantages in terms of speed and scalability, it also introduces challenges like communication overhead and implementation complexity. PyTorch provides robust tools for implementing data parallelism, making it accessible for both simple and advanced use cases.\n\n\n\n\n\nSummary\n\n\n\n\n  \nConcept\n:\n    \n\n      \nData parallelism involves splitting the dataset into smaller batches and distributing these batches across multiple GPUs. Each GPU holds a complete copy of the model and processes a subset of the data independently. After computation, the results (such as gradients) are synchronized and aggregated to update the model parameters.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nEach GPU computes the forward and backward passes for its portion of the data.\n\n      \nGradients are averaged across all GPUs.\n\n      \nModel parameters are updated synchronously to ensure all GPUs have identical parameters.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nEasy to implement and highly effective for many types of models.\n\n      \nScales well with the number of GPUs.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nLimited by the memory of a single GPU, as each GPU must hold the entire model.\n\n      \nCommunication overhead during synchronization can become a bottleneck.\n\n    \n\n  \n\n\n\n\n\nPipeline Parallelism\n\n\n\n\n  \nConcept\n:\n    \n\n      \nPipeline parallelism is a specific form of model parallelism where different stages of the model are distributed across multiple GPUs. The model is divided into stages, and data is processed in a pipeline fashion, allowing different GPUs to work on different mini-batches of data simultaneously.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nThe model is split into several stages, each assigned to a different GPU.\n\n      \nMini-batches are divided into micro-batches and fed sequentially through the stages.\n\n    \n\n  \n\n  \n\n    \nWhile one GPU is processing a micro-batch, another GPU can start processing the next micro-batch.\n\n  \n\n  \nPros\n:\n    \n\n      \nReduces idle time by overlapping computation and communication.\n\n      \nEffective for deep models with many sequential layers.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nIntroduces pipeline bubbles (idle time) at the start and end of the pipeline.\n\n      \nRequires careful tuning to balance the stages and manage the pipeline efficiently.\n\n    \n\n  \n\n\n\n\n\nTensor Parallelism\n\n\n\n\n  \nConcept\n:\n    \n\n      \nTensor parallelism involves splitting individual tensors (such as weight matrices) across multiple GPUs. This approach allows large tensors to be distributed and processed in parallel, effectively balancing the computation load.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nTensors are divided along one or more dimensions, and each GPU holds a slice of the tensor.\n\n      \nDuring computation, operations are performed on these slices in parallel, and the results are combined as needed.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nCan handle extremely large tensors that exceed the memory capacity of a single GPU.\n\n      \nDistributes computational load more evenly across GPUs.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nRequires sophisticated implementation to manage tensor slicing and the corresponding operations.\n\n      \nCommunication overhead for combining the results can impact performance.\n\n    \n\n  \n\n\n\n\n\nChoosing the right strategy: Data v/s Model v/s Pipeline v/s Tensor Parallelism\n\n\n\n\n  \n\n    \nEach parallelism paradigm offers unique advantages and is suitable for different scenarios:\n\n\n    \n\n      \nData Parallelism\n is simple and effective for many models but limited by GPU memory constraints.\n\n      \nModel Parallelism\n allows for training very large models but requires careful management of data flow between GPUs.\n\n      \nPipeline Parallelism\n optimizes utilization by overlapping computations but introduces complexity in balancing the pipeline.\n\n      \nTensor Parallelism\n handles extremely large tensors efficiently but involves intricate implementation and communication overhead.\n\n    \n\n  \n\n  \nChoosing the right parallelism strategy depends on the specific requirements of the model, the hardware architecture, and the desired performance characteristics.\n\n  \nThe infographic below (credits to \nSebastian Raschka\n) offers a quick overview of the four different paradigms for multi-GPU training, such as data parallelism, model parallelism, pipeline parallelism, and tensor parallelism.\n\n\n\n\n\n\n\n\nData Parallelism\n\n\n\n\n  \nData parallelism involves splitting the data across multiple devices (such as GPUs). Each device processes a different chunk of the data, but they all use the same model to perform computations. After processing, the results are combined to update the model. This is accomplished by synchronizing the gradients during the backward pass to ensure consistent updates.\n\n\n\n\n\nDataParallel (DP)\n\n\n\n\n  \nData parallelism in PyTorch refers to the process of distributing data across multiple GPUs to perform parallel computations. This approach helps to accelerate training by utilizing the computational power of multiple GPUs simultaneously. The primary mechanism for achieving data parallelism in PyTorch is through the \ntorch.nn.DataParallel\n module.\n\n  \nHere’s a detailed explanation of how \ntorch.nn.DataParallel\n works and a small code sample to demonstrate its usage.\n\n\n\n\n\nHow DP Works\n\n\n\n\n  \nModel Replication:\n The model is replicated across multiple GPUs, with each replica handling a portion of the input data.\n\n  \nData Splitting:\n The input data is split into smaller batches, and each smaller batch is sent to a different GPU.\n\n  \nParallel Computation:\n Each GPU processes its portion of the data in parallel, performing forward and backward passes independently.\n\n  \nGradient Aggregation:\n Gradients from all GPUs are gathered and averaged.\n\n  \nModel Update:\n The averaged gradients are used to update the model parameters.\n\n\n\n\n\nKey Steps in Using DP\n\n\n\n\n  \nModel Definition:\n Define the model as usual.\n\n  \nWrap with DataParallel:\n Use \ntorch.nn.DataParallel\n to wrap the model.\n\n  \nData Loading:\n Use \ntorch.utils.data.DataLoader\n to load the dataset.\n\n  \nForward and Backward Pass:\n Perform the forward and backward pass in the same way as with a single GPU, but now the computations will be distributed across multiple GPUs.\n\n\n\n\n\nCode Sample\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\nfrom\n \ntorchvision\n \nimport\n \ndatasets\n,\n \ntransforms\n\n\n\n# Define a simple CNN model\n\nclass\n \nSimpleCNN\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nSimpleCNN\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nconv1\n \n=\n \nnn\n.\nConv2d\n(\n1\n,\n \n32\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nconv2\n \n=\n \nnn\n.\nConv2d\n(\n32\n,\n \n64\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nfc1\n \n=\n \nnn\n.\nLinear\n(\n64\n \n*\n \n28\n \n*\n \n28\n,\n \n128\n)\n\n        \nself\n.\nfc2\n \n=\n \nnn\n.\nLinear\n(\n128\n,\n \n10\n)\n\n    \n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv1\n(\nx\n))\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv2\n(\nx\n))\n\n        \nx\n \n=\n \nx\n.\nview\n(\n-\n1\n,\n \n64\n \n*\n \n28\n \n*\n \n28\n)\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nfc1\n(\nx\n))\n\n        \nx\n \n=\n \nself\n.\nfc2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\n# Check if multiple GPUs are available\n\ndevice\n \n=\n \ntorch\n.\ndevice\n(\n\"cuda\"\n \nif\n \ntorch\n.\ncuda\n.\nis_available\n()\n \nelse\n \n\"cpu\"\n)\n\n\nprint\n(\nf\n\"Using device: \n{\ndevice\n}\n\"\n)\n\n\n\n# Instantiate the model\n\nmodel\n \n=\n \nSimpleCNN\n()\n\n\n\n# Wrap the model with DataParallel\n\nif\n \ntorch\n.\ncuda\n.\ndevice_count\n()\n \n>\n \n1\n:\n\n    \nprint\n(\nf\n\"Using \n{\ntorch\n.\ncuda\n.\ndevice_count\n()\n}\n GPUs\"\n)\n\n    \nmodel\n \n=\n \nnn\n.\nDataParallel\n(\nmodel\n)\n\n\n\n# Move the model to the appropriate device\n\nmodel\n.\nto\n(\ndevice\n)\n\n\n\n# Define a loss function and optimizer\n\ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n\noptimizer\n \n=\n \noptim\n.\nAdam\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.001\n)\n\n\n\n# Data loading and preprocessing\n\ntransform\n \n=\n \ntransforms\n.\nCompose\n([\n\n    \ntransforms\n.\nToTensor\n(),\n\n    \ntransforms\n.\nNormalize\n((\n0.1307\n,),\n \n(\n0.3081\n,))\n\n\n])\n\n\n\ntrain_dataset\n \n=\n \ndatasets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \ntrain\n=\nTrue\n,\n \ndownload\n=\nTrue\n,\n \ntransform\n=\ntransform\n)\n\n\ntrain_loader\n \n=\n \ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \nbatch_size\n=\n64\n,\n \nshuffle\n=\nTrue\n)\n\n\n\n# Training loop\n\nfor\n \nepoch\n \nin\n \nrange\n(\n5\n):\n\n    \nmodel\n.\ntrain\n()\n\n    \nrunning_loss\n \n=\n \n0.0\n\n    \nfor\n \ni\n,\n \n(\ninputs\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n        \ninputs\n,\n \nlabels\n \n=\n \ninputs\n.\nto\n(\ndevice\n),\n \nlabels\n.\nto\n(\ndevice\n)\n\n        \n        \noptimizer\n.\nzero_grad\n()\n\n        \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n        \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n        \nloss\n.\nbackward\n()\n\n        \noptimizer\n.\nstep\n()\n\n        \n        \nrunning_loss\n \n+=\n \nloss\n.\nitem\n()\n\n        \nif\n \ni\n \n%\n \n100\n \n==\n \n99\n:\n    \n# Print every 100 mini-batches\n\n            \nprint\n(\nf\n'Epoch [\n{\nepoch\n \n+\n \n1\n}\n, \n{\ni\n \n+\n \n1\n}\n] loss: \n{\nrunning_loss\n \n/\n \n100\n:\n.\n3\nf\n}\n'\n)\n\n            \nrunning_loss\n \n=\n \n0.0\n\n\n\nprint\n(\n'Training finished.'\n)\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n  \nModel Definition:\n A simple CNN model is defined.\n\n  \nDevice Check:\n The code checks if CUDA is available and sets the appropriate device.\n\n  \nModel Wrapping:\n The model is wrapped with \ntorch.nn.DataParallel\n if multiple GPUs are available.\n\n  \nData Loading:\n The MNIST dataset is loaded and transformed.\n\n  \nTraining Loop:\n The model is trained in the usual way, but the computations are distributed across multiple GPUs if available.\n\n\n\n\n\nDistributed Data Parallel (DDP)\n\n\n\n\n  \nBy using \ntorch.nn.DataParallel\n, you can easily scale your model training to utilize multiple GPUs, which can significantly speed up the training process.\n\n  \nDistributed Data Parallel (DDP) in PyTorch is a more advanced and efficient method for parallelizing training across multiple GPUs, potentially across multiple nodes. Unlike \ntorch.nn.DataParallel\n, which uses a single process to manage all devices, DDP creates a separate process for each GPU, allowing for better scalability and reduced inter-GPU communication overhead. This approach is particularly beneficial for large-scale deep learning tasks.\n\n\n\n\n\nKey Features of DDP\n\n\n\n\n  \nSeparate Processes:\n Each GPU is managed by a separate process, which helps in minimizing the bottleneck caused by the Global Interpreter Lock (GIL) in Python.\n\n  \nGradient Synchronization:\n Gradients are synchronized across processes using an all-reduce operation, ensuring that all processes have consistent model parameters.\n\n  \nScalability:\n Better performance and scalability compared to \ntorch.nn.DataParallel\n, especially for large-scale training.\n\n\n\n\n\nTechnical Steps to Use DDP\n\n\n\n\n  \nInitialize the Process Group:\n Set up the communication backend and initialize the process group.\n\n  \nCreate DDP Model:\n Wrap the model with \ntorch.nn.parallel.DistributedDataParallel\n.\n\n  \nSet Up Distributed Sampler:\n Use a distributed sampler to ensure each process gets a unique subset of the dataset.\n\n  \nConfigure Data Loaders:\n Set up data loaders with the distributed sampler.\n\n  \nLaunch Training Processes:\n Use a launcher utility to spawn multiple processes for training.\n\n\n\n\n\nCode Sample\n\n\n\n\n  \nHere’s a basic example demonstrating how to set up and use DDP in PyTorch.\n\n\n\n\n\nimport\n \nos\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.distributed\n \nas\n \ndist\n\n\nimport\n \ntorch.multiprocessing\n \nas\n \nmp\n\n\nfrom\n \ntorch.nn.parallel\n \nimport\n \nDistributedDataParallel\n \nas\n \nDDP\n\n\nfrom\n \ntorchvision\n \nimport\n \ndatasets\n,\n \ntransforms\n\n\nfrom\n \ntorch.utils.data\n \nimport\n \nDataLoader\n,\n \nDistributedSampler\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\n\ndef\n \nsetup\n(\nrank\n,\n \nworld_size\n):\n\n    \nos\n.\nenviron\n[\n'MASTER_ADDR'\n]\n \n=\n \n'localhost'\n\n    \nos\n.\nenviron\n[\n'MASTER_PORT'\n]\n \n=\n \n'12355'\n\n    \ndist\n.\ninit_process_group\n(\n\"nccl\"\n,\n \nrank\n=\nrank\n,\n \nworld_size\n=\nworld_size\n)\n\n\n\ndef\n \ncleanup\n():\n\n    \ndist\n.\ndestroy_process_group\n()\n\n\n\nclass\n \nSimpleCNN\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nSimpleCNN\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nconv1\n \n=\n \nnn\n.\nConv2d\n(\n1\n,\n \n32\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nconv2\n \n=\n \nnn\n.\nConv2d\n(\n32\n,\n \n64\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nfc1\n \n=\n \nnn\n.\nLinear\n(\n64\n \n*\n \n28\n \n*\n \n28\n,\n \n128\n)\n\n        \nself\n.\nfc2\n \n=\n \nnn\n.\nLinear\n(\n128\n,\n \n10\n)\n\n    \n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv1\n(\nx\n))\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv2\n(\nx\n))\n\n        \nx\n \n=\n \nx\n.\nview\n(\n-\n1\n,\n \n64\n \n*\n \n28\n \n*\n \n28\n)\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nfc1\n(\nx\n))\n\n        \nx\n \n=\n \nself\n.\nfc2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\ndef\n \ntrain\n(\nrank\n,\n \nworld_size\n,\n \nepochs\n):\n\n    \nsetup\n(\nrank\n,\n \nworld_size\n)\n\n    \n    \ntransform\n \n=\n \ntransforms\n.\nCompose\n([\n\n        \ntransforms\n.\nToTensor\n(),\n\n        \ntransforms\n.\nNormalize\n((\n0.1307\n,),\n \n(\n0.3081\n,))\n\n    \n])\n\n    \n    \ntrain_dataset\n \n=\n \ndatasets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \ntrain\n=\nTrue\n,\n \ndownload\n=\nTrue\n,\n \ntransform\n=\ntransform\n)\n\n    \ntrain_sampler\n \n=\n \nDistributedSampler\n(\ntrain_dataset\n,\n \nnum_replicas\n=\nworld_size\n,\n \nrank\n=\nrank\n)\n\n    \ntrain_loader\n \n=\n \nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \nbatch_size\n=\n64\n,\n \nsampler\n=\ntrain_sampler\n)\n\n    \n    \nmodel\n \n=\n \nSimpleCNN\n().\nto\n(\nrank\n)\n\n    \nmodel\n \n=\n \nDDP\n(\nmodel\n,\n \ndevice_ids\n=\n[\nrank\n])\n\n    \n    \ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n().\nto\n(\nrank\n)\n\n    \noptimizer\n \n=\n \noptim\n.\nAdam\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.001\n)\n\n    \n    \nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n        \nmodel\n.\ntrain\n()\n\n        \nrunning_loss\n \n=\n \n0.0\n\n        \nfor\n \ni\n,\n \n(\ninputs\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n            \ninputs\n,\n \nlabels\n \n=\n \ninputs\n.\nto\n(\nrank\n),\n \nlabels\n.\nto\n(\nrank\n)\n\n            \n            \noptimizer\n.\nzero_grad\n()\n\n            \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n            \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n            \nloss\n.\nbackward\n()\n\n            \noptimizer\n.\nstep\n()\n\n            \n            \nrunning_loss\n \n+=\n \nloss\n.\nitem\n()\n\n            \nif\n \ni\n \n%\n \n100\n \n==\n \n99\n:\n\n                \nprint\n(\nf\n'Rank \n{\nrank\n}\n, Epoch [\n{\nepoch\n \n+\n \n1\n}\n, \n{\ni\n \n+\n \n1\n}\n] loss: \n{\nrunning_loss\n \n/\n \n100\n:\n.\n3\nf\n}\n'\n)\n\n                \nrunning_loss\n \n=\n \n0.0\n\n    \n    \ncleanup\n()\n\n\n\ndef\n \nmain\n():\n\n    \nworld_size\n \n=\n \ntorch\n.\ncuda\n.\ndevice_count\n()\n\n    \nepochs\n \n=\n \n5\n\n    \nmp\n.\nspawn\n(\ntrain\n,\n \nargs\n=\n(\nworld_size\n,\n \nepochs\n),\n \nnprocs\n=\nworld_size\n,\n \njoin\n=\nTrue\n)\n\n\n\nif\n \n__name__\n \n==\n \n\"__main__\"\n:\n\n    \nmain\n()\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n  \nSetup and Cleanup:\n Functions to initialize and destroy the process group.\n\n  \nModel Definition:\n A simple CNN model is defined.\n\n  \nTrain Function:\n\n    \n\n      \nInitializes the process group.\n\n      \nCreates a distributed sampler and data loader.\n\n      \nWraps the model with \nDistributedDataParallel\n.\n\n      \nRuns the training loop.\n\n    \n\n  \n\n  \nMain Function:\n Uses \nmp.spawn\n to launch multiple processes for training.\n\n\n\n\n\n\n  \nThis setup ensures that each GPU gets its own process and subset of the data, with gradients synchronized across processes, allowing for efficient parallel training.\n\n\n\n\n\nModel Parallelism\n\n\n\n\n  \nModel parallelism involves splitting the neural network model across multiple devices (such as GPUs) so that each device is responsible for a part of the model. This approach is particularly useful when the model is too large to fit into the memory of a single device. By distributing different parts of the model across multiple GPUs, model parallelism enables the training of very large models.\n\n  \nTypes of Model Parallelism\n:\n    \n\n      \nLayer-wise Parallelism\n\n      \nTensor-wise Parallelism\n\n      \nOperator-wise Parallelism\n\n    \n\n  \n\n\n\n\n\nLayer-wise Parallelism\n\n\n\n\n  \nConcept\n:\n    \n\n      \nDifferent layers or groups of layers of the model are assigned to different GPUs. For example, in a deep neural network, you might place the first few layers on one GPU, the next few layers on another GPU, and so on.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nDuring the forward pass, the output of one GPU is transferred to the next GPU for further processing.\n\n      \nDuring the backward pass, gradients are computed in the reverse order, with each GPU handling the gradients for its assigned layers.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nSimple to implement for sequential models.\n\n      \nGood for models with distinct layers or blocks.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nCan lead to inefficient GPU utilization if layers have varying computational loads.\n\n      \nCommunication overhead can be significant if layers produce large outputs.\n\n    \n\n  \n\n  \nExample\n:\n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\n\nclass\n \nLayerwiseModel\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nLayerwiseModel\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nlayer1\n \n=\n \nnn\n.\nLinear\n(\n10\n,\n \n50\n).\nto\n(\n'cuda:0'\n)\n\n        \nself\n.\nlayer2\n \n=\n \nnn\n.\nLinear\n(\n50\n,\n \n10\n).\nto\n(\n'cuda:1'\n)\n\n\n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nself\n.\nlayer1\n(\nx\n)\n\n        \nx\n \n=\n \nx\n.\nto\n(\n'cuda:1'\n)\n\n        \nx\n \n=\n \nself\n.\nlayer2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\n\n\nTensor-wise Parallelism\n\n\n\n\n  \nConcept\n:\n    \n\n      \nIndividual tensors, such as weight matrices or activations, are split across multiple GPUs. This approach involves slicing tensors along one or more dimensions and distributing these slices across different GPUs.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nDuring computation, each GPU processes its slice of the tensor in parallel.\n\n      \nIntermediate results are combined as needed to complete the computation.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nEfficiently utilizes GPU memory by distributing large tensors.\n\n      \nBalances computational load more evenly across GPUs.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nRequires sophisticated implementation to handle tensor slicing and combining.\n\n      \nHigh communication overhead for combining intermediate results.\n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nTensor-wise parallelism is often implemented at a lower level within deep learning frameworks and may not be directly visible in user-defined model code. It’s commonly used in distributed training libraries like Megatron-LM for large-scale models.\n\n    \n\n  \n\n\n\n\n\nOperator-wise Parallelism\n\n\n\n\n  \nConcept\n:\n    \n\n      \nDifferent operations (or parts of operations) within a layer are assigned to different GPUs. This fine-grained parallelism involves splitting the computation of individual operations across multiple devices.\n\n    \n\n  \n\n  \nMechanism\n:\n    \n\n      \nOperations within a layer are divided, with each GPU performing a portion of the computation.\n\n      \nResults from each GPU are combined to produce the final output of the operation.\n\n    \n\n  \n\n  \nPros\n:\n    \n\n      \nCan achieve high parallel efficiency by leveraging all available GPUs.\n\n      \nUseful for very large operations that can be broken down into smaller, parallelizable tasks.\n\n    \n\n  \n\n  \nCons\n:\n    \n\n      \nComplex to implement and manage.\n\n      \nCommunication overhead can be significant, particularly for operations with large intermediate results.\n\n    \n\n  \n\n  \nExample\n:\n    \n\n      \nSimilar to tensor-wise parallelism, operator-wise parallelism is usually implemented within deep learning libraries and frameworks, optimizing specific operations like matrix multiplications.\n\n    \n\n  \n\n\n\n\n\nSummary\n\n\n\n\n  \n\n    \nModel parallelism can be categorized into three main types, each suited to different scenarios and offering unique advantages and challenges:\n\n\n    \n\n      \nLayer-wise Parallelism\n: Simple to implement and effective for models with clear layer separations but may lead to imbalanced GPU utilization and communication overhead.\n\n      \nTensor-wise Parallelism\n: Efficiently uses GPU memory and balances computational load by splitting large tensors but requires sophisticated slicing and communication management.\n\n      \nOperator-wise Parallelism\n: Maximizes parallel efficiency by splitting operations within layers but is complex to implement and can incur significant communication overhead.\n\n    \n\n  \n\n  \n\n    \nChoosing the appropriate type of model parallelism depends on the model architecture, the available hardware, and the specific requirements of the training task. Advanced deep learning frameworks and libraries provide tools and functionalities to facilitate the implementation of these parallelism strategies, enabling the training of large and complex models.\n\n  \n\n\n\n\n\nComparative Analysis: Types of Model Parallelism\n\n\n\nCriteria for Comparison\n\n\n\n  \nImplementation Complexity\n\n  \nGPU Utilization\n\n  \nCommunication Overhead\n\n  \nScalability\n\n  \nSuitable Use Cases\n\n\n\n\n\nLayer-wise Parallelism\n\n\n\n\n  \nImplementation Complexity\n:\n    \n\n      \nLow to Medium\n: It’s relatively straightforward to assign different layers or groups of layers to different GPUs. This method is particularly simple for models with a clear sequential structure.\n\n    \n\n  \n\n  \nGPU Utilization\n:\n    \n\n      \nVariable\n: The utilization can be uneven if different layers have varying computational loads. Some GPUs may be underutilized while waiting for others to complete their tasks.\n\n    \n\n  \n\n  \nCommunication Overhead\n:\n    \n\n      \nHigh\n: Significant overhead can arise from transferring large intermediate results between GPUs. This can be a bottleneck, especially with deep models producing large outputs.\n\n    \n\n  \n\n  \nScalability\n:\n    \n\n      \nModerate\n: Scalability is limited by the number of layers that can be effectively divided among GPUs. Deep models with many layers can benefit, but shallow models may not.\n\n    \n\n  \n\n  \nSuitable Use Cases\n:\n    \n\n      \nSequential models with distinct layers, such as feedforward neural networks or deep convolutional networks.\n\n    \n\n  \n\n\n\n\n\nTensor-wise Parallelism\n\n\n\n\n  \nImplementation Complexity\n:\n    \n\n      \nHigh\n: Requires advanced techniques to split and manage tensors across multiple GPUs. It involves slicing tensors along specific dimensions and ensuring correct aggregation of results.\n\n    \n\n  \n\n  \nGPU Utilization\n:\n    \n\n      \nHigh\n: Distributes large tensors effectively across multiple GPUs, leading to balanced utilization. Each GPU handles a portion of the tensor, optimizing memory and computational resources.\n\n    \n\n  \n\n  \nCommunication Overhead\n:\n    \n\n      \nModerate to High\n: While each GPU processes its tensor slice independently, the results must be combined, leading to communication overhead. However, this is often more manageable compared to layer-wise parallelism.\n\n    \n\n  \n\n  \nScalability\n:\n    \n\n      \nHigh\n: Scales well with the number of GPUs, as large tensors can be divided into more slices to distribute the load evenly.\n\n    \n\n  \n\n  \nSuitable Use Cases\n:\n    \n\n      \nLarge models with extremely large tensors, such as language models with massive weight matrices.\n\n    \n\n  \n\n\n\n\n\nOperator-wise Parallelism\n\n\n\n\n  \nImplementation Complexity\n:\n    \n\n      \nVery High\n: This method is the most complex to implement. It requires decomposing individual operations within a layer and distributing these subtasks across multiple GPUs. Fine-grained synchronization is necessary.\n\n    \n\n  \n\n  \nGPU Utilization\n:\n    \n\n      \nVery High\n: Offers optimal utilization by leveraging parallelism within operations. Each GPU can work on different parts of the same operation concurrently, maximizing efficiency.\n\n    \n\n  \n\n  \nCommunication Overhead\n:\n    \n\n      \nHigh\n: The fine-grained nature of this parallelism results in frequent communication between GPUs, which can be a significant overhead if not managed carefully.\n\n    \n\n  \n\n  \nScalability\n:\n    \n\n      \nVery High\n: Highly scalable as even small operations can be divided among GPUs, making it suitable for extremely large models with complex operations.\n\n    \n\n  \n\n  \nSuitable Use Cases\n:\n    \n\n      \nExtremely large and complex models, such as those used in cutting-edge research in AI and deep learning, where maximizing computational efficiency is crucial.\n\n    \n\n  \n\n\n\n\n\nSummary Table\n\n\n\n\n\n\n \n\n\n\n\nCriterion\n\n\nLayer-wise Parallelism\n\n\nTensor-wise Parallelism\n\n\nOperator-wise Parallelism\n\n\n\n\n\n\n\n\n\n\nImplementation Complexity\n\n\nLow to Medium\n\n\nHigh\n\n\nVery High\n\n\n\n\n\n\nGPU Utilization\n\n\nVariable\n\n\nHigh\n\n\nVery High\n\n\n\n\n\n\nCommunication Overhead\n\n\nHigh\n\n\nModerate to High\n\n\nHigh\n\n\n\n\n\n\nScalability\n\n\nModerate\n\n\nHigh\n\n\nVery High\n\n\n\n\n\n\nSuitable Use Cases\n\n\nSequential models with distinct layers\n\n\nLarge models with very large tensors\n\n\nExtremely large and complex models\n\n\n\n\n\n\n\n\n\n\n\nChoosing the Right Type\n\n\n\n\n  \nLayer-wise Parallelism\n: Best suited for simpler models with clear, sequential layers where ease of implementation is a priority, and communication overhead can be tolerated.\n\n  \nTensor-wise Parallelism\n: Ideal for models with very large tensors where balanced GPU utilization and memory efficiency are critical.\n\n  \n\n    \nOperator-wise Parallelism\n: Appropriate for the most demanding and complex models where maximizing computational efficiency and scalability is essential, despite the high implementation complexity.\n\n  \n\n  \nUnderstanding these differences can help in selecting the most appropriate model parallelism strategy based on the specific requirements and constraints of the neural network model and the available hardware.\n\n\n\n\n\nHybrid (Model and Data) Parallelism\n\n\n\nFully Sharded Data Parallel (FSDP)\n\n\n\n\n  \nFully Sharded Data Parallel (FSDP) is a feature in PyTorch that addresses the limitations of data parallelism by allowing more efficient utilization of memory and compute resources. FSDP shards (i.e., splits) both the model parameters and optimizer states across all available GPUs, significantly reducing memory usage and enabling the training of very large models that otherwise wouldn’t fit into GPU memory.\n\n  \n\n    \nFully Sharded Data Parallel (FSDP) is primarily a form of model parallelism. However, it incorporates elements of data parallelism as well. Here’s how it works:\n\n\n    \n\n      \n\n        \nModel Parallelism\n: FSDP divides the model parameters across multiple GPUs. This means that each GPU only holds a shard (a portion) of the entire model. This is a key characteristic of model parallelism, where the model is split and distributed across different devices to manage memory more efficiently and to enable training of larger models that wouldn’t fit into a single device’s memory.\n\n      \n\n      \n\n        \nData Parallelism\n: Within each shard, FSDP also performs data parallel training. This means that the data is split across the GPUs, and each GPU processes a different subset of the data. Gradients are computed locally on each GPU and then synchronized across GPUs to update the model parameters consistently.\n\n      \n\n    \n\n  \n\n  \nBy combining these two approaches, FSDP aims to maximize the efficiency of both memory usage and computational resources. This hybrid approach allows for the training of very large models that would be infeasible with either pure data parallelism or pure model parallelism alone.\n\n\n\n\n\nKey Features of FSDP\n\n\n\n\n  \nParameter Sharding:\n Each GPU holds only a shard of the full model parameters, reducing memory overhead.\n\n  \nOptimizer State Sharding:\n Similar to parameter sharding, the optimizer states are also sharded across GPUs.\n\n  \nGradient Sharding:\n During backpropagation, gradients are sharded across GPUs, minimizing memory usage.\n\n  \nEfficient Communication:\n Uses collective communication to gather and reduce gradients across GPUs.\n\n\n\n\n\nTechnical Details\n\n\n\n\n  \nInitialization:\n The process group is initialized for communication between GPUs.\n\n  \nWrapping the Model:\n The model is wrapped with \ntorch.distributed.fsdp.FullyShardedDataParallel\n.\n\n  \nData Loading:\n Data loaders and samplers are configured to distribute data across GPUs.\n\n  \nTraining Loop:\n The training loop is similar to standard PyTorch, but with the added benefit of memory efficiency and scalability.\n\n\n\n\n\nCode Sample\n\n\n\n\n  \nHere’s a simple example demonstrating how to set up and use FSDP in PyTorch.\n\n\n\n\n\nimport\n \nos\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.distributed\n \nas\n \ndist\n\n\nimport\n \ntorch.multiprocessing\n \nas\n \nmp\n\n\nfrom\n \ntorch.distributed.fsdp\n \nimport\n \nFullyShardedDataParallel\n \nas\n \nFSDP\n\n\nfrom\n \ntorch.distributed.fsdp.wrap\n \nimport\n \nwrap\n\n\nfrom\n \ntorchvision\n \nimport\n \ndatasets\n,\n \ntransforms\n\n\nfrom\n \ntorch.utils.data\n \nimport\n \nDataLoader\n,\n \nDistributedSampler\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\n\ndef\n \nsetup\n(\nrank\n,\n \nworld_size\n):\n\n    \nos\n.\nenviron\n[\n'MASTER_ADDR'\n]\n \n=\n \n'localhost'\n\n    \nos\n.\nenviron\n[\n'MASTER_PORT'\n]\n \n=\n \n'12355'\n\n    \ndist\n.\ninit_process_group\n(\n\"nccl\"\n,\n \nrank\n=\nrank\n,\n \nworld_size\n=\nworld_size\n)\n\n\n\ndef\n \ncleanup\n():\n\n    \ndist\n.\ndestroy_process_group\n()\n\n\n\nclass\n \nSimpleCNN\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nSimpleCNN\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nconv1\n \n=\n \nnn\n.\nConv2d\n(\n1\n,\n \n32\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nconv2\n \n=\n \nnn\n.\nConv2d\n(\n32\n,\n \n64\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nfc1\n \n=\n \nnn\n.\nLinear\n(\n64\n \n*\n \n28\n \n*\n \n28\n,\n \n128\n)\n\n        \nself\n.\nfc2\n \n=\n \nnn\n.\nLinear\n(\n128\n,\n \n10\n)\n\n    \n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv1\n(\nx\n))\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv2\n(\nx\n))\n\n        \nx\n \n=\n \nx\n.\nview\n(\n-\n1\n,\n \n64\n \n*\n \n28\n \n*\n \n28\n)\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nfc1\n(\nx\n))\n\n        \nx\n \n=\n \nself\n.\nfc2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\ndef\n \ntrain\n(\nrank\n,\n \nworld_size\n,\n \nepochs\n):\n\n    \nsetup\n(\nrank\n,\n \nworld_size\n)\n\n    \n    \ntransform\n \n=\n \ntransforms\n.\nCompose\n([\n\n        \ntransforms\n.\nToTensor\n(),\n\n        \ntransforms\n.\nNormalize\n((\n0.1307\n,),\n \n(\n0.3081\n,))\n\n    \n])\n\n    \n    \ntrain_dataset\n \n=\n \ndatasets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \ntrain\n=\nTrue\n,\n \ndownload\n=\nTrue\n,\n \ntransform\n=\ntransform\n)\n\n    \ntrain_sampler\n \n=\n \nDistributedSampler\n(\ntrain_dataset\n,\n \nnum_replicas\n=\nworld_size\n,\n \nrank\n=\nrank\n)\n\n    \ntrain_loader\n \n=\n \nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \nbatch_size\n=\n64\n,\n \nsampler\n=\ntrain_sampler\n)\n\n    \n    \nmodel\n \n=\n \nSimpleCNN\n().\nto\n(\nrank\n)\n\n    \nmodel\n \n=\n \nwrap\n(\nmodel\n)\n  \n# Wrap the model with FSDP\n\n    \n    \nmodel\n \n=\n \nFSDP\n(\nmodel\n).\nto\n(\nrank\n)\n\n    \n    \ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n().\nto\n(\nrank\n)\n\n    \noptimizer\n \n=\n \noptim\n.\nAdam\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.001\n)\n\n    \n    \nfor\n \nepoch\n \nin\n \nrange\n(\nepochs\n):\n\n        \nmodel\n.\ntrain\n()\n\n        \nrunning_loss\n \n=\n \n0.0\n\n        \nfor\n \ni\n,\n \n(\ninputs\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n            \ninputs\n,\n \nlabels\n \n=\n \ninputs\n.\nto\n(\nrank\n),\n \nlabels\n.\nto\n(\nrank\n)\n\n            \n            \noptimizer\n.\nzero_grad\n()\n\n            \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n            \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n            \nloss\n.\nbackward\n()\n\n            \noptimizer\n.\nstep\n()\n\n            \n            \nrunning_loss\n \n+=\n \nloss\n.\nitem\n()\n\n            \nif\n \ni\n \n%\n \n100\n \n==\n \n99\n:\n\n                \nprint\n(\nf\n'Rank \n{\nrank\n}\n, Epoch [\n{\nepoch\n \n+\n \n1\n}\n, \n{\ni\n \n+\n \n1\n}\n] loss: \n{\nrunning_loss\n \n/\n \n100\n:\n.\n3\nf\n}\n'\n)\n\n                \nrunning_loss\n \n=\n \n0.0\n\n    \n    \ncleanup\n()\n\n\n\ndef\n \nmain\n():\n\n    \nworld_size\n \n=\n \ntorch\n.\ncuda\n.\ndevice_count\n()\n\n    \nepochs\n \n=\n \n5\n\n    \nmp\n.\nspawn\n(\ntrain\n,\n \nargs\n=\n(\nworld_size\n,\n \nepochs\n),\n \nnprocs\n=\nworld_size\n,\n \njoin\n=\nTrue\n)\n\n\n\nif\n \n__name__\n \n==\n \n\"__main__\"\n:\n\n    \nmain\n()\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n  \nSetup and Cleanup:\n Functions to initialize and destroy the process group for distributed training.\n\n  \nModel Definition:\n A simple CNN model.\n\n  \nTrain Function:\n\n    \n\n      \nInitializes the process group.\n\n      \nSets up a distributed sampler and data loader.\n\n      \nWraps the model with FSDP for efficient sharding.\n\n      \nRuns the training loop.\n\n    \n\n  \n\n  \nMain Function:\n Uses \nmp.spawn\n to launch multiple processes for distributed training.\n\n\n\n\n\nBenefits of FSDP\n\n\n\n\n  \nMemory Efficiency:\n By sharding parameters and optimizer states, FSDP significantly reduces memory overhead, allowing for the training of larger models.\n\n  \nScalability:\n Efficient communication and sharding mechanisms enable scaling to multiple GPUs and nodes with minimal overhead.\n\n  \n\n    \nEase of Use:\n Integrates seamlessly with PyTorch’s existing APIs and workflows, requiring minimal changes to existing code.\n\n  \n\n  \nOverall, FSDP is a powerful tool for scaling deep learning models efficiently across multiple GPUs, providing both memory and computational benefits.\n\n\n\n\n\nTensor Parallelism\n\n\n\nConcept\n\n\n\n\n  \nTensor parallelism is a technique used in training deep learning models to distribute the computational load of processing large tensors (multi-dimensional arrays of numbers) across multiple devices, such as GPUs. This allows for the efficient training of very large models that would otherwise be too large to fit into the memory of a single device. By splitting the tensors across devices, tensor parallelism enables faster computation and better utilization of hardware resources.\n\n\n\n\n\nMechanism\n\n\n\n\n  \nThe mechanism of tensor parallelism involves dividing large tensors along certain dimensions and distributing these chunks across multiple GPUs. Each GPU performs computations on its subset of the data, and the results are then combined to form the final output. This can be done at various stages of the model, such as during the forward pass, backward pass, or during gradient updates.\n\n\n\n\n\nTypes of Tensor Parallelism\n\n\n\n\n  \n\n    \nModel Parallelism\n: This involves splitting the model itself across different devices. Each device handles a different part of the model. This is often combined with tensor parallelism for efficient training.\n\n  \n\n  \n\n    \nData Parallelism\n: Here, the entire model is replicated across different devices, and each device processes different mini-batches of the data. The results are then averaged or combined.\n\n  \n\n  \n\n    \nPipeline Parallelism\n: The model is divided into sequential stages, and each stage is assigned to a different device. Data is passed through these stages in a pipeline fashion.\n\n  \n\n  \n\n    \nTensor-Slicing Parallelism\n: Specifically focuses on slicing tensors along specific dimensions and distributing the slices across multiple devices. This can be done within layers of a neural network.\n\n  \n\n\n\n\n\nPros and Cons\n\n\n\nPros:\n\n\n\n\n  \nScalability\n: Allows for the training of larger models by utilizing the combined memory of multiple devices.\n\n  \nEfficiency\n: Improves computational efficiency by parallelizing operations.\n\n  \nFlexibility\n: Can be combined with other parallelism strategies like data and pipeline parallelism for optimal performance.\n\n\n\n\n\nCons:\n\n\n\n\n  \nComplexity\n: Increases the complexity of model implementation and debugging.\n\n  \nCommunication Overhead\n: Requires efficient communication between devices, which can become a bottleneck.\n\n  \nResource Management\n: Needs careful management of resources and synchronization.\n\n\n\n\n\nUse Cases\n\n\n\n\n  \nLarge Language Models\n: Training models like GPT-3 and BERT that require vast amounts of computational power and memory.\n\n  \nComputer Vision\n: Handling high-resolution images and videos in models like CNNs.\n\n  \nScientific Computing\n: Simulations and modeling in physics, chemistry, and other sciences that involve large-scale tensor computations.\n\n  \nRecommender Systems\n: Managing and processing large embeddings and user-item interaction matrices.\n\n\n\n\n\nImplementation in PyTorch\n\n\n\n\n  \n\n    \nImplementing tensor parallelism in PyTorch involves the following steps:\n\n\n    \n\n      \nModel Partitioning\n: Split the model layers or tensors across different devices.\n\n      \nData Distribution\n: Distribute the input data across these devices.\n\n      \nComputation\n: Perform the necessary computations on each device.\n\n      \nSynchronization\n: Combine the results from different devices.\n\n    \n\n  \n\n  \n\n    \nPyTorch provides various utilities and functions to facilitate tensor parallelism, such as:\n\n\n    \n\n      \ntorch.distributed\n: A package that includes functionalities for distributed computing.\n\n      \ntorch.nn.parallel.DistributedDataParallel\n: Wraps a model for multi-GPU parallelism.\n\n      \nRPC Framework\n: Allows for remote procedure calls and managing distributed model components.\n\n    \n\n  \n\n  \n\n    \nExample:\n\n  \n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.distributed\n \nas\n \ndist\n\n\nfrom\n \ntorch.nn.parallel\n \nimport\n \nDistributedDataParallel\n \nas\n \nDDP\n\n\n\n# Initialize the process group\n\ndist\n.\ninit_process_group\n(\nbackend\n=\n'nccl'\n)\n\n\n\n# Create model and move to GPU\n\nmodel\n \n=\n \nMyModel\n().\nto\n(\ndevice\n)\n\n\nmodel\n \n=\n \nDDP\n(\nmodel\n)\n\n\n\n# Define optimizer and loss function\n\noptimizer\n \n=\n \ntorch\n.\noptim\n.\nAdam\n(\nmodel\n.\nparameters\n())\n\n\ncriterion\n \n=\n \ntorch\n.\nnn\n.\nCrossEntropyLoss\n()\n\n\n\n# Training loop\n\nfor\n \nepoch\n \nin\n \nrange\n(\nnum_epochs\n):\n\n    \nfor\n \ndata\n,\n \ntarget\n \nin\n \ndataloader\n:\n\n        \ndata\n,\n \ntarget\n \n=\n \ndata\n.\nto\n(\ndevice\n),\n \ntarget\n.\nto\n(\ndevice\n)\n\n        \noptimizer\n.\nzero_grad\n()\n\n        \noutput\n \n=\n \nmodel\n(\ndata\n)\n\n        \nloss\n \n=\n \ncriterion\n(\noutput\n,\n \ntarget\n)\n\n        \nloss\n.\nbackward\n()\n\n        \noptimizer\n.\nstep\n()\n\n\n\n\n\nConclusion\n\n\n\n\n  \nTensor parallelism is a powerful technique for training large-scale deep learning models by distributing tensor computations across multiple devices. While it introduces some complexity and requires efficient communication mechanisms, it enables the training of models that would be infeasible on a single device. Combining tensor parallelism with other forms of parallelism can further optimize performance and resource utilization.\n\n\n\n\n\nSummary\n\n\n\n\n  \nConcept\n: Tensor parallelism distributes large tensor computations across multiple devices to enable efficient training of large models.\n\n  \nMechanism\n: Involves dividing tensors and distributing them across devices, performing computations, and combining results.\n\n  \nTypes\n: Includes model parallelism, data parallelism, pipeline parallelism, and tensor-slicing parallelism.\n\n  \nPros and Cons\n: Offers scalability and efficiency but introduces complexity and potential communication overhead.\n\n  \nUse Cases\n: Suitable for large language models, computer vision, scientific computing, and recommender systems.\n\n  \nImplementation in PyTorch\n: Uses PyTorch’s distributed computing utilities to partition models, distribute data, and synchronize results.\n\n  \nConclusion\n: Tensor parallelism is essential for training very large models, enabling the use of multiple devices to overcome memory and computational limitations.\n\n\n\n\n\nPipeline Parallelism\n\n\n\nConcept\n\n\n\n\n  \nPipeline parallelism in deep learning refers to the technique of distributing the workload of training a deep learning model across multiple devices or processing units. Unlike data parallelism, where different batches of data are processed simultaneously by multiple replicas of a model, pipeline parallelism splits the model itself into different stages and assigns each stage to a different device. This allows for the concurrent execution of different parts of the model on different devices, thus improving training efficiency and reducing time-to-train.\n\n\n\n\n\nMechanism\n\n\n\n\n  \n\n    \nThe mechanism of pipeline parallelism involves dividing the model into sequential stages. Each stage consists of a subset of the model’s layers, which are assigned to different devices. During training, a minibatch of data is passed through the first stage on the first device, which then forwards the intermediate results (activations) to the next stage on the second device, and so on. While the first minibatch progresses through the pipeline, subsequent minibatches can start being processed by earlier stages, creating an overlapping execution of different minibatches.\n\n  \n\n  \n\n    \nKey steps in the mechanism:\n\n    \n\n      \nModel Partitioning:\n Split the model into sequential stages.\n\n      \nDevice Allocation:\n Assign each stage to a different device.\n\n      \nForward Pass:\n Process minibatches through the pipeline stages in sequence.\n\n      \nBackward Pass:\n Perform gradient computations and backpropagation in reverse order through the pipeline stages.\n\n    \n\n  \n\n\n\n\n\nTypes of Pipeline Parallelism\n\n\n\n\n  \n1F1B (One Forward One Backward):\n Each device alternates between forward and backward passes, with only one minibatch being processed at a time per device.\n\n  \nGPipe:\n Introduces micro-batching, where a minibatch is further divided into smaller micro-batches to keep all devices busy by processing different micro-batches simultaneously.\n\n  \nInterleaved Pipeline Parallelism:\n Combines pipeline and data parallelism by interleaving multiple pipeline replicas, allowing for more efficient utilization of resources.\n\n\n\n\n\nPros and Cons\n\n\n\n\n  \nPros:\n\n    \n\n      \nEfficient Resource Utilization:\n By splitting the model, all devices can be kept busy, reducing idle time.\n\n      \nScalability:\n Allows training of larger models that do not fit into the memory of a single device.\n\n      \nReduced Training Time:\n Concurrent execution of different parts of the model can lead to faster training.\n\n    \n\n  \n\n  \nCons:\n\n    \n\n      \nComplexity:\n Implementing pipeline parallelism requires careful model partitioning and synchronization between devices.\n\n      \nCommunication Overhead:\n Transferring intermediate activations between devices can introduce significant communication overhead.\n\n      \nLatency:\n The sequential nature of the pipeline can introduce latency, especially if there are imbalances in the computational load across stages.\n\n    \n\n  \n\n\n\n\n\nUse Cases\n\n\n\n\n  \nLarge Model Training:\n Ideal for training very large models that exceed the memory capacity of a single device, such as transformer-based models in NLP.\n\n  \nResource-Constrained Environments:\n Useful in environments with limited device memory but multiple devices available.\n\n  \nPerformance Optimization:\n Can be used to optimize training performance by balancing the load across multiple devices.\n\n\n\n\n\nImplementation in PyTorch\n\n\n\n\n  \nPyTorch provides support for pipeline parallelism through the \ntorch.distributed.pipeline.sync\n package. Here’s a high-level overview of how to implement it:\n\n\n\n\n\n\n  \nPartition the Model:\n\n    \nfrom\n \ntorch.distributed.pipeline.sync\n \nimport\n \nPipe\n\n\n\n# Assume 'model' is the original large model\n# Split the model into two stages\n\nmodel\n \n=\n \nnn\n.\nSequential\n(...)\n\n\nmodel\n \n=\n \nnn\n.\nSequential\n(\n\n    \nnn\n.\nSequential\n(\n*\nmodel\n[:\nlen\n(\nmodel\n)\n//\n2\n]),\n\n    \nnn\n.\nSequential\n(\n*\nmodel\n[\nlen\n(\nmodel\n)\n//\n2\n:])\n\n\n)\n\n\n\n# Wrap the model with Pipe\n\nmodel\n \n=\n \nPipe\n(\nmodel\n,\n \nchunks\n=\n8\n)\n\n\n    \n\n  \n\n  \nSetup Devices:\n\n    \n# Assume we have 2 GPUs\n\ndevices\n \n=\n \n[\ntorch\n.\ndevice\n(\n'cuda:0'\n),\n \ntorch\n.\ndevice\n(\n'cuda:1'\n)]\n\n\nmodel\n \n=\n \nmodel\n.\nto\n(\ndevices\n)\n\n\n    \n\n  \n\n  \nTraining Loop:\n\n    \nfor\n \ninput\n,\n \ntarget\n \nin\n \ndata_loader\n:\n\n    \noutput\n \n=\n \nmodel\n(\ninput\n)\n\n    \nloss\n \n=\n \ncriterion\n(\noutput\n,\n \ntarget\n)\n\n    \nloss\n.\nbackward\n()\n\n    \noptimizer\n.\nstep\n()\n\n\n    \n\n  \n\n\n\n\n\nConclusion\n\n\n\n\n  \nPipeline parallelism is a powerful technique for improving the efficiency and scalability of training deep learning models. By partitioning the model and distributing it across multiple devices, it enables concurrent processing and better resource utilization. However, it also introduces complexity in implementation and communication overhead. Tools like PyTorch make it easier to implement pipeline parallelism, allowing researchers and engineers to train larger models more efficiently.\n\n\n\n\n\nSummary\n\n\n\n\n  \nPipeline parallelism distributes the training of deep learning models across multiple devices by partitioning the model into stages. It allows for concurrent execution of different model parts, improving training efficiency and scalability. There are various types, such as 1F1B and GPipe, each with its own advantages and trade-offs. While it offers significant benefits in terms of resource utilization and training speed, it also presents challenges related to complexity and communication overhead. PyTorch provides tools to facilitate the implementation of pipeline parallelism, making it accessible for large-scale model training.\n\n\n\n\n\nDeepSpeed\n\n\n\n\n  \nDeepSpeed is an open-source deep learning optimization library developed by Microsoft. It is designed to enable the training of large-scale models efficiently by providing state-of-the-art optimizations for memory, computation, and distributed training. DeepSpeed combines various techniques and tools to achieve these goals, including memory optimization, parallelism strategies, and efficient kernel implementations.\n\n  \nDeepSpeed offers both data parallelism and model parallelism, with a particular focus on optimizing and scaling large model training. Here’s how each is implemented:\n    \n\n      \nData Parallelism\n:\n        \n\n          \nDeepSpeed\n provides standard data parallelism, where the dataset is split across multiple GPUs or nodes. Each GPU processes a different subset of the data, and the gradients are synchronized across GPUs to ensure consistent model updates.\n\n        \n\n      \n\n      \nTensor and Pipeline Parallelism\n:\n        \n\n          \nDeepSpeed\n supports tensor slicing or pipeline parallelism, where different layers or parts of the model are distributed across different GPUs.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nKey Features of DeepSpeed\n\n\n\n\n  \nZeRO (Zero Redundancy Optimizer):\n DeepSpeed introduces ZeRO to optimize memory usage by partitioning model states across data parallel processes. ZeRO has different stages to progressively reduce memory consumption:\n    \n\n      \nZeRO Stage 1:\n Shards optimizer states.\n\n      \nZeRO Stage 2:\n Shards gradients.\n\n      \nZeRO Stage 3:\n Shards model parameters.\n\n    \n\n  \n\n  \nMemory Optimization:\n Techniques such as activation checkpointing, gradient accumulation, and offloading to CPU/NVMe to manage and reduce memory footprint.\n\n  \nMixed Precision Training:\n Support for FP16 and BF16 training to leverage hardware acceleration and reduce memory usage.\n\n  \nEfficient Kernel Implementations:\n Optimized kernels for various operations to accelerate training.\n\n  \nEase of Integration:\n Simple APIs to integrate DeepSpeed into existing PyTorch codebases with minimal changes.\n\n\n\n\n\nTechnical Details\n\n\n\n\n  \nDeepSpeed’s architecture is built around the ZeRO optimization framework, which targets memory efficiency and scalability. It breaks down the training process into manageable parts and distributes them across multiple GPUs, reducing the per-GPU memory load and enabling the training of very large models.\n\n\n\n\n\nCode Sample\n\n\n\n\n  \nHere’s a basic example demonstrating how to set up and use DeepSpeed in a PyTorch training script.\n\n\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\nfrom\n \ntorchvision\n \nimport\n \ndatasets\n,\n \ntransforms\n\n\nimport\n \ndeepspeed\n\n\nfrom\n \ntorch.utils.data\n \nimport\n \nDataLoader\n\n\n\nclass\n \nSimpleCNN\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nSimpleCNN\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nconv1\n \n=\n \nnn\n.\nConv2d\n(\n1\n,\n \n32\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nconv2\n \n=\n \nnn\n.\nConv2d\n(\n32\n,\n \n64\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nfc1\n \n=\n \nnn\n.\nLinear\n(\n64\n \n*\n \n28\n \n*\n \n28\n,\n \n128\n)\n\n        \nself\n.\nfc2\n \n=\n \nnn\n.\nLinear\n(\n128\n,\n \n10\n)\n\n    \n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv1\n(\nx\n))\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv2\n(\nx\n))\n\n        \nx\n \n=\n \nx\n.\nview\n(\n-\n1\n,\n \n64\n \n*\n \n28\n \n*\n \n28\n)\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nfc1\n(\nx\n))\n\n        \nx\n \n=\n \nself\n.\nfc2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\ndef\n \nmain\n():\n\n    \n# Define transformations and dataset\n\n    \ntransform\n \n=\n \ntransforms\n.\nCompose\n([\n\n        \ntransforms\n.\nToTensor\n(),\n\n        \ntransforms\n.\nNormalize\n((\n0.1307\n,),\n \n(\n0.3081\n,))\n\n    \n])\n\n    \n    \ntrain_dataset\n \n=\n \ndatasets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \ntrain\n=\nTrue\n,\n \ndownload\n=\nTrue\n,\n \ntransform\n=\ntransform\n)\n\n    \ntrain_loader\n \n=\n \nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \nbatch_size\n=\n64\n,\n \nshuffle\n=\nTrue\n)\n\n    \n    \n# Initialize model\n\n    \nmodel\n \n=\n \nSimpleCNN\n()\n\n\n    \n# Define optimizer\n\n    \noptimizer\n \n=\n \noptim\n.\nAdam\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.001\n)\n\n    \n    \n# DeepSpeed configuration\n\n    \ndeepspeed_config\n \n=\n \n{\n\n        \n\"train_batch_size\"\n:\n \n64\n,\n\n        \n\"gradient_accumulation_steps\"\n:\n \n1\n,\n\n        \n\"optimizer\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"Adam\"\n,\n\n            \n\"params\"\n:\n \n{\n\n                \n\"lr\"\n:\n \n0.001\n,\n\n                \n\"betas\"\n:\n \n[\n0.9\n,\n \n0.999\n],\n\n                \n\"eps\"\n:\n \n1e-8\n\n            \n}\n\n        \n},\n\n        \n\"fp16\"\n:\n \n{\n\n            \n\"enabled\"\n:\n \nTrue\n\n        \n},\n\n        \n\"zero_optimization\"\n:\n \n{\n\n            \n\"stage\"\n:\n \n1\n\n        \n}\n\n    \n}\n\n    \n    \n# Initialize DeepSpeed\n\n    \nmodel\n,\n \noptimizer\n,\n \n_\n,\n \n_\n \n=\n \ndeepspeed\n.\ninitialize\n(\nmodel\n=\nmodel\n,\n \noptimizer\n=\noptimizer\n,\n \nconfig\n=\ndeepspeed_config\n)\n\n    \n    \ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n\n    \n# Training loop\n\n    \nmodel\n.\ntrain\n()\n\n    \nfor\n \nepoch\n \nin\n \nrange\n(\n5\n):\n\n        \nrunning_loss\n \n=\n \n0.0\n\n        \nfor\n \ni\n,\n \n(\ninputs\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n            \ninputs\n,\n \nlabels\n \n=\n \ninputs\n.\nto\n(\nmodel\n.\ndevice\n),\n \nlabels\n.\nto\n(\nmodel\n.\ndevice\n)\n\n            \n            \noptimizer\n.\nzero_grad\n()\n\n            \noutputs\n \n=\n \nmodel\n(\ninputs\n)\n\n            \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n            \nmodel\n.\nbackward\n(\nloss\n)\n\n            \nmodel\n.\nstep\n()\n\n            \n            \nrunning_loss\n \n+=\n \nloss\n.\nitem\n()\n\n            \nif\n \ni\n \n%\n \n100\n \n==\n \n99\n:\n\n                \nprint\n(\nf\n'Epoch [\n{\nepoch\n \n+\n \n1\n}\n, \n{\ni\n \n+\n \n1\n}\n] loss: \n{\nrunning_loss\n \n/\n \n100\n:\n.\n3\nf\n}\n'\n)\n\n                \nrunning_loss\n \n=\n \n0.0\n\n    \n    \nprint\n(\n'Training finished.'\n)\n\n\n\nif\n \n__name__\n \n==\n \n\"__main__\"\n:\n\n    \nmain\n()\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n  \nModel Definition:\n A simple CNN model is defined.\n\n  \nData Loading:\n The MNIST dataset is loaded and transformed.\n\n  \nOptimizer Definition:\n An Adam optimizer is defined.\n\n  \nDeepSpeed Configuration:\n A configuration dictionary for DeepSpeed is defined, specifying batch size, optimizer settings, mixed precision (fp16), and ZeRO optimization stage.\n\n  \nInitialize DeepSpeed:\n The model and optimizer are initialized with DeepSpeed, which wraps the model to handle optimization.\n\n  \nTraining Loop:\n The training loop runs similarly to standard PyTorch but uses \nmodel.backward(loss)\n and \nmodel.step()\n to leverage DeepSpeed’s optimization.\n\n\n\n\n\nBenefits of DeepSpeed\n\n\n\n\n  \nMemory Efficiency:\n Significantly reduces memory usage, enabling training of larger models.\n\n  \nScalability:\n Efficiently scales training across multiple GPUs and nodes.\n\n  \nPerformance:\n Provides performance improvements through optimized kernels and mixed precision training.\n\n  \nEase of Use:\n Integrates seamlessly with PyTorch, requiring minimal code changes.\n\n  \nOverall, DeepSpeed is a powerful tool for scaling deep learning models efficiently, providing advanced memory optimizations and performance enhancements to facilitate large-scale model training.\n\n\n\n\n\nDeepSpeed ZeRO\n\n\n\n\n  \nDeepSpeed ZeRO (Zero Redundancy Optimizer) is a highly optimized and memory-efficient approach for training large-scale deep learning models. ZeRO reduces the memory footprint of model states (i.e., optimizer states, gradients, and model parameters) by partitioning them across data-parallel processes. This approach enables the training of models that are significantly larger than what could fit in the memory of a single GPU.\n\n  \nZeRO offers both data parallelism and model parallelism, with a particular focus on optimizing and scaling large model training. Here’s how each is implemented:\n    \n\n      \nData Parallelism in ZeRO\n:\n        \n\n          \nZeRO\n enhances data parallelism by minimizing memory redundancies. It splits the optimizer states, gradients, and parameters across the GPUs, reducing memory usage and allowing for the training of larger models. This is known as ZeRO Stage 1 and 2 optimizations.\n\n        \n\n      \n\n      \nModel Parallelism in ZeRO\n:\n        \n\n          \nZeRO\n implements model parallelism more effectively in its ZeRO Stage 3 optimization. In this stage, it further shards all elements of model states (including optimizer states, gradients, and parameters) across all GPUs, thus combining model parallelism with data parallelism. This allows for an even more efficient distribution of memory and computational load.\n\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nKey Features of DeepSpeed ZeRO\n\n\n\n\n  \nOptimizer State Partitioning (ZeRO Stage 1):\n Partitions optimizer states across GPUs, reducing memory redundancy.\n\n  \nGradient Partitioning (ZeRO Stage 2):\n Partitions gradients across GPUs, further reducing memory usage.\n\n  \nParameter Partitioning (ZeRO Stage 3):\n Partitions model parameters across GPUs, enabling the largest possible models to be trained.\n\n\n\n\n\nTechnical Details\n\n\n\n\n  \nStage 1 (Optimizer State Sharding):\n Optimizer states (e.g., momentum and variance in Adam) are divided among all data-parallel processes.\n\n  \nStage 2 (Gradient Sharding):\n In addition to sharding optimizer states, gradients are also partitioned, reducing memory requirements during backpropagation.\n\n  \nStage 3 (Parameter Sharding):\n Parameters are sharded across processes, with each process holding only a part of the model parameters. During forward and backward passes, parameters are gathered and then distributed again.\n\n\n\n\n\nBenefits\n\n\n\n\n  \nMemory Efficiency:\n Significantly reduces memory overhead, allowing for training of larger models.\n\n  \nScalability:\n Scales efficiently across multiple GPUs and nodes.\n\n  \nPerformance:\n Maintains high training performance through efficient communication and computation strategies.\n\n\n\n\n\nCode Sample\n\n\n\nHere is a code sample demonstrating the use of DeepSpeed with ZeRO optimization.\n\n\n\nimport\n \ntorch\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch.optim\n \nas\n \noptim\n\n\nfrom\n \ntorchvision\n \nimport\n \ndatasets\n,\n \ntransforms\n\n\nimport\n \ndeepspeed\n\n\nfrom\n \ntorch.utils.data\n \nimport\n \nDataLoader\n\n\n\nclass\n \nSimpleCNN\n(\nnn\n.\nModule\n):\n\n    \ndef\n \n__init__\n(\nself\n):\n\n        \nsuper\n(\nSimpleCNN\n,\n \nself\n).\n__init__\n()\n\n        \nself\n.\nconv1\n \n=\n \nnn\n.\nConv2d\n(\n1\n,\n \n32\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nconv2\n \n=\n \nnn\n.\nConv2d\n(\n32\n,\n \n64\n,\n \nkernel_size\n=\n3\n,\n \nstride\n=\n1\n,\n \npadding\n=\n1\n)\n\n        \nself\n.\nfc1\n \n=\n \nnn\n.\nLinear\n(\n64\n \n*\n \n28\n \n*\n \n28\n,\n \n128\n)\n\n        \nself\n.\nfc2\n \n=\n \nnn\n.\nLinear\n(\n128\n,\n \n10\n)\n\n    \n    \ndef\n \nforward\n(\nself\n,\n \nx\n):\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv1\n(\nx\n))\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nconv2\n(\nx\n))\n\n        \nx\n \n=\n \nx\n.\nview\n(\n-\n1\n,\n \n64\n \n*\n \n28\n \n*\n \n28\n)\n\n        \nx\n \n=\n \nnn\n.\nfunctional\n.\nrelu\n(\nself\n.\nfc1\n(\nx\n))\n\n        \nx\n \n=\n \nself\n.\nfc2\n(\nx\n)\n\n        \nreturn\n \nx\n\n\n\ndef\n \nmain\n():\n\n    \n# Define transformations and dataset\n\n    \ntransform\n \n=\n \ntransforms\n.\nCompose\n([\n\n        \ntransforms\n.\nToTensor\n(),\n\n        \ntransforms\n.\nNormalize\n((\n0.1307\n,),\n \n(\n0.3081\n,))\n\n    \n])\n\n    \n    \ntrain_dataset\n \n=\n \ndatasets\n.\nMNIST\n(\nroot\n=\n'./data'\n,\n \ntrain\n=\nTrue\n,\n \ndownload\n=\nTrue\n,\n \ntransform\n=\ntransform\n)\n\n    \ntrain_loader\n \n=\n \nDataLoader\n(\ndataset\n=\ntrain_dataset\n,\n \nbatch_size\n=\n64\n,\n \nshuffle\n=\nTrue\n)\n\n    \n    \n# Initialize model\n\n    \nmodel\n \n=\n \nSimpleCNN\n()\n\n\n    \n# Define optimizer\n\n    \noptimizer\n \n=\n \noptim\n.\nAdam\n(\nmodel\n.\nparameters\n(),\n \nlr\n=\n0.001\n)\n\n    \n    \n# DeepSpeed configuration\n\n    \ndeepspeed_config\n \n=\n \n{\n\n        \n\"train_batch_size\"\n:\n \n64\n,\n\n        \n\"gradient_accumulation_steps\"\n:\n \n1\n,\n\n        \n\"optimizer\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"Adam\"\n,\n\n            \n\"params\"\n:\n \n{\n\n                \n\"lr\"\n:\n \n0.001\n,\n\n                \n\"betas\"\n:\n \n[\n0.9\n,\n \n0.999\n],\n\n                \n\"eps\"\n:\n \n1e-8\n\n            \n}\n\n        \n},\n\n        \n\"fp16\"\n:\n \n{\n\n            \n\"enabled\"\n:\n \nTrue\n\n        \n},\n\n        \n\"zero_optimization\"\n:\n \n{\n\n            \n\"stage\"\n:\n \n2\n,\n  \n# Use ZeRO Stage 2 for gradient sharding\n\n            \n\"allgather_bucket_size\"\n:\n \n5e8\n,\n\n            \n\"reduce_bucket_size\"\n:\n \n5e8\n\n        \n}\n\n    \n}\n\n    \n    \n# Initialize DeepSpeed\n\n    \nmodel_engine\n,\n \noptimizer\n,\n \n_\n,\n \n_\n \n=\n \ndeepspeed\n.\ninitialize\n(\nmodel\n=\nmodel\n,\n \noptimizer\n=\noptimizer\n,\n \nconfig\n=\ndeepspeed_config\n)\n\n    \n    \ncriterion\n \n=\n \nnn\n.\nCrossEntropyLoss\n()\n\n\n    \n# Training loop\n\n    \nmodel_engine\n.\ntrain\n()\n\n    \nfor\n \nepoch\n \nin\n \nrange\n(\n5\n):\n\n        \nrunning_loss\n \n=\n \n0.0\n\n        \nfor\n \ni\n,\n \n(\ninputs\n,\n \nlabels\n)\n \nin\n \nenumerate\n(\ntrain_loader\n):\n\n            \ninputs\n,\n \nlabels\n \n=\n \ninputs\n.\nto\n(\nmodel_engine\n.\nlocal_rank\n),\n \nlabels\n.\nto\n(\nmodel_engine\n.\nlocal_rank\n)\n\n            \n            \noptimizer\n.\nzero_grad\n()\n\n            \noutputs\n \n=\n \nmodel_engine\n(\ninputs\n)\n\n            \nloss\n \n=\n \ncriterion\n(\noutputs\n,\n \nlabels\n)\n\n            \nmodel_engine\n.\nbackward\n(\nloss\n)\n\n            \nmodel_engine\n.\nstep\n()\n\n            \n            \nrunning_loss\n \n+=\n \nloss\n.\nitem\n()\n\n            \nif\n \ni\n \n%\n \n100\n \n==\n \n99\n:\n\n                \nprint\n(\nf\n'Epoch [\n{\nepoch\n \n+\n \n1\n}\n, \n{\ni\n \n+\n \n1\n}\n] loss: \n{\nrunning_loss\n \n/\n \n100\n:\n.\n3\nf\n}\n'\n)\n\n                \nrunning_loss\n \n=\n \n0.0\n\n    \n    \nprint\n(\n'Training finished.'\n)\n\n\n\nif\n \n__name__\n \n==\n \n\"__main__\"\n:\n\n    \nmain\n()\n\n\n\n\n\nExplanation of the Code\n\n\n\n\n  \nModel Definition:\n A simple CNN model is defined.\n\n  \nData Loading:\n The MNIST dataset is loaded and transformed.\n\n  \nOptimizer Definition:\n An Adam optimizer is defined.\n\n  \nDeepSpeed Configuration:\n A configuration dictionary for DeepSpeed is defined, specifying batch size, optimizer settings, mixed precision (fp16), and ZeRO optimization stage.\n\n  \nInitialize DeepSpeed:\n The model and optimizer are initialized with DeepSpeed, which wraps the model to handle optimization.\n\n  \nTraining Loop:\n The training loop runs similarly to standard PyTorch but uses \nmodel_engine.backward(loss)\n and \nmodel_engine.step()\n to leverage DeepSpeed’s optimization.\n\n\n\n\n\nComparison of ZeRO Stages\n\n\n\n\n  \nZeRO Stage 1:\n Optimizer state partitioning, suitable for moderate memory savings.\n\n  \nZeRO Stage 2:\n Adds gradient partitioning to further reduce memory usage.\n\n  \nZeRO Stage 3:\n Full parameter partitioning, enabling the training of the largest models.\n\n  \nDeepSpeed ZeRO is a powerful tool for scaling deep learning models efficiently, providing advanced memory optimizations and performance enhancements to facilitate large-scale model training.\n\n\n\n\n\nComparative Analysis: DP, DDP, FSDP, DeepSpeed, and DeepSpeed ZeRO\n\n\n\n\n  \nHere’s a comparative analysis of Data Parallel (DP), Distributed Data Parallel (DDP), Fully Sharded Data Parallel (FSDP), DeepSpeed, and DeepSpeed ZeRO, highlighting their differences, use cases, and key features:\n\n\n\n\n\nData Parallel (DP)\n\n\n\nOverview\n\n\n\n  \nMechanism:\n Splits the input data across multiple GPUs and replicates the model on each GPU.\n\n  \nSynchronization:\n Gradients are averaged across GPUs after each backward pass.\n\n  \nScalability:\n Limited scalability due to single-process bottleneck and high communication overhead.\n\n  \nEase of Use:\n Simple to implement using \ntorch.nn.DataParallel\n.\n\n\n\n\n\nPros\n\n\n\n  \nEasy to set up and use.\n\n  \nSuitable for small to medium-sized models and datasets.\n\n\n\n\n\nCons\n\n\n\n  \nInefficient memory usage as each GPU holds a full copy of the model.\n\n  \nNot scalable for large models or large-scale distributed training.\n\n\n\n\n\nCode Sample\n\n\nimport\n \ntorch.nn\n \nas\n \nnn\n\n\nimport\n \ntorch\n\n\n\nmodel\n \n=\n \nnn\n.\nDataParallel\n(\nmodel\n)\n  \n# Wrap the model for Data Parallel\n\nmodel\n \n=\n \nmodel\n.\nto\n(\ndevice\n)\n\n\n\n\n\nDistributed Data Parallel (DDP)\n\n\n\nOverview\n\n\n\n  \nMechanism:\n Each GPU runs a separate process with its own replica of the model.\n\n  \nSynchronization:\n Uses an all-reduce operation to synchronize gradients across GPUs.\n\n  \nScalability:\n More scalable than DP as it avoids the GIL bottleneck by using multiple processes.\n\n  \nEase of Use:\n More complex setup than DP but provides better performance.\n\n\n\n\n\nPros\n\n\n\n  \nBetter scalability and performance compared to DP.\n\n  \nMore efficient GPU utilization.\n\n\n\n\n\nCons\n\n\n\n  \nSlightly more complex to implement due to process management.\n\n  \nStill requires each GPU to hold a full copy of the model.\n\n\n\n\n\nCode Sample\n\n\nimport\n \ntorch.distributed\n \nas\n \ndist\n\n\nimport\n \ntorch.multiprocessing\n \nas\n \nmp\n\n\nfrom\n \ntorch.nn.parallel\n \nimport\n \nDistributedDataParallel\n \nas\n \nDDP\n\n\n\n# Initialize the process group\n\ndist\n.\ninit_process_group\n(\n\"nccl\"\n,\n \nrank\n=\nrank\n,\n \nworld_size\n=\nworld_size\n)\n\n\n\nmodel\n \n=\n \nDDP\n(\nmodel\n.\nto\n(\nrank\n),\n \ndevice_ids\n=\n[\nrank\n])\n\n\n\n\n\nFully Sharded Data Parallel (FSDP)\n\n\n\nOverview\n\n\n\n  \nMechanism:\n Shards model parameters, gradients, and optimizer states across GPUs.\n\n  \nSynchronization:\n Parameters are gathered and reduced as needed during forward and backward passes.\n\n  \nScalability:\n Highly scalable, suitable for very large models.\n\n  \nEase of Use:\n Requires more complex setup and understanding of model sharding.\n\n\n\n\n\nPros\n\n\n\n  \nSignificantly reduces memory overhead by sharding model states.\n\n  \nAllows training of very large models that don’t fit in memory with traditional DP or DDP.\n\n\n\n\n\nCons\n\n\n\n  \nMore complex to set up and debug.\n\n  \nCan introduce additional communication overhead depending on model size and shard granularity.\n\n\n\n\n\nCode Sample\n\n\nfrom\n \ntorch.distributed.fsdp\n \nimport\n \nFullyShardedDataParallel\n \nas\n \nFSDP\n\n\n\nmodel\n \n=\n \nFSDP\n(\nmodel\n)\n\n\n\n\n\nDeepSpeed\n\n\n\nOverview\n\n\n\n  \nMechanism:\n Combines multiple optimization techniques, including ZeRO (Zero Redundancy Optimizer) which shards model states across GPUs.\n\n  \nSynchronization:\n Uses advanced techniques to optimize communication and memory usage.\n\n  \nScalability:\n Extremely scalable, designed for training trillion-parameter models.\n\n  \nEase of Use:\n Requires integration with DeepSpeed library and configuration.\n\n\n\n\n\nPros\n\n\n\n  \nState-of-the-art memory and performance optimizations.\n\n  \nSupports very large models with efficient memory usage and compute.\n\n\n\n\n\nCons\n\n\n\n  \nMore complex integration compared to basic DP or DDP.\n\n  \nRequires careful tuning and configuration for optimal performance.\n\n\n\n\n\nCode Sample\n\n\nimport\n \ndeepspeed\n\n\n\ndeepspeed_config\n \n=\n \n{\n\n    \n\"train_batch_size\"\n:\n \n64\n,\n\n    \n\"gradient_accumulation_steps\"\n:\n \n1\n,\n\n    \n\"optimizer\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"Adam\"\n,\n\n        \n\"params\"\n:\n \n{\n\n            \n\"lr\"\n:\n \n0.001\n\n        \n}\n\n    \n},\n\n    \n\"fp16\"\n:\n \n{\n\n        \n\"enabled\"\n:\n \nTrue\n\n    \n},\n\n    \n\"zero_optimization\"\n:\n \n{\n\n        \n\"stage\"\n:\n \n2\n  \n# Use ZeRO Stage 2 for gradient sharding\n\n    \n}\n\n\n}\n\n\n\nmodel\n,\n \noptimizer\n,\n \n_\n,\n \n_\n \n=\n \ndeepspeed\n.\ninitialize\n(\nmodel\n=\nmodel\n,\n \noptimizer\n=\noptimizer\n,\n \nconfig\n=\ndeepspeed_config\n)\n\n\n\n\n\nDeepSpeed ZeRO\n\n\n\nOverview\n\n\n\n  \nMechanism:\n Part of DeepSpeed, specifically focused on Zero Redundancy Optimizer to partition model states (optimizer states, gradients, parameters) across GPUs.\n\n  \nSynchronization:\n Efficient communication strategies to gather and reduce necessary states during forward and backward passes.\n\n  \nScalability:\n Highly scalable, designed for training extremely large models with minimal memory footprint.\n\n  \nEase of Use:\n More complex setup but offers significant memory and performance benefits.\n\n\n\n\n\nPros\n\n\n\n  \nOptimizes memory usage by partitioning model states across GPUs.\n\n  \nSupports training very large models that wouldn’t fit in memory otherwise.\n\n  \nThree stages of optimization for varying degrees of memory savings.\n\n\n\n\n\nCons\n\n\n\n  \nMore complex to configure and tune.\n\n  \nRequires understanding of the ZeRO stages for optimal use.\n\n\n\n\n\nCode Sample\n\n\nimport\n \ndeepspeed\n\n\n\ndeepspeed_config\n \n=\n \n{\n\n    \n\"train_batch_size\"\n:\n \n64\n,\n\n    \n\"gradient_accumulation_steps\"\n:\n \n1\n,\n\n    \n\"optimizer\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"Adam\"\n,\n\n        \n\"params\"\n:\n \n{\n\n            \n\"lr\"\n:\n \n0.001\n,\n\n            \n\"betas\"\n:\n \n[\n0.9\n,\n \n0.999\n],\n\n            \n\"eps\"\n:\n \n1e-8\n\n        \n}\n\n    \n},\n\n    \n\"fp16\"\n:\n \n{\n\n        \n\"enabled\"\n:\n \nTrue\n\n    \n},\n\n    \n\"zero_optimization\"\n:\n \n{\n\n        \n\"stage\"\n:\n \n2\n,\n  \n# Use ZeRO Stage 2 for gradient sharding\n\n        \n\"allgather_bucket_size\"\n:\n \n5e8\n,\n\n        \n\"reduce_bucket_size\"\n:\n \n5e8\n\n    \n}\n\n\n}\n\n\n\nmodel_engine\n,\n \noptimizer\n,\n \n_\n,\n \n_\n \n=\n \ndeepspeed\n.\ninitialize\n(\nmodel\n=\nmodel\n,\n \noptimizer\n=\noptimizer\n,\n \nconfig\n=\ndeepspeed_config\n)\n\n\n\n\n\nComparative Summary\n\n\n\n\n\n\n \n\n\n\n\nFeature\n\n\nData Parallel (DP)\n\n\nDistributed Data Parallel (DDP)\n\n\nFully Sharded Data Parallel (FSDP)\n\n\nDeepSpeed\n\n\nDeepSpeed ZeRO\n\n\n\n\n\n\n\n\n\n\nModel Replicas\n\n\nFull model on each GPU\n\n\nFull model on each GPU\n\n\nSharded model across GPUs\n\n\nSharded model across GPUs\n\n\nSharded model states\n\n\n\n\n\n\nMemory Usage\n\n\nHigh (full model on each GPU)\n\n\nHigh (full model on each GPU)\n\n\nLow (sharded model)\n\n\nLow (sharded model and states)\n\n\nVery Low (sharded states)\n\n\n\n\n\n\nGradient Synchronization\n\n\nAveraging across GPUs\n\n\nAll-reduce across processes\n\n\nGather and reduce as needed\n\n\nAdvanced optimizations\n\n\nEfficient sharding strategies\n\n\n\n\n\n\nScalability\n\n\nLimited\n\n\nModerate\n\n\nHigh\n\n\nVery High\n\n\nVery High\n\n\n\n\n\n\nEase of Use\n\n\nSimple\n\n\nModerate\n\n\nComplex\n\n\nComplex\n\n\nComplex\n\n\n\n\n\n\nPerformance\n\n\nModerate\n\n\nHigh\n\n\nHigh\n\n\nVery High\n\n\nVery High\n\n\n\n\n\n\n\n\n\n\n\nUse Cases\n\n\n\n\n  \nData Parallel (DP):\n Suitable for small to medium-sized models and datasets when ease of use is a priority.\n\n  \nDistributed Data Parallel (DDP):\n Preferred for scenarios requiring better scalability and efficiency compared to DP.\n\n  \nFully Sharded Data Parallel (FSDP):\n Ideal for training very large models that don’t fit in memory using traditional methods.\n\n  \nDeepSpeed:\n Best for state-of-the-art large-scale model training, offering the most advanced optimizations for memory and performance.\n\n  \nDeepSpeed ZeRO:\n Specifically focused on optimizing memory usage, suitable for extremely large models and resource-constrained environments.\n\n  \nOverall, Choosing the right parallelization strategy depends on the specific requirements of the model, available hardware, and desired scalability.\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledDistributedTrainingParallelism,\n  title   = {Distributed Training Parallelism},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/distributed-training-parallelism/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • SVM Kernel/Polynomial Trick\n\n  \n\n\n  \n\n  \n\n  \nKernel/Polynomial Trick\n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nKernel/Polynomial Trick\n\n\n\n\n  \n\n    \nIn classic mathematical induction, the goal of a classification task is to learn a function \\(y = f(x; \\theta)\\) aka fit a model (or a curve) to the data with a decision boundary that separates the classes (say, approve/reject loan applications).\n\n  \n\n  \n\n    \nThe diagram below from \nPrithvi Da\n summarizes the approaches.\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n    \nFig A: If the data points are linearly separable, it’s simple you can fit a linear model using algorithms like Logistic regression or SVM.\n\n  \n\n  \n\n    \nFig B: What do you do if the data points are NOT linearly separable? of course we can fit a Neural network which in theory can learn any Borel measurable function under the sky. But we have a simple solution especially if you have a smaller tabular dataset.\n\n  \n\n  \n\n    \nFig C: Shows you can simply magically transform a line to a parabola and then learn a function that linearly separates the data points. This idea of applying a non-linear transformation using a special function called a “kernel” to convert linear input space (\\(y= mx+c\\), 1st-degree polynomial) to a high degree polynomial (\\(x^2 = 4ay\\), 2nd-degree polynomial) is the gist of the kernel trick. The bottom-line is that learning a linear model in the higher-dimensional space is equivalent to learning a non-linear model in the original lower-dimensional input space.\n\n  \n\n  \n\n    \nTL;DR - It pays in spades to bend the problem, literally sometimes.\n\n  \n\n\n\n\n\nReferences\n\n\n\n\n  \nPrithvi Da on LinkedIn\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledKernelTrick,\n  title   = {Kernel Trick},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/svm-kernel-trick/"},
{"page_content": "aman.ai\n • \nprivacy policy\n\n\n        \nLast updated July 22, 2024\n\n        \n        \nOVERVIEW\n\n        \n        \nThis privacy notice for aman.ai (\"\nwe\n\", \"\nus\n\", or \"\nour\n\"), describes how and why we might collect, store, use, and/or share (\"\nprocess\n\") your information when you use our services (\"\nServices\n\"), such as when you:\n\n        \n\n            \nVisit our website at \nhttps://aman.ai\n, or any website of ours that links to this privacy notice.\n\n            \nEngage with us in other related ways, including any sales, marketing, or events.\n\n        \n\n\n        \nQuestions or concerns?\n Reading this privacy notice will help you understand your privacy rights and choices. If you do not agree with our policies and practices, please do not use our Services. If you still have any questions or concerns, please contact us at \nprivacy [at] aman.ai\n.\n\n\n        \nSUMMARY OF KEY POINTS\n\n        \nThis summary provides key points from our privacy notice, but you can find out more details about any of these topics by clicking the link following each key point or by using our \ntable of contents\n below to find the section you are looking for.\n\n        \n\n            \nWhat personal information do we process?\n When you visit, use, or navigate our Services, we may process personal information depending on how you interact with us and the Services, the choices you make, and the products and features you use. Learn more about \npersonal information you disclose to us\n.\n\n            \nDo we process any sensitive personal information?\n We do not process sensitive personal information.\n\n            \nDo we receive any information from third parties?\n We do not receive any information from third parties.\n\n            \nHow do we process your information?\n We process your information to provide, improve, and administer our Services, communicate with you, for security and fraud prevention, and to comply with law. We may also process your information for other purposes with your consent. Learn more about \nhow we process your information\n.\n\n            \nIn what situations and with which parties do we share personal information?\n We may share information in specific situations and with specific third parties. Learn more about \nwhen and with whom we share your personal information\n.\n\n            \nWhat are your rights?\n Depending on where you are located geographically, the applicable privacy law may mean you have certain rights regarding your personal information. Learn more about \nyour privacy rights\n.\n\n            \nHow do you exercise your rights?\n The easiest way to exercise your rights is by submitting a \ndata subject access request\n, or by contacting us. We will consider and act upon any request in accordance with applicable data protection laws.\n\n        \n\n        \nWant to learn more about what we do with any information we collect? \nReview the privacy notice in full\n.\n\n\n        \nTABLE OF CONTENTS\n\n        \n\n            \n1. WHAT INFORMATION DO WE COLLECT?\n\n            \n2. HOW DO WE PROCESS YOUR INFORMATION?\n\n            \n3. WHAT LEGAL BASES DO WE RELY ON TO PROCESS YOUR PERSONAL INFORMATION?\n\n            \n4. WHEN AND WITH WHOM DO WE SHARE YOUR PERSONAL INFORMATION?\n\n            \n5. WHAT IS OUR STANCE ON THIRD-PARTY WEBSITES?\n\n            \n6. HOW LONG DO WE KEEP YOUR INFORMATION?\n\n            \n7. DO WE COLLECT INFORMATION FROM MINORS?\n\n            \n8. WHAT ARE YOUR PRIVACY RIGHTS?\n\n            \n9. CONTROLS FOR DO-NOT-TRACK FEATURES\n\n            \n10. DO UNITED STATES RESIDENTS HAVE SPECIFIC PRIVACY RIGHTS?\n\n        \n\n\n        \n1. WHAT INFORMATION DO WE COLLECT?\n\n        \nPersonal information you disclose to us\n\n        \nIn Short:\n We collect personal information that you provide to us.\n\n        \nWe collect personal information that you voluntarily provide to us when you express an interest in obtaining information about us or our products and Services, when you participate in activities on the Services, or otherwise when you contact us.\n\n        \nSensitive Information.\n\n        \nWe do not process sensitive information.\n\n        \nAll personal information that you provide to us must be true, complete, and accurate, and you must notify us of any changes to such personal information.\n\n\n        \n2. HOW DO WE PROCESS YOUR INFORMATION?\n\n        \nIn Short:\n We process your information to provide, improve, and administer our Services, communicate with you, for security and fraud prevention, and to comply with law. We may also process your information for other purposes with your consent.\n\n        \nWe process your personal information for a variety of reasons, depending on how you interact with our Services, including:\n\n        \n\n            \nTo save or protect an individual's vital interest.\n We may process your information when necessary to save or protect an individual’s vital interest, such as to prevent harm.\n\n        \n\n\n        \n3. WHAT LEGAL BASES DO WE RELY ON TO PROCESS YOUR PERSONAL INFORMATION?\n\n        \nIn Short:\n We only process your personal information when we believe it is necessary and we have a valid legal reason (i.e., legal basis) to do so under applicable law, like with your consent, to comply with laws, to provide you with services to enter into or fulfill our contractual obligations, to protect your rights, or to fulfill our legitimate business interests.\n\n        \nIf you are located in the EU or UK, this section applies to you.\n\n        \nThe General Data Protection Regulation (GDPR) and UK GDPR require us to explain the valid legal bases we rely on in order to process your personal information. As such, we may rely on the following legal bases to process your personal information:\n\n        \n\n            \nConsent.\n We may process your information if you have given us permission (i.e., consent) to use your personal information for a specific purpose. You can withdraw your consent at any time. Learn more about \nwithdrawing your consent\n.\n\n            \nLegal Obligations.\n We may process your information where we believe it is necessary for compliance with our legal obligations, such as to cooperate with a law enforcement body or regulatory agency, exercise or defend our legal rights, or disclose your information as evidence in litigation in which we are involved.\n\n            \nVital Interests.\n We may process your information where we believe it is necessary to protect your vital interests or the vital interests of a third party, such as situations involving potential threats to the safety of any person.\n\n        \n\n        \nIf you are located in Canada, this section applies to you.\n\n        \nWe may process your information if you have given us specific permission (i.e., express consent) to use your personal information for a specific purpose, or in situations where your permission can be inferred (i.e., implied consent). You can \nwithdraw your consent\n\n\n at any time.\n\n        \nIn some exceptional cases, we may be legally permitted under applicable law to process your information without your consent, including, for example:\n\n        \n\n            \nIf collection is clearly in the interests of an individual and consent cannot be obtained in a timely way.\n\n            \nFor investigations and fraud detection and prevention.\n\n            \nFor business transactions provided certain conditions are met.\n\n            \nIf it is contained in a witness statement and the collection is necessary to assess, process, or settle an insurance claim.\n\n            \nFor identifying injured, ill, or deceased persons and communicating with next of kin.\n\n            \nIf we have reasonable grounds to believe an individual has been, is, or may be victim of financial abuse.\n\n            \nIf it is reasonable to expect collection and use with consent would compromise the availability or the accuracy of the information and the collection is reasonable for purposes related to investigating a breach of an agreement or a contravention of the laws of Canada or a province.\n\n            \nIf disclosure is required to comply with a subpoena, warrant, court order, or rules of the court relating to the production of records.\n\n            \nIf it was produced by an individual in the course of their employment, business, or profession and the collection is consistent with the purposes for which the information was produced.\n\n            \nIf the collection is solely for journalistic, artistic, or literary purposes.\n\n            \nIf the information is publicly available and is specified by the regulations.\n\n        \n\n\n        \n4. WHEN AND WITH WHOM DO WE SHARE YOUR PERSONAL INFORMATION?\n\n        \nIn Short:\n We may share information in specific situations described in this section and/or with the following third parties.\n\n        \nWe may need to share your personal information in the following situations:\n\n        \n\n            \nBusiness Transfers.\n We may share or transfer your information in connection with, or during negotiations of, any merger, sale of company assets, financing, or acquisition of all or a portion of our business to another company.\n\n        \n\n\n        \n5. WHAT IS OUR STANCE ON THIRD-PARTY WEBSITES?\n\n        \nIn Short:\n We are not responsible for the safety of any information that you share with third parties that we may link to or who advertise on our Services, but are not affiliated with, our Services.\n\n        \nThe Services may link to third-party websites, online services, or mobile applications and/or contain advertisements from third parties that are not affiliated with us and which may link to other websites, services, or applications. Accordingly, we do not make any guarantee regarding any such third parties, and we will not be liable for any loss or damage caused by the use of such third-party websites, services, or applications. The inclusion of a link towards a third-party website, service, or application does not imply an endorsement by us. We cannot guarantee the safety and privacy of data you provide to any third parties. Any data collected by third parties is not covered by this privacy notice. We are not responsible for the content or privacy and security practices and policies of any third parties, including other websites, services, or applications that may be linked to or from the Services. You should review the policies of such third parties and contact them directly to respond to your questions.\n\n\n        \n6. HOW LONG DO WE KEEP YOUR INFORMATION?\n\n        \nIn Short:\n We keep your information for as long as necessary to fulfill the purposes outlined in this privacy notice unless otherwise required by law.\n\n        \nWe will only keep your personal information for as long as it is necessary for the purposes set out in this privacy notice, unless a longer retention period is required or permitted by law (such as tax, accounting, or other legal requirements).\n\n        \nWhen we have no ongoing legitimate business need to process your personal information, we will either delete or anonymize such information, or, if this is not possible (for example, because your personal information has been stored in backup archives), then we will securely store your personal information and isolate it from any further processing until deletion is possible.\n\n\n        \n7. DO WE COLLECT INFORMATION FROM MINORS?\n\n        \nIn Short:\n We do not knowingly collect data from or market to children under 18 years of age.\n\n        \nWe do not knowingly solicit data from or market to children under 18 years of age. By using the Services, you represent that you are at least 18 or that you are the parent or guardian of such a minor and consent to such minor dependent’s use of the Services. If we learn that personal information from users less than 18 years of age has been collected, we will deactivate the account and take reasonable measures to promptly delete such data from our records. If you become aware of any data we may have collected from children under age 18, please contact us at \nprivacy [at] aman.ai\n.\n\n\n        \n8. WHAT ARE YOUR PRIVACY RIGHTS?\n\n        \nIn Short:\n In some regions, such as the European Economic Area (EEA), United Kingdom (UK), and Canada, you have rights that allow you greater access to and control over your personal information. You may review, change, or terminate your account at any time.\n\n        \nIn some regions (like the EEA, UK, and Canada), you have certain rights under applicable data protection laws. These may include the right (i) to request access and obtain a copy of your personal information, (ii) to request rectification or erasure; (iii) to restrict the processing of your personal information; and (iv) if applicable, to data portability. In certain circumstances, you may also have the right to object to the processing of your personal information. You can make such a request by contacting us by using the contact details provided in the section \"How can you contact us about this notice?\" below.\n\n        \nWe will consider and act upon any request in accordance with applicable data protection laws.\n\n        \nIf you are located in the EEA or UK and you believe we are unlawfully processing your personal information, you also have the right to complain to your \nMember State data protection authority\n or \nUK data protection authority\n.\n\n        \nIf you are located in Switzerland, you may contact the \nFederal Data Protection and Information Commissioner\n.\n\n\n        \nWithdrawing your consent:\n\n        \nIf we are relying on your consent to process your personal information, which may be express and/or implied consent depending on the applicable law, you have the right to withdraw your consent at any time. You can withdraw your consent at any time by contacting us by using the contact details provided in the section \"How can you contact us about this notice?\" below.\n\n        \nHowever, please note that this will not affect the lawfulness of the processing before its withdrawal nor, when applicable law allows, will it affect the processing of your personal information conducted in reliance on lawful processing grounds other than consent.\n\n        \nIf you have questions or comments about your privacy rights, you may email us at \nprivacy [at] aman.ai\n.\n\n\n        \n9. CONTROLS FOR DO-NOT-TRACK FEATURES\n\n        \nMost web browsers and some mobile operating systems and mobile applications include a Do-Not-Track (\"DNT\") feature or setting you can activate to signal your privacy preference not to have data about your online browsing activities monitored and collected. At this stage no uniform technology standard for recognizing and implementing DNT signals has been finalized. As such, we do not currently respond to DNT browser signals or any other mechanism that automatically communicates your choice not to be tracked online. If a standard for online tracking is adopted that we must follow in the future, we will inform you about that practice in a revised version of this privacy notice.\n\n\n        \n10. DO UNITED STATES RESIDENTS HAVE SPECIFIC PRIVACY RIGHTS?\n\n        \nIn Short:\n\n        \nIf you are a resident of California, Colorado, Connecticut, or Virginia, you are granted specific rights regarding access to your personal information.\n\n\n        \nWhat categories of personal information do we collect?\n\n        \nWe have collected the following categories of personal information in the past twelve (12) months:\n\n        \n\n            \n\n                \nCategory\n\n                \nExamples\n\n                \nCollected\n\n            \n\n            \n\n                \nA. Identifiers\n\n                \nContact details, such as real name, alias, postal address, telephone or mobile contact number, unique personal identifier, online identifier, Internet Protocol address, email address, and account name\n\n                \nNO\n\n            \n\n            \n\n                \nB. Protected classification characteristics under state or federal law\n\n                \nGender and date of birth\n\n                \nNO\n\n            \n\n            \n\n                \nC. Commercial information\n\n                \nTransaction information, purchase history, financial details, and payment information\n\n                \nNO\n\n            \n\n            \n\n                \nD. Biometric information\n\n                \nFingerprints and voiceprints\n\n                \nNO\n\n            \n\n            \n\n                \nE. Internet or other similar network activity\n\n                \nBrowsing history, search history, online behavior, interest data, and interactions with our and other websites, applications, systems, and advertisements\n\n                \nNO\n\n            \n\n            \n\n                \nF. Geolocation data\n\n                \nDevice location\n\n                \nNO\n\n            \n\n            \n\n                \nG. Audio, electronic, visual, thermal, olfactory, or similar information\n\n                \nImages and audio, video or call recordings created in connection with our business activities\n\n                \nNO\n\n            \n\n            \n\n                \nH. Professional or employment-related information\n\n                \nBusiness contact details in order to provide you our Services at a business level or job title, work history, and professional qualifications if you apply for a job with us\n\n                \nNO\n\n            \n\n            \n\n                \nI. Education Information\n\n                \nStudent records and directory information\n\n                \nNO\n\n            \n\n            \n\n                \nJ. Inferences drawn from collected personal information\n\n                \nInferences drawn from any of the collected personal information listed above to create a profile or summary about, for example, an individual’s preferences and characteristics\n\n                \nNO\n\n            \n\n            \n\n                \nK. Sensitive personal information\n\n                \n\n                \nNO\n\n            \n\n            \n\n                \nL. Personal information as defined in the California Customer Records statute\n\n                \nName, contact information, education, employment, employment history, and financial information\n\n                \nNO\n\n            \n\n        \n\n\n        \nAdditional Information\n\n        \nWe may also collect other personal information outside of these categories through instances where you interact with us in person, online, or by phone or mail in the context of:\n\n        \n\n            \nReceiving help through our customer support channels\n\n            \nParticipation in customer surveys or contests\n\n            \nFacilitation in the delivery of our Services and to respond to your inquiries\n\n        \n\n\n        \nHow do we use and share your personal information?\n\n        \nLearn about how we use your personal information in the section, \nHOW DO WE PROCESS YOUR INFORMATION?\n\n        \nMore information about our data collection and sharing practices can be found in this privacy notice.\n\n\n        \nWill your information be shared with anyone else?\n\n        \nWe may disclose your personal information with our service providers pursuant to a written contract between us and each service provider. Learn more about who we disclose personal information to in the section, \nWHEN AND WITH WHOM DO WE SHARE YOUR PERSONAL INFORMATION?\n\n        \nWe may use your personal information for our own business purposes, such as for undertaking internal research for technological development and demonstration. This is not considered to be \"selling\" of your personal information.\n\n        \nWe have not disclosed, sold, or shared any personal information to third parties for a business or commercial purpose in the preceding twelve (12) months. We will not sell or share personal information in the future belonging to website visitors, users, and other consumers.\n\n\n        \nCalifornia Residents\n\n        \nCalifornia Civil Code Section 1798.83, also known as the \"Shine The Light\" law, permits our users who are California residents to request and obtain from us, once a year and free of charge, information about categories of personal information (if any) we disclosed to third parties for direct marketing purposes and the names and addresses of all third parties with which we shared personal information in the immediately preceding calendar year. If you are a California resident and would like to make such a request, please submit your request in writing to us using the contact information provided below.\n\n        \nIf you are under 18 years of age, reside in California, and have a registered account with the Services, you have the right to request removal of unwanted data that you publicly post on the Services. To request removal of such data, please contact us using the contact information provided below and include the email address associated with your account and a statement that you reside in California. We will make sure the data is not publicly displayed on the Services, but please be aware that the data may not be completely or comprehensively removed from all our systems (e.g., backups, etc.).\n\n\n        \nCCPA Privacy Notice\n\n        \nThis section applies only to California residents. Under the California Consumer Privacy Act (CCPA), you have the rights listed below.\n\n        \nThe California Code of Regulations defines a \"resident\" as:\n\n        \n\n            \n(1) every individual who is in the State of California for other than a temporary or transitory purpose and\n\n            \n(2) every individual who is domiciled in the State of California who is outside of the State of California for a temporary or transitory purpose\n\n        \n\n        \nAll other individuals are defined as \"non-residents.\"\n\n        \nIf this definition of \"resident\" applies to you, we must adhere to certain rights and obligations regarding your personal information.\n\n\n        \nYour rights with respect to your personal data\n\n        \nRight to request deletion of the data — Request to delete\n\n        \nYou can ask for the deletion of your personal information. If you ask us to delete your personal information, we will respect your request and delete your personal information, subject to certain exceptions provided by law, such as (but not limited to) the exercise by another consumer of his or her right to free speech, our compliance requirements resulting from a legal obligation, or any processing that may be required to protect against illegal activities.\n\n\n        \nRight to be informed — Request to know\n\n        \nDepending on the circumstances, you have a right to know:\n\n        \n\n            \nwhether we collect and use your personal information;\n\n            \nthe categories of personal information that we collect;\n\n            \nthe purposes for which the collected personal information is used;\n\n            \nwhether we sell or share personal information to third parties;\n\n            \nthe categories of personal information that we sold, shared, or disclosed for a business purpose;\n\n            \nthe categories of third parties to whom the personal information was sold, shared, or disclosed for a business purpose;\n\n            \nthe business or commercial purpose for collecting, selling, or sharing personal information; and\n\n            \nthe specific pieces of personal information we collected about you.\n\n        \n\n        \nIn accordance with applicable law, we are not obligated to provide or delete consumer information that is de-identified in response to a consumer request or to re-identify individual data to verify a consumer request.\n\n\n        \nRight to Non-Discrimination for the Exercise of a Consumer’s Privacy Rights\n\n        \nWe will not discriminate against you if you exercise your privacy rights.\n\n\n        \nRight to Limit Use and Disclosure of Sensitive Personal Information\n\n        \nWe do not process consumer's sensitive personal information.\n\n\n        \nVerification process\n\n        \nUpon receiving your request, we will need to verify your identity to determine you are the same person about whom we have the information in our system. These verification efforts require us to ask you to provide information so that we can match it with information you have previously provided us. For instance, depending on the type of request you submit, we may ask you to provide certain information so that we can match the information you provide with the information we already have on file, or we may contact you through a communication method (e.g., phone or email) that you have previously provided to us. We may also use other verification methods as the circumstances dictate.\n\n        \nWe will only use personal information provided in your request to verify your identity or authority to make the request. To the extent possible, we will avoid requesting additional information from you for the purposes of verification. However, if we cannot verify your identity from the information already maintained by us, we may request that you provide additional information for the purposes of verifying your identity and for security or fraud-prevention purposes. We will delete such additionally provided information as soon as we finish verifying you.\n\n\n        \nOther privacy rights\n\n        \n\n            \nYou may object to the processing of your personal information.\n\n            \nYou may request correction of your personal data if it is incorrect or no longer relevant, or ask to restrict the processing of the information.\n\n            \nYou can designate an authorized agent to make a request under the CCPA on your behalf. We may deny a request from an authorized agent that does not submit proof that they have been validly authorized to act on your behalf in accordance with the CCPA.\n\n            \nYou may request to opt out from future selling or sharing of your personal information to third parties. Upon receiving an opt-out request, we will act upon the request as soon as feasibly possible, but no later than fifteen (15) days from the date of the request submission.\n\n        \n\n\n        \nColorado Residents\n\n        \nThis section applies only to Colorado residents. Under the Colorado Privacy Act (CPA), you have the rights listed below. However, these rights are not absolute, and in certain cases we may decline your request as permitted by law.\n\n        \n\n            \nRight to be informed whether or not we are processing your personal data\n\n            \nRight to access your personal data\n\n            \nRight to correct inaccuracies in your personal data\n\n            \nRight to request deletion of your personal data\n\n            \nRight to obtain a copy of the personal data you previously shared with us\n\n            \nRight to opt out of the processing of your personal data if it is used for targeted advertising, the sale of personal data, or profiling in furtherance of decisions that produce legal or similarly significant effects (\"profiling\")\n\n        \n\n        \nTo submit a request to exercise these rights described above, please email \nprivacy [at] aman.ai\n.\n\n        \nIf we decline to take action regarding your request and you wish to appeal our decision, please email us at \nprivacy [at] aman.ai\n. Within forty-five (45) days of receipt of an appeal, we will inform you in writing of any action taken or not taken in response to the appeal, including a written explanation of the reasons for the decisions.\n\n\n        \nConnecticut Residents\n\n        \nThis section applies only to Connecticut residents. Under the Connecticut Data Privacy Act (CTDPA), you have the rights listed below. However, these rights are not absolute, and in certain cases we may decline your request as permitted by law.\n\n        \n\n            \nRight to be informed whether or not we are processing your personal data\n\n            \nRight to access your personal data\n\n            \nRight to correct inaccuracies in your personal data\n\n            \nRight to request deletion of your personal data\n\n            \nRight to obtain a copy of the personal data you previously shared with us\n\n            \nRight to opt out of the processing of your personal data if it is used for targeted advertising, the sale of personal data, or profiling in furtherance of decisions that produce legal or similarly significant effects (\"profiling\")\n\n        \n\n        \nTo submit a request to exercise these rights described above, please email \nprivacy [at] aman.ai\n.\n\n        \nIf we decline to take action regarding your request and you wish to appeal our decision, please email us at \nprivacy [at] aman.ai\n. Within sixty (60) days of receipt of an appeal, we will inform you in writing of any action taken or not taken in response to the appeal, including a written explanation of the reasons for the decisions.\n\n\n        \nVirginia Residents\n\n        \nUnder the Virginia Consumer Data Protection Act (VCDPA):\n\n        \n\"Consumer\" means a natural person who is a resident of the Commonwealth acting only in an individual or household context. It does not include a natural person acting in a commercial or employment context.\n\n        \n\"Personal data\" means any information that is linked or reasonably linkable to an identified or identifiable natural person. \"Personal data\" does not include de-identified data or publicly available information.\n\n        \n\"Sale of personal data\" means the exchange of personal data for monetary consideration.\n\n        \nIf this definition of \"consumer\" applies to you, we must adhere to certain rights and obligations regarding your personal data.\n\n\n        \nYour rights with respect to your personal data\n\n        \n\n            \nRight to be informed whether or not we are processing your personal data\n\n            \nRight to access your personal data\n\n            \nRight to correct inaccuracies in your personal data\n\n            \nRight to request deletion of your personal data\n\n            \nRight to obtain a copy of the personal data you previously shared with us\n\n            \nRight to opt out of the processing of your personal data if it is used for targeted advertising, the sale of personal data, or profiling in furtherance of decisions that produce legal or similarly significant effects (\"profiling\")\n\n        \n\n        \nTo submit a request to exercise these rights described above, please email \nprivacy [at] aman.ai\n.\n\n        \nIf you are using an authorized agent to exercise your rights, we may deny a request if the authorized agent does not submit proof that they have been validly authorized to act on your behalf.\n\n\n        \nVerification process\n\n        \nWe may request that you provide additional information reasonably necessary to verify you and your consumer's request. If you submit the request through an authorized agent, we may need to collect additional information to verify your identity before processing your request.\n\n        \nUpon receiving your request, we will respond without undue delay, but in all cases, within forty-five (45) days of receipt. The response period may be extended once by forty-five (45) additional days when reasonably necessary. We will inform you of any such extension within the initial 45-day response period, together with the reason for the extension.\n\n\n        \nRight to appeal\n\n        \nIf we decline to take action regarding your request, we will inform you of our decision and reasoning behind it. If you wish to appeal our decision, please email us at \nprivacy [at] aman.ai\n. Within sixty (60) days of receipt of an appeal, we will inform you in writing of any action taken or not taken in response to the appeal, including a written explanation of the reasons for the decisions. If your appeal is denied, you may contact the \nAttorney General to submit a complaint\n.\n\n\n        \n11. DO WE MAKE UPDATES TO THIS NOTICE?\n\n        \nIn Short:\n Yes, we will update this notice as necessary to stay compliant with relevant laws.\n\n        \nWe may update this privacy notice from time to time. The updated version will be indicated by an updated \"Revised\" date and the updated version will be effective as soon as it is accessible. If we make material changes to this privacy notice, we may notify you either by prominently posting a notice of such changes or by directly sending you a notification. We encourage you to review this privacy notice frequently to be informed of how we are protecting your information.\n\n\n        \n12. HOW CAN YOU CONTACT US ABOUT THIS NOTICE?\n\n        \nIf you have questions or comments about this notice, you may email us at \nprivacy [at] aman.ai\n or contact us by post at:\n\n\n        \n13. HOW CAN YOU REVIEW, UPDATE, OR DELETE THE DATA WE COLLECT FROM YOU?\n\n        \nBased on the applicable laws of your country, you may have the right to request access to the personal information we collect from you, change that information, or delete it. To request to review, update, or delete your personal information, please fill out and submit a \ndata subject access request\n.", "source": "https://aman.ai/privacy-policy.htm"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Hypernetworks\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nKey features of Hypernets\n\n  \nHyperDNN\n\n  \nArchitectures of Hypernets\n\n  \nHypernets Pros and Cons\n\n  \nHyperDNNs Pros and Cons\n\n  \nReferences\n\n\n\n\n\nOverview\n\n\n\n  \nHypernetworks (or hypernets) are neural networks that produce the weights for another neural network, which is termed the target network.\n\n  \nThey provide several advantages in deep learning, such as flexibility, adaptability, speedier training, sharing information between tasks, and reducing the model size.\n\n  \nHypernets have been effective across a range of deep learning challenges, from continual learning (where the model needs to learn tasks sequentially without forgetting) to zero-shot learning (where the model predicts classes it hasn’t seen during training) and even complex tasks in NLP and reinforcement learning.\n\n  \nHypernets and HyperDNNs (which we will discuss below as well) are related in spirit to \nPEFT\n but they address the challenge in a distinct manner.\n\n\n\n\n\nKey features of Hypernets\n\n\n\n  \nSoft Weight Sharing: Hypernets can train to produce weights for many DNNs, especially when tasks are related. This is different from traditional weight sharing and allows the transfer of information between tasks.\n\n  \nDynamic Architectures: Hypernets can create a neural network whose structure (like the number of layers) might change during training or when making predictions. This is helpful when you’re uncertain about the best network design beforehand.\n\n  \nData-Adaptive DNNs: Unlike typical DNNs that have set weights once training is done, HyperDNNs can tweak the primary network based on the input data.\n\n  \nUncertainty Quantification: Hypernets can be used to train networks that are aware of their prediction uncertainty. This is done by creating multiple variations of the main network. This feature is essential for critical applications, such as in healthcare.\n\n  \nParameter Efficiency: HyperDNNs may require fewer parameters compared to traditional DNNs, leading to faster training and potentially reduced computational needs.\n\n  \nThe image below, \nsource\n, displays hypernetworks in action for a diffusion model.\n\n\n\n\n\n\nHyperDNN\n\n\n\n  \nAs previously described, hypernets generate the weights for a primary or target network. This means that instead of storing a full set of weights for different tasks or conditions, a hypernet can generate them dynamically.\n\n  \nHowever, HyperDNNs, which are systems composed of hypernets and their target networks, can adapt to different tasks or conditions without necessarily increasing the number of parameters linearly with the number of tasks.\n\n  \nWhile a hypernetwork is like a “controller” that decides the behavior of another network by determining its weights.\n\n  \nA HyperDNN refers to the combined system of a hypernetwork (that generates weights) and its associated target neural network (that utilizes these weights for some task).\n\n  \nThe HyperDNN is the actual deep neural network that performs tasks, but its weights are dynamically generated or modulated by a hypernet. This means the behavior and performance of the HyperDNN can be adaptive and flexible, depending on how the hypernet is conditioned or designed.\n\n  \nA HyperDNN is the full system that includes both the controller (hypernetwork) and the network being controlled (target network).\n\n  \nLet’s look at an example by using the analogy of a radio:\n    \n\n      \nThe hypernetwork is like the tuner dial, determining which frequency (or station) the radio will tune into.\n\n      \nThe HyperDNN is the entire radio system, including the tuner dial and the speaker that produces the sound for the chosen station.\n\n    \n\n  \n\n  \nThe image below, \nsource\n, displays a standard DNN learns weights \\(\\Theta\\) directly through gradient flows, whereas a HyperDNN uses a hypernet with weights \\(\\Phi\\) to dynamically generate the DNN’s weights \\(\\Theta\\) during training.\n\n\n\n\n\n\n\n\nArchitectures of Hypernets\n\n\n\n  \nHypernetworks can be classified based on their architectures into four main types:\n\n\n\n\n\n\n  \nMLPs (Multi-Layer Perceptrons): These use a fully connected structure where every input neuron connects to every output neuron, enabling extensive weight generation by utilizing the full input information.\n\n  \nCNNs (Convolutional Neural Networks): These utilize convolutional layers to identify local and spatial patterns, making them ideal for tasks that involve spatial data like image or video analysis.\n\n  \nRNNs (Recurrent Neural Networks): RNN hypernetworks contain recurrent connections, allowing them to process sequential information. This makes them apt for tasks like natural language processing or time series analysis.\n\n  \nAttention-based Networks: These hypernetworks integrate attention mechanisms, enabling them to focus on relevant input features selectively. This helps in handling long-range dependencies and enhances the quality of the generated outputs.\n\n\n\n\n\n\n  \nEach architecture has distinct advantages, allowing hypernetworks to generate weights in tune with the specific requirements of the target network and the data at hand.\n\n  \nThat being said, let’s look at a few pros anc cons of hypernets below.\n\n\n\n\n\nHypernets Pros and Cons\n\n\n\n  \nHypernets (Hypernetworks) Pros:\n    \n\n      \nFlexibility and Adaptability: They can generate weights dynamically, allowing for flexible and adaptable model designs.\n\n      \nSoft Weight Sharing: Hypernets can be conditioned to generate weights for multiple related DNNs, enabling information sharing among tasks and promoting transfer learning.\n\n      \nDynamic Architectures: Capable of generating weights for a network whose architecture might change during training or inference.\n\n      \nData-adaptive: Unlike standard DNNs, which have fixed weights at inference, HyperDNNs can adapt to input data, making the model more versatile.\n\n      \nUncertainty Quantification: They can produce an ensemble of models by generating different sets of weights, aiding in estimating model uncertainty.\n\n    \n\n  \n\n  \nHypernets Cons:\n    \n\n      \nIncreased Complexity: Introducing a network to generate weights for another network adds a layer of complexity to the model design.\n\n      \nTraining Stability: Training hypernets can be more challenging due to the indirect weight generation, potentially leading to instability.\n\n      \nResource Intensive: Despite potential weight compression, the introduction of hypernets may increase the overall computational requirements.\n\n      \nHarder to Interpret: The added layer of abstraction might make the model harder to understand and interpret.\n\n    \n\n  \n\n\n\n\n\nHyperDNNs Pros and Cons\n\n\n\n  \nHyperDNNs Pros:\n    \n\n      \nParameter Efficiency: HyperDNNs can achieve weight compression, potentially resulting in fewer parameters than standard DNNs.\n\n      \nAdaptability: Since the weights are generated dynamically, HyperDNNs can better adapt to varying input data or tasks.\n\n      \nPotential for Better Performance: By tailoring weights based on input data or conditions, HyperDNNs might achieve better performance on certain tasks compared to traditional DNNs.\n\n      \nVersatility Across Tasks: HyperDNNs can be employed across a wide range of deep learning problems, from ensemble learning to neural architecture search.\n\n    \n\n  \n\n  \nHyperDNNs Cons:\n    \n\n      \nTraining Challenges: The indirect weight generation might make the training process more intricate and challenging.\n\n      \nPossible Overfitting: Due to the added complexity and adaptability, there’s a potential risk of overfitting if not managed properly.\n\n      \nPerformance Variability: Since weights are generated dynamically, there might be variability in performance across different inputs or conditions.\n\n      \nResource Considerations: Even though they might have fewer weights, the computational cost of dynamically generating these weights can be resource-intensive.\n\n    \n\n  \n\n  \nIt’s essential to note that the choice between hypernets, HyperDNNs, and traditional DNNs would depend on the specific problem at hand, the available resources, and the desired outcomes.\n\n\n\n\n\nReferences\n\n\n\n  \nA Brief Review of Hypernetworks in Deep Learning\n\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/hypernetworks/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Training Loss > Validation Loss?\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nTheories\n\n  \nRemedies\n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nOverview\n\n\n\n\n  \nSometimes, you’ll notice the training loss being more than the validation loss. Ever wondered why?\n\n\n\n\n\n\n\n\nTheories\n\n\n\n\n  \nHere are some theories as to why that might be the case.\n\n  \nRegularization:\n The most common reason is \nregularization\n (e.g., dropout), since it applies during training, but not during validation and testing. If we add the regularization loss to the validation loss, here’s how things look:\n\n\n\n\n\n\n\n\n\n  \nEpoch delta between training and validation loss:\n The training loss is measured \nduring\n each epoch, while the validation loss is measured \nafter\n each epoch, so on average the training loss is measured \nhalf an epoch\n earlier. If we \nshift the training loss by half an epoch\n to the left (where it should be), things look much different:\n\n\n\n\n\n\n\n\n\n  \n\n    \nEasier validation set:\n Perhaps, the validation set is easier than the training set! This can happen by chance if the validation set is too small, or if it wasn’t properly sampled (e.g., too many easy classes).\n\n  \n\n  \n\n    \nData leaks:\n It might also be possible that the training set \nleaked\n into the validation set.\n\n  \n\n  \nData augmentation:\n Using \ndata augmentation\n during training might also cause this.\n    \n\n      \nAs an example, suppose that the augmentation algorithm involves randomly cropping the images, and 10% of the time, the resulting cropped image misses the main object in the image. Classifying the training images will be \nmore difficult\n than classifying the validation images.\n\n      \nAnother common case is when the augmentation algorithm involves many transformations, and the resulting images have more \ndiversity\n (in lighting, rotation, scale, etc.) than the validation images. Again, the training images would be harder to classify than the validation images.\n\n      \nTo validate this theory, using the same augmentation procedure (as used in training) for validation to compare the training and validation losses, would make sense.\n        \n\n          \nHowever, make sure to \nnot do this\n if you’re using early stopping or comparing different models, since in these cases, you are really only interested in the test performance, not the train performance.\n\n        \n\n      \n\n    \n\n  \n\n  \nNote that even if the validation loss is close to the train loss, your model may still be \noverfitting\n.\n\n\n\n\n\nRemedies\n\n\n\n\n  \nAccount for the regularization loss when comparing training and validation losses.\n\n  \nShift the training loss by half an epoch.\n\n  \nMake sure the validation set is large enough.\n\n  \nSample the validation set from the same distribution as train, without leaks.\n\n\n\n\n\nReferences\n\n\n\n\n  \nAurélien Geron’s Twitter\n for the great inputs.\n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledTrainingLossValidationLoss,\n  title   = {Training Loss > Validation Loss?},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/train-val-loss/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nGradient Accumulation and Checkpointing\n\n  \n\n\n  \n\n  \n\n  \nOverview\n\n  \nGradient Accumulation\n\n  \nGradient Checkpointing\n    \n\n      \nRelated: FSDP and QLoRA to train a 70b LLM at home!\n\n    \n\n  \n\n  \nReferences\n\n\n\n\n\nOverview\n\n\n\n\n  \nWith models getting larger, running out of GPU memory and getting a \nCUDA: out of memory (OOM) error\n has become more ubiquitous.\n\n  \nIn this article, we will talk about a few ways to make the training process more efficient by some gradient hacks and use GPU memory optimally.\n\n\n\n\n\nGradient Accumulation\n\n\n\n  \nGradient accumulation is a technique used to overcome memory limitations when training large models or processing large batches of data. Normally, during backpropagation, the gradients of the model parameters are calculated and updated after each batch of data. However, in gradient accumulation, instead of updating the parameters after each batch, the gradients are accumulated over multiple batches before performing a parameter update. This allows for a more memory-efficient training process by reducing the memory requirements for storing gradients.\n\n  \nFor example, if gradient accumulation is set to accumulate gradients over four batches, the gradients of the model parameters are summed over these batches, and the parameter update is applied once after the accumulation. This reduces the memory footprint by performing fewer updates and enables training with larger batch sizes or more complex models.\n\n  \nGradient accumulation is a technique used in deep learning to increase the effective batch size during training. Normally, the weights of a neural network are updated based on the gradients computed from a single batch of training data. However, for larger models or datasets, the batch size may be limited by the memory capacity of the GPU, leading to a significantly longer time to convergence due to vectorization.\n\n  \nAs shown in the image below (\nsource\n), gradient accumulation splits the batch of samples (that are used to train a neural network) into several mini-batches that are run sequentially. Put simply, the idea behind gradient accumulation is to accumulate the gradients iteratively over several mini-batches.\n\n\n\n\n\n\n\n\n\n  \nOnce we have enough gradients accumulated via the above process, we run the model’s optimization step (via the usual \noptimizer.step()\n) to increase the overall batch size.\n\n  \nThe code sample below (\nsource\n] shows how the model gets impacted positively by gradient accumulation.\n\n\n\n\n\ntraining_args\n \n=\n \nTrainingArguments\n(\nper_device_train_batch_size\n=\n1\n,\n \ngradient_accumulation_steps\n=\n4\n,\n \n**\ndefault_args\n)\n\n\n\ntrainer\n \n=\n \nTrainer\n(\nmodel\n=\nmodel\n,\n \nargs\n=\ntraining_args\n,\n \ntrain_dataset\n=\nds\n)\n\n\nresult\n \n=\n \ntrainer\n.\ntrain\n()\n\n\nprint_summary\n(\nresult\n)\n\n\n\n\n\n>\n \nBEFORE\n\n\nTime\n:\n \n57.82\n\n\nSamples\n/\nsecond\n:\n \n8.86\n\n\nGPU\n \nmemory\n:\n \n14949\n \nMB\n\n\n\n>\n \nAFTER\n\n\nTime\n:\n \n66.03\n\n\nSamples\n/\nsecond\n:\n \n7.75\n\n\nGPU\n \nmemory\n:\n \n8681\n \nMB\n\n\n\n\n\n\n  \nGradient accumulation can lead to slower convergence and longer training times, as the gradients are accumulated over several mini-batches before an update is made. However, it can be a useful technique in situations where memory is limited and a larger effective batch size is desired (especially with contrastive learning where larger batch sizes lead to better learning due to added diversity within large training batches).\n\n  \nThe code below helps illustrate the basic idea behind gradient accumulation. In it, we train a loop of \nnum_iterations\n iterations and within each iteration, \naccumulation_step\n mini-batches are processed before updating the weights.\n\n  \nDuring each iteration, the gradients for each mini-batch are computed separately using \ncompute_gradients()\n. The gradients for each mini-batch are then accumulated in accumulated_gradients variable. After processing accumulation_steps mini-batches, the accumulated gradients are then used to update the weights using \nupdate_weights()\n.\n\n\n\n\n\n# Training loop\n\n\n\nfor\n \ni\n \nin\n \nrange\n(\nnum_iterations\n):\n\n    \naccumulated_gradients\n \n=\n \n0\n\n    \nfor\n \nj\n \nin\n \nrange\n(\naccumulation_steps\n):\n\n        \nbatch\n \n=\n \nnext\n(\ntraining_batch\n)\n\n        \ngradients\n \n=\n \ncompute_gradients\n(\nbatch\n)\n\n        \naccumulated_gradients\n \n+=\n \ngradients\n\n    \nupdate_weights\n(\naccumulated_gradients\n)\n\n\n\n\n\nGradient Checkpointing\n\n\n\n\n  \nGradient checkpointing is a technique used to trade off memory usage for computation time during backpropagation. In deep neural networks, backpropagation requires storing intermediate activations for computing gradients during the backward pass. However, for models with a large number of layers or limited memory, storing all the intermediate activations can be memory-intensive.\n\n  \nGradient checkpointing addresses this issue by selectively recomputing a subset of intermediate activations during backpropagation. Instead of storing all activations, only a subset of them, typically those necessary for computing gradients, are cached. The remaining intermediate activations are recomputed on-the-fly during the backward pass. By recomputing rather than storing all intermediate activations, memory usage is reduced at the cost of increased computation time.\n\n  \nThe specific choice of which intermediate activations to checkpoint depends on the memory requirements and computational trade-offs of the model. By strategically checkpointing activations, gradient checkpointing allows for more memory-efficient training, enabling the use of larger models or reducing memory bottlenecks in deep learning tasks.\n\n  \nGradient checkpointing helps to reduce the memory requirements during the backpropagation phase of training, especially in models with a large number of layers or parameters.\n\n  \nIn order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead.\n\n  \nInstead of storing all the intermediate activations during the forward pass, gradient checkpointing stores only a subset of them. During the backward pass, the missing intermediate activations are recomputed on-the-fly, reducing the amount of memory required during training.\n\n  \nAlternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\n\n  \nThis trade-off allows the use of larger models or batch sizes that would be otherwise infeasible due to memory constraints.\n\n  \nThere are two ways you can think of doing gradient checkpointing:\n\n  \nIn summary, gradient accumulation addresses memory constraints by accumulating gradients over multiple batches before updating model parameters, while gradient checkpointing selectively recomputes a subset of intermediate activations to reduce memory usage during backpropagation. Both techniques offer ways to optimize memory and computational resources in deep learning training.\n\n  \nThe code below (\nsource\n), with addition of gradient checkpointing along with gradient accumulation, we can see that some memory is saved but the training time has become slower. As \nHuggingFace\n mentions, a good rule of thumb is that gradient checkpointing slows down training by 20%.\n\n\n\n\n\ntraining_args\n \n=\n \nTrainingArguments\n(\n\n    \nper_device_train_batch_size\n=\n1\n,\n \ngradient_accumulation_steps\n=\n4\n,\n \ngradient_checkpointing\n=\nTrue\n,\n \n**\ndefault_args\n\n\n)\n\n\n\ntrainer\n \n=\n \nTrainer\n(\nmodel\n=\nmodel\n,\n \nargs\n=\ntraining_args\n,\n \ntrain_dataset\n=\nds\n)\n\n\nresult\n \n=\n \ntrainer\n.\ntrain\n()\n\n\nprint_summary\n(\nresult\n)\n\n\n\n\n\n>\n \nBEFORE\n\n\nTime\n:\n \n66.03\n\n\nSamples\n/\nsecond\n:\n \n7.75\n\n\nGPU\n \nmemory\n:\n \n8681\n \nMB\n\n\n\n>\n \nAFTER\n\n\nTime\n:\n \n85.47\n\n\nSamples\n/\nsecond\n:\n \n5.99\n\n\nGPU\n \nmemory\n \noccupied\n:\n \n6775\n \nMB\n.\n\n\n\n\n\nRelated: FSDP and QLoRA to train a 70b LLM at home!\n\n\n\n\n  \nAnswer.AI\n details an open source system, based on FSDP and QLoRA, that can train a 70b model on two consumer 24GB gaming GPUs.\n\n  \nIn collaboration with Tim Dettmers (University of Washington) and Hugging Face’s Titus von Koeller and Sourab Mangrulkar, Answer.AI has released an open-source system that enables the training of a 70 billion parameter language model on a standard gaming PC.\n\n  \nDemocratizing AI with Gaming GPUs\n\n    \n\n      \nThe limiting factor in training large language models on consumer-grade GPUs is the amount of memory available on these cards. While gaming GPUs like the RTX 3090 or 4090 offer impressive computational power, they typically have a maximum of 24GB of RAM. This is significantly less than the memory found on data center-class GPUs, such as the A100 or H100, which can have up to 80GB of RAM.\n\n      \nThe memory limitation becomes a bottleneck when training large models, as the entire model, along with activations, gradients, and optimization states, needs to fit within the GPU’s memory.\n\n      \nThis constraint has made it challenging to train state-of-the-art models with billions of parameters on consumer hardware, as the model size alone can exceed the available memory. Consequently, the limited memory capacity of gaming GPUs has been the primary obstacle in making large model training accessible to a wider audience.\n\n    \n\n  \n\n  \nThis innovation makes large model training more accessible by leveraging the power of gaming GPUs like the RTX 3090 or 4090. The cost-effectiveness and accessibility of this approach have the potential to revolutionize the AI landscape.\n\n  \nThe Technology Behind the Scenes: FSDP and QLoRA\n\n    \n\n      \nThe system combines two innovative technologies:\n        \n\n          \n“Fully Sharded Data Parallel (FSDP)”:\n Allows efficient model training across multiple GPUs.\n\n          \n“Quantization and Low-Rank Adaptation (QLoRA)”:\n Overcomes memory limitations of gaming GPUs.\n\n        \n\n      \n\n    \n\n  \n\n  \nTogether, FSDP and QLoRA enable small labs and individuals to train large models locally, without the need for expensive specialized hardware.\n\n  \nEmpowering the Open Source Community\n\n    \n\n      \nThis development has the potential to accelerate AI innovation by making state-of-the-art models more accessible to researchers, startups, and enthusiasts. Teknium, the creator of the popular OpenHermes models and datasets, stated, “With this capability, we can take huge models to new heights locally, and gigantic, hundreds of billions of parameter models are now accessible by small labs.”\n\n    \n\n  \n\n\n\n\n\n\n\n\nReferences\n\n\n\n  \nRaz Rotenberg’s What is Gradient Accumulation in Deep Learning?\n\n  \nHugging Face’s Perfomance and Scalability\n\n  \nYaroslav Bulatov’s Fitting larger network into memory\n\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/grad-accum-checkpoint/"},
{"page_content": "addBackToTop({\n        backgroundColor: '#fff',\n        innerHTML: 'Back to Top',\n        textColor: '#333'\n      })\n\n      \n\n        #back-to-top {\n          border: 1px solid #ccc;\n          border-radius: 0;\n          font-family: sans-serif;\n          font-size: 14px;\n          width: 100px;\n          text-align: center;\n          line-height: 30px;\n          height: 30px;\n        }\n      \n   \n\n    \n\n\n  \nDistilled AI\n\n\n  \nBack to aman.ai\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n\n  \n\n  \n\n  document.getElementById('search-input').value='';\n  SimpleJekyllSearch({\n    searchInput: document.getElementById('search-input'),\n    resultsContainer: document.getElementById('results-container'),\n    exclude: [\"cs231a\"],\n    searchResultTemplate: '<div class=\"site-search-results\"><a href=\"{url}\">{title}</a></div>',\n    noResultsText: '<div class=\"site-search-results\"><p>No results found</p></div>',\n    json: 'https://aman.ai/search.json',\n    limit: 5,\n    fuzzy: false,\n  })\n  \n    \n\n\n     \n\n    \n\n      \n\n      \n\n\n  \n\n    \nPrimers • Receptive Field\n\n  \n\n\n  \n\n  \n\n  \nIntroduction\n\n  \nWhat is the receptive field in deep learning?\n\n  \nWhy do we care about the receptive field of a convolutional network?\n\n  \nClosed-form calculations of the receptive field for single-path networks\n\n  \nHow can we increase the receptive field in a convolutional network?\n    \n\n      \nAdd more convolutional layers\n\n      \nSub-sampling and dilated convolutions\n        \n\n          \nAnalysis\n\n        \n\n      \n\n      \nDepth-wise convolutions\n\n      \nSkip-connections and receptive field\n\n      \nReceptive field and transposed convolutions, upsampling, separable convolutions, and batch normalization\n        \n\n          \nUpsampling\n\n          \nSeparable convolutions\n\n          \nBatch normalization\n\n        \n\n      \n\n    \n\n  \n\n  \nUnderstanding the effective receptive field\n\n  \nConclusion\n\n  \nFurther Reading\n\n  \nReferences\n\n  \nCitation\n\n\n\n\n\nIntroduction\n\n\n\n\n  \n\n    \nIn this article, we will discuss multiple perspectives that involve the receptive field of a deep convolutional architecture. We will address the influence of the receptive field starting for the human visual system. As you will see, a lot of terminology of deep learning comes from neuroscience. As a short motivation, convolutions are awesome but it is not enough just to understand how it works. The idea of the receptive field will help you dive into the architecture that you are using or developing. This article offers an in-depth analysis to understand how you can calculate the receptive field of your model as well as the most effective ways to increase it.\n\n  \n\n  \n\n    \nAccording to the \n“Receptive field”\n article on Wikipedia, the receptive field (of a biological neuron) is “the portion of the sensory space that can elicit neuronal responses, when stimulated”. The sensory space can be defined in any dimension (e.g. a 2D perceived image for an eye). Simply, the neuronal response can be defined as the firing rate (i.e., number of action potentials generated by a neuron). It is related to the time dimension based on the stimuli. What is important is that it affects the received frames per second (FPS) of our visual system. It is not clear what is the exact FPS of our visual system, and it is definitely changing in different situations (i.e., when we are in danger). The \n“Frame rate: Human Vision”\n article on Wikipedia says:\n\n  \n\n\n\n\n\n\n  \nInsight: The human visual system can process 10 to 12 images per second and perceive them individually, while higher rates are perceived as motion.\n\n\n\n\n\n\n  \nLet’s observe the visual human system (image taken from \nbrainconnection\n) to further clarify these concepts:\n\n\n\n\n\n\n\n\n\n  \n\n    \nBased on the image, the entire area (in this case, the grid in the figure) an eye can see is called the field of view. The human visual system consists of millions of neurons, where each one captures different information. We define the neuron’s receptive field as the patch of the total field of view – in other words, the information a single neuron has access to. This is in simple terms the biological cell’s receptive field.\n\n  \n\n  \n\n    \nNow, let’s see how we can extend this idea to convolutional networks.\n\n  \n\n\n\n\n\nWhat is the receptive field in deep learning?\n\n\n\n\n  \nPer \nAraujo et al.\n, in a deep learning context, the Receptive Field (RF) is defined as the size of the region in the input that produces the feature. Basically, it is a measure of association of an output feature (of any layer) to the input region (patch), i.e., the “perspective” of the model when generating the output feature. Before we move on, let’s clarify one important thing:\n\n\n\n\n\n\n  \nInsight: The idea of receptive fields applies to local operations (i.e., convolution, pooling).\n\n\n\n\n\n\n  \nThe following image from \nResearch Gate\n shows the receptive field of three successive conv layers:\n\n\n\n\n\n\n\n\n\n  \n\n    \nA convolutional unit only depends on a local region (patch) of the input. That’s why we never refer to the RF on fully connected layers since each unit has access to all the input region. To this end, the aim of this article is to develop intuition about this concept, in order to understand and analyze how deep convolutional networks with local operations work.\n\n  \n\n  \n\n    \nOk, but why should anyone care about the RF?\n\n  \n\n\n\n\n\nWhy do we care about the receptive field of a convolutional network?\n\n\n\n\n  \nThere is no better way to clarify this than a couple of computer vision examples. In particular, let’s revisit a couple of dense prediction computer vision tasks. Specifically, in \nimage segmentation\n and optical flow estimation, we produce a prediction for each pixel in the input image, which corresponds to a new image, the semantic label map. Ideally, we would like each output pixel of the label map to have a big receptive field, so as to ensure that no crucial information was not taken into account. For instance, if we want to predict the boundaries of an object (i.e., a car, an organ like the heart, a tumor) it is important that we provide the model access to all the relevant parts of the input object that we want to segment. In the image below (from \nNvidia’s blog\n), you can see two receptive fields: the green and the orange one. Which one would you prefer?\n\n\n\n\n\n\n\n\n\n  \n\n    \nSimilarly, in \nobject detection\n, a small receptive field may not be able to recognize large objects. That’s why you usually see multi-scale approaches in object detection. Furthermore, in motion-based tasks, like video prediction and optical flow estimation, we want to capture large motions (displacements of pixels in a 2D grid), so we want to have an adequate receptive field. Specifically, the receptive field should be sufficient if it is larger than the largest flow magnitude of the dataset.\n\n  \n\n  \n\n    \nTherefore, our goal is to design a convolutional model so that we ensure that its RF covers the entire relevant input image region.\n\n  \n\n  \n\n    \nIn the diagram below (borrowed from \nAraujo et al.\n), you can see the relation of the RF to the classification accuracy in ImageNet. The radius refers to the amount of floating-point operations (FLOPs) of each model. The purple corresponds to the \nResNet\n family (50-layer, 101, and 152 layers), while the yellow is the \nInception family\n (v2, v3, v4). Light-blue is the \nMobileNet\n architecture.\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n  \nAs described in \n“Computing receptive fields of convolutional neural networks”\n by Araujo et al. (2019):\n\n\n\n\n\n\n  \n“We observe a logarithmic relationship between classification accuracy and receptive field size, which suggests that \nlarge receptive fields\n are necessary for high-level recognition tasks, but with \ndiminishing rewards\n.”\n\n\n\n\n\n\n  \n\n    \nNevertheless, the receptive field size alone is not the only factor contributing to improved recognition performance. However, the point is that you should definitely be aware of your model’s receptive field.\n\n  \n\n  \n\n    \nOk, so how can we measure it?\n\n  \n\n\n\n\n\nClosed-form calculations of the receptive field for single-path networks\n\n\n\n\n  \n\n    \nAraujo et al. (2019)\n provide an intuitive way to calculate in an analytical form the RF of your model. A single path literally means no \nskip connections\n in the architecture, like the famous AlexNet. Let’s see some math! For two sequential convolutional layers \\(f_2\\), \\(f_1\\) with kernel size \\(k\\), stride \\(s\\), receptive field \\(r\\):\n\n\n\\[r_1 = s_2 \\times r_2 + (k_2-s_2)\\]\n\n    \n\n      \nOr in a more general form:\n\n    \n\n\n\\[r_{(i-1)} = s_{i} \\times r_{i} + (k_{i}-s_{i})\\]\n  \n\n  \n\n    \nThe image below may help you clarify this equation. Note that we are interested to see the influence of the receptive field starting from the last layer towards the input. So, in that sense, we go backwards. The following diagram (taken from \nAraujo et al. (2019)\n) visualizes 1D sequential conv:\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n    \nIt seems like this equation can be generalized to a beautiful compact form that simply applies this operation recursively for \\(L\\) layers. By further analyzing the recursive equation, we can derive a closed form solution that depends only on the convolutional parameters of kernels and strides:\n\n\n\\[r_0 = \\sum_{i=1}^{L} ( (k_{i} -1) \\prod_{j=1}^{l-1} s_{j} ) + 1\n  \\tag{1}\\]\n\n    \n\n      \nwhere \\(r_0\\) denotes the desired RF of the architecture.\n\n    \n\n  \n\n  \n\n    \nNow the next question is… Now that we have measured the theoretical RF of our model, how do we increase it?\n\n  \n\n\n\n\n\nHow can we increase the receptive field in a convolutional network?\n\n\n\n\n  \n\n    \nIn essence, there are a plethora of ways and tricks to increase the RF, that can be summarized as follows:\n\n\n    \n\n      \n\n        \nAdd more convolutional layers (make the network deeper)\n\n      \n\n      \n\n        \nAdd pooling layers or higher stride convolutions (sub-sampling)\n\n      \n\n      \n\n        \nUse \ndilated convolutions\n\n      \n\n      \n\n        \nDepth-wise convolutions\n\n      \n\n    \n\n  \n\n  \n\n    \nLet’s look at the distinct characteristics of these approaches.\n\n  \n\n\n\n\n\nAdd more convolutional layers\n\n\n\n\n  \nPer \n“Understanding the effective receptive field in deep convolutional neural networks”\n by Luo et al. (2016), adding more convolutional layers increases the receptive field size linearly, as each extra layer increases the \nreceptive field size by the kernel size\n. Moreover, it is experimentally validated that as the theoretical receptive field is increasing but the effective (experimental) receptive field (ERF) is reducing. The following figure (taken from \nLuo et al. (2016)\n) shows the impact of increasing the number of layers decreases the ERF ration:\n\n\n\n\n\n\n\n\nSub-sampling and dilated convolutions\n\n\n\n\n  \n\n    \nSub-sampling techniques like pooling on the other hand, increases the receptive field size multiplicatively. Modern architectures like ResNet combine adding more convolutional layers and sub-sampling. On the other hand, sequentially placed dilated convolutions, increase the RF exponentially.\n\n  \n\n  \n\n    \nBut first, let’s revisit the idea of dilated convolutions.\n\n  \n\n  \n\n    \nIn essence, dilated convolutions introduce another parameter, denoted as \\(r\\), called the dilation rate. Dilations introduce “holes” in a convolutional kernel. The “holes” basically define a spacing between the values of the kernel. So, while the number of weights in the kernel is unchanged, the weights are no longer applied to spatially adjacent samples. Dilating a kernel by a factor of \\(r\\) introduces a kind of striding of \\(r\\).\n\n  \n\n  \n\n    \nThe equation \\((1)\\) in the \nabove\n section can be reused by simply replacing the kernel size \\(k\\) for all layers using dilations:\n\n  \n\n\n\n\n\\[k’= r (k−1)+1\\]\n\n\n\n  \nAll the above can be illustrated in the following animation, from \n“A guide to convolutional arithmetic”\n by Dumoulin (2016).\n\n\n\n\n\n\n\n\n\n  \n\n    \nNow, let’s briefly inspect how dilated convolutions can influence the receptive field.\n\n  \n\n  \n\n    \nLet’s dig into three sequential conv. Layers (denoted by \\(a, b, c\\)) that are illustrated in the image with normal convolution, \\(r = 2\\) dilation factor, and \\(r = 4\\) dilation factor. We will intuitively understand why dilation supports an exponential expansion of the receptive field without loss of resolution (i.e., pooling) or coverage. Refer to the following image from \n“Multi-scale context aggregation by dilated convolutions”\n by Yu and Koltun (2015):\n\n  \n\n\n\n\n\n\n\n\nAnalysis\n\n\n\n\n  \nIn (a), we have a normal \\(3 \\times 3\\) convolution with receptive field \\(3 \\times 3\\).\n\n  \nIn (b), we have a 2-dilated \\(3 \\times 3\\) convolution that is applied in the output of layer (a) which is a normal convolution. As a result, each element in the 2 coupled layers now has a receptive field of \\(7 \\times 7\\). If we studied 2-dilated conv alone the receptive field would be simply \\(5 \\times 5\\) with the same number of parameters.\n\n  \n\n    \nIn (c), by applying a 4-dilated convolution, each element in the third sequential conv layer now has a receptive field of \\(15 \\times 15\\). As a result, the receptive field grows exponentially while the number of parameters grows linearly per \nYu and Koltun (2015)\n.\n\n  \n\n  \nIn other words, a \\(3 \\times 3\\) kernel with a dilation rate of 2 will have the same receptive field as a \\(5 \\times 5\\) kernel, while only using 9 parameters. Similarly, a \\(3 \\times 3\\) kernel with a dilation rate of 4 will have the same receptive field as a \\(9 \\times 9\\) kernel without dilation. Mathematically:\n\n\n\n\n\\[r(k −1)+1 = k_{prev}\\]\n\n\n\n  \nInsight: In deep architectures, we often introduce dilated convolutions in the last convolutional layers.\n\n\n\n\n\n\n  \nBelow you can observe the resulting ERF (effective receptive field) when introducing pooling and dilation in an experimental study performed by \nLuo et al. (2016)\n. The receptive field is larger in both cases than the vanilla conv, but is largest with pooling. We’ll see more about the effective receptive field in a \nlater\n section. The following figure (taken from \nLuo et al. (2016)\n) shows a visualization of the effective receptive field (ERF) by introducing pooling strategies and dilation:\n\n\n\n\n\n\n\n\n\n  \nInsight: Based on \nLuo et al. (2016)\n, pooling operations and dilated convolutions turn out to be effective ways to increase the receptive field size quickly.\n\n\n\n\n\nDepth-wise convolutions\n\n\n\n\n  \nFinally as described in \n“Computing receptive fields of convolutional neural networks”\n by Araujo et al. (2019), with depth-wise convolutions the receptive field is increased with a small compute footprint, so it is considered a compact way to increase the receptive field with fewer parameters. Depthwise convolution is the channel-wise spatial convolution. However, note that depth-wise convolutions do not directly increase the receptive field. But since we use fewer parameters with more compact computations, we can add more layers. Thus, with roughly the same number of parameters, we can get a bigger receptive field. \nMobileNet\n by Howard et al. (2017) achieves high recognition performance based on this idea.\n\n\n\n\n\nSkip-connections and receptive field\n\n\n\n\n  \n\n    \nIf you want to revisit the intuition behind skip connections, check out our article on \nSkip Connections\n.\n\n  \n\n  \n\n    \nIn a model without any skip-connections, the receptive field is considered fixed. However, when introducing \\(n\\) skip-residual blocks, the networks utilize \\(2^n\\) different paths and therefore \nfeatures can be learned with a large range of different receptive fields\n by \nLi et al. (2017)\n. For example, the \nHighResNet architecture\n has a maximum receptive field of 87 pixels, coming from 29 unique paths. In the following figure, we can observe the distribution of the receptive field of these paths in the architecture. The receptive field, in this case, ranges from 3 to 87, following a \nbinomial distribution\n. The histogram of receptive field distribution from \n“On the compactness, efficiency, and representation of 3D convolutional networks: brain parcellation as a pretext task”\n by Li et al. (2017) is as follows:\n\n  \n\n\n\n\n\n\n\n\n\n  \nInsight: Skip-connections may provide more paths, however, based on \nLuo et al. (2016)\n, they tend to make the effective receptive field smaller.\n\n\n\n\n\nReceptive field and transposed convolutions, upsampling, separable convolutions, and batch normalization\n\n\n\nUpsampling\n\n\n\n\n  \nUpsampling is also a local operation. For the purposes of RF computation, we can consider the kernel size to be equal to the number of input features involved in the computation of an output feature. Since we usually double the spatial dimension, as shown in the figure below, the kernel is \\(k=1\\).\n\n\n\n\n\n\n\n\nSeparable convolutions\n\n\n\n\n  \nIn short, the RF properties of the separable convolution are identical to its corresponding equivalent non-separable convolution. So, practically nothing changes in terms of the receptive field.\n\n\n\n\n\nBatch normalization\n\n\n\n\n  \nDuring training, batch normalization parameters are computed based on all the channel elements of the feature map. Thus, one can state that its receptive field is the whole input image.\n\n\n\n\n\nUnderstanding the effective receptive field\n\n\n\n\n  \n\n    \nIn \n“Understanding the effective receptive field in deep convolutional neural networks”\n by Luo et al. (2016) discover that not all pixels in a receptive field contribute equally to an output unit’s response. In the previous image, we observed that the receptive field varies with skip connections.\n\n  \n\n  \n\n    \nObviously, the output feature is not equally impacted by all pixels within its receptive field. Intuitively, it is easy to perceive that pixels at the center of a receptive field have a much larger impact on output since they have more “paths” to contribute to the output.\n\n  \n\n  \n\n    \nAs a natural consequence, one can define the relative importance of each input pixel as the effective receptive field (ERF) of the feature. In other words, ERF defines the effective receptive field of a central output unit as the region that contains any input pixel with a non-negligible impact on that unit.\n\n  \n\n  \n\n    \nSpecifically, as it is referenced in \nLuo et al. (2016)\n we can intuitively realize the contribution of central pixels in the forward and backward pass as:\n\n  \n\n\n\n\n\n\n  \n“In the forward pass, central pixels can propagate information to the output through many different paths, while the pixels in the outer area of the receptive field have very few paths to propagate its impact. In the backward pass, gradients from an output unit are propagated across all the paths, and therefore the central pixels have a much larger magnitude for the gradient from that output.” - by \nLuo et al. (2016)\n.\n\n\n\n\n\n\n  \nA natural way to measure this impact is of course the partial derivative, rate of change of the unit with respect to the input, as it is computed by backpropagation.\n\n\n\n\n\n\n\n\n\n  \n\n    \nAs illustrated in the figure above, the ERF is a perfect example of a textbook 2D Gaussian distribution. However, when we add non-linearities, we force the distribution to deviate from a perfect Gaussian. In simple terms, when the pixel-value is zeroed with the ReLU, no path from the receptive field can reach the output, hence the gradient is zero.\n\n  \n\n  \n\n    \nBased on this study the main insight is the following:\n\n  \n\n\n\n\n\n\n  \nInsight: The ERF in deep convolutional networks actually grows a lot slower than we calculate in theory - by \nLuo et al. (2016)\n.\n\n\n\n\n\n\n  \nLast but not least, it is super important to highlight that after the training process the ERF is increased, minimizing the gap between the theoretical RF and the ERF before training.\n\n\n\n\n\nConclusion\n\n\n\n\n  \n\n    \nIn this article, we inspected several aspects of the concept of the Receptive Field. We drew parallels with the human visual system and the concept of RF in CNNs. We discussed the closed-form math, skip connections in RF, and how you can increase it efficiently. Based on that, you can implement the referenced design choices in your model, while being aware of its implications.\n\n  \n\n  \n\n    \nKey takeaways\n:\n\n\n    \n\n      \nThe idea of receptive fields applies to local operations.\n\n      \nWe want to design a model so that it’s receptive field covers the entire relevant input image region.\n\n      \nAdding more convolutional layers increases the receptive field size linearly, as each extra layer increases the receptive field size by the kernel size.\n\n      \nBy using sequential dilated convolutions the receptive field grows exponentially, while the number of parameters grows linearly.\n\n      \nPooling operations and dilated convolutions turn out to be effective ways to increase the receptive field size quickly.\n\n      \nSkip-connections may provide more paths, but tend to make the effective receptive field smaller.\n\n      \nThe effective receptive field is increased after training.\n\n    \n\n  \n\n  \n\n    \nAs a final note, the understanding of RF in convolutional neural networks is an open research topic that will provide a lot of insights on what makes deep convolutional networks tick.\n\n  \n\n\n\n\n\nFurther Reading\n\n\n\n\n  \nHere are some (optional) links you may find interesting:\n    \n\n      \nUnderstanding the Effective Receptive Field in Deep Convolutional Neural Networks\n by Luo et al. (2016)\n\n      \nAs an additional resource on the interpretation and visualization of RF, check out \n“Interpretation of ResNet by Visualization of Preferred Stimulus in Receptive Fields”\n by Kobayashi et al. (2020). For our more practical reader, if you want a toolkit to automatically measure the receptive field of your model in PyTorch in TensorFlow, we got your back. Finally, for those of you who are hungry for knowledge and curious for bio-inspired concepts like me, especially about the human visual system, you can watch this highly recommended starting video:\n\n    \n\n  \n\n\n\n\n\n\n\n\nReferences\n\n\n\n\n  \n\n    \nReceptive field\n on Wikipedia\n\n  \n\n  \n\n    \nFrame rate: Human Vision\n on Wikipedia\n\n  \n\n  \n\n    \nThe effective receptive field on CNNs\n by Christian Perone\n\n  \n\n  \n\n    \nUnderstanding the receptive field of deep convolutional networks\n by Nikolas Adaloglou\n\n  \n\n  \n\n    \nComputing receptive fields of convolutional neural networks\n by Araujo et al. (2019)\n\n  \n\n  \n\n    \nDeep residual learning for image recognition\n by He et al. (2016)\n\n  \n\n  \n\n    \nRethinking the inception architecture for computer vision\n by Szegedy et al. (2016)\n\n  \n\n  \n\n    \nUnderstanding the Effective Receptive Field in Deep Convolutional Neural Networks\n by Luo et al. (2016)\n\n  \n\n  \n\n    \nOn the compactness, efficiency, and representation of 3D convolutional networks: brain parcellation as a - ext task\n by Li et al. (2017)\n\n  \n\n  \n\n    \nMulti-scale context aggregation by dilated convolutions\n by Yu and Koltun (2015)\n\n  \n\n  \n\n    \nMobileNets: Efficient convolutional neural networks for mobile vision applications\n by Howard et al. (2017)\n\n  \n\n  \n\n    \nA guide to convolution arithmetic for deep learning\n by Dumoulin and Visin (2016).\n\n  \n\n  \n\n    \nInterpretation of ResNet by Visualization of Preferred Stimulus in Receptive Fields\n by Kobayashi and Shouno (2020).\n\n  \n\n\n\n\n\nCitation\n\n\n\nIf you found our work useful, please cite it as:\n\n\n\n@article{Chadha2020DistilledReceptiveField,\n  title   = {Receptive Field},\n  author  = {Chadha, Aman},\n  journal = {Distilled AI},\n  year    = {2020},\n  note    = {\\url{https://aman.ai}}\n}\n\n\n\n  \n\n\n\n\n\n      \n\n    \n\n\n    \n\n   \n\n      \n\n         \n\n            \n\n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n               \n\n                  \n\n                     \n\n                        \n\n                     \n\n                  \n\n               \n\n               \n\n                | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n               | \n               \n\n                  \n\n                     \n\n                  \n\n               \n\n            \n\n         \n\n      \n\n      \n\n         \nwww.amanchadha.com\n\n      \n\n      \n\n   \n\n   \n\n   \n\n\n   \n\n   \n   \n   \n\n      var headings = document.querySelectorAll(\"h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]\");\n      \n      for (var i = 0; i < headings.length; i++) {\n          headings[i].innerHTML = titleCase(headings[i].innerHTML);\n      }\n      \n      var toc = document.querySelectorAll(\"a[id^='markdown-toc-']\");\n      \n      for (var i = 0; i < toc.length; i++) {\n          toc[i].innerHTML = titleCase(toc[i].innerHTML);\n      }      \n   \n        \n\n\n\n    \n\n    \n\n    var options = {\n      classname: 'my-class',\n        id: 'my-id'\n    };\n    var nanobar = new Nanobar( options );\n    nanobar.go(100);\n    \n     \n\n    \n\n    \n\n    \n\n    \n\n    var element = document.documentElement,\n      body = document.body,\n      scrollTop = 'scrollTop',\n      scrollHeight = 'scrollHeight',\n      progress = document.querySelector('.progress-bar'),\n      scroll;\n\n    document.addEventListener('scroll', function() {\n      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;\n      progress.style.setProperty('--scroll', scroll + '%');\n    });\n    \n    \n    \n\n    \n\n    \n\n\n\n    \n\n    \n\n    \n\n      MathJax.Hub.Config({\n       \"HTML-CSS\": { linebreaks: { automatic: true } },\n       \"SVG\": { linebreaks: { automatic: true } },\n      });", "source": "https://aman.ai/primers/ai/receptive-field/"}
]